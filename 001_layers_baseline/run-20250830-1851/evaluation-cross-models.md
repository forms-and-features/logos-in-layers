# Cross‑Model Evaluation

**Result Synthesis**
Meaning generally consolidates late in the stack across these pre‑norm families, with one notable early outlier. Using rank milestones and within‑model KL trajectories (tuned‑lens style; arXiv:2303.08112), semantic collapse occurs at ≥70% depth for every model except Meta‑Llama‑3‑70B. Late group (≥70% of layers): Gemma‑2‑9B (42/42), Gemma‑2‑27B (46/46), Qwen‑3‑8B (31/36), Qwen‑3‑14B (36/40), Qwen‑2.5‑72B (80/80), Yi‑34B (44/60), Mistral‑7B‑v0.1 (25/32), Mistral‑Small‑24B‑Base‑2501 (33/40). Early group (<70%): Llama‑3‑70B (40/80). Across models with clean final‑head alignment, KL to final decreases toward the head and only crosses <1.0,<0.5 at the very last layer (e.g., first_kl_below_1.0 = n_layers for Llama‑3‑8B, Mistral‑7B, Qwen‑3‑8B/14B, Yi‑34B, Mistral‑Small‑24B, Qwen‑2.5‑72B). Gemma’s final‑layer KL is not ≈0 and is flagged in JSON, so cross‑model probability comparisons are deferred for that family (see below).

Copy‑reflex is rare under the strict ID‑level subsequence rule (k=1, τ=0.95, δ=0.10): only Gemma‑2‑9B and Gemma‑2‑27B show `copy_collapse = True` in layers 0–3 in the pure CSV. All other models have no early copy flag. Using the diagnostic L indices for Δ = L_sem − L_copy (fallback to L_copy_H), Gemma shows maximal separation (Δ̂ ≈ 1.0; 42/42 and 46/46), Qwen‑3‑14B shows a modest gap (Δ = 4, Δ̂ = 0.10), and Qwen‑3‑8B collapses copy and semantics together (Δ = 0). For models with null L_copy/H, Δ is unresolved by this rule and we rely on rank milestones instead. Entropy drops between L_copy and L_semantic mirror these patterns: Gemma shows near‑deterministic copy then slightly higher entropy at semantic collapse (ΔH ≈ −0.37 bits for 9B; −0.12 for 27B), Qwen‑3‑14B shows a sharper drop (ΔH ≈ +0.50 bits), and Qwen‑3‑8B is flat (ΔH ≈ 0). Within‑model cosine alignment to the final direction rises late in all cases, typically spiking at the head: e.g., Llama‑3‑8B cos_to_final grows from ~0.21 at L20 to ≈1.0 at L32; Qwen‑3‑14B reaches ≈0.61 at L36 and ≈1.0 only at the head; Mistral‑7B and Yi‑34B show similar late alignment spikes. Treat cosine only as a within‑model trajectory; do not compare absolute levels across families.

Lens sanity checks support conservative, rank‑centric readings for early layers. JSON `raw_lens_check.summary` flags “norm‑only semantics” for Meta‑Llama‑3‑8B and Yi‑34B: “first_norm_only_semantic_layer: 25” [L1005–L1007 in Llama‑3‑8B EVAL referencing JSON] and “first_norm_only_semantic_layer: 46” [Yi‑34B EVAL], respectively. Risk tiers: high for Gemma‑2‑9B (max_kl_norm_vs_raw_bits ≈ 12.91; mode=sample), Gemma‑2‑27B (≈ 80.10; sample), Qwen‑3‑8B (≈ 13.60; sample), Qwen‑3‑14B (≈ 17.67; sample), Qwen‑2.5‑72B (≈ 19.91; sample), Mistral‑7B‑v0.1 (≈ 1.17; sample), and Yi‑34B (≈ 80.57; sample). Low‑risk models here are Meta‑Llama‑3‑70B (≈ 0.043; sample) and Mistral‑Small‑24B‑Base‑2501 (≈ 0.179; sample). For high‑risk cases, we downgrade confidence in pre‑final “early semantics” and prefer rank milestones over raw probabilities.

Within‑family contrasts. Qwen: both 8B and 14B lack an early copy reflex and collapse semantics late (31/36 and 36/40). The 14B shows a larger Δ and ΔH (Δ̂ ≈ 0.10; ΔH ≈ +0.50 bits) than the 8B (Δ̂ = 0; ΔH ≈ 0), with a qualitatively sharper probability concentration at L_sem and a similar late cosine surge; ablation leaves L_sem unchanged in both (ΔL_sem = 0). Gemma: both 9B and 27B show early copy reflex (layer‑0 firing) and “late only” semantics at the head (42/42; 46/46), with head calibration mismatches (`warn_high_last_layer_kl = true` in JSON) producing non‑zero last‑layer KL (“kl_to_final_bits ≈ 1.01–1.14”, EVAL lines citing JSON), so we avoid cross‑family probability comparisons and stick to ranks. Ablation in 27B delays copy by +3 layers without moving semantics, echoing that the stylistic word “simply” affects copy behavior more than semantic resolution.

Link to capability. Grouping by collapse depth, the “late” cluster aligns with higher reasoning scores in several families: Qwen‑3‑14B (36/40; MMLU 66% vs 64% for 8B) and Mistral‑Small‑24B (33/40; MMLU 80.7% vs 60.1% for 7B) both move semantics later relative to their smaller counterparts. The strongest absolute model here, Llama‑3‑70B, is the exception: it collapses earlier (40/80) yet scores highly (MMLU 79.5%, ARC‑C 93.0%). Overall, later collapse correlates loosely with higher scores within families where training pipelines are shared, but scale and head calibration dominate across families. As for sharpness, a steeper ΔH does not uniformly predict higher MMLU across families: Gemma‑2‑27B’s ΔH is small/negative despite lower scores, while Qwen‑3‑14B’s larger positive ΔH coincides with its within‑family score lift.

Architecture/size factors. Across families we do not see a monotonic relation between sharper collapse and width/heads; however, within Qwen, the wider 14B (d_model 5120, 40 heads) shows a clearer separation (Δ, ΔH) than the 8B (4096, 32). Most models show KL milestones only at the very top regardless of width, indicating that calibration is finalized by the head rather than gradually along depth. These observations align with tuned‑lens reports that residual representations rotate toward the final direction while probabilistic calibration occurs late (arXiv:2303.08112).

Entropy profiles. Between L_copy and L_semantic, Gemma exhibits a “flat‑to‑rise” pattern (e.g., “entropy(L0) ≈ 0.0000 bits; entropy(L_sem) ≈ 0.37 bits” in 9B), Qwen‑3‑14B shows a “sharp plunge” (~0.82 → 0.31 bits), Qwen‑3‑8B is degenerate (same layer; ΔH ≈ 0), and larger stacks like Yi‑34B and Qwen‑2.5‑72B concentrate only at the head (final entropies ≈ 2.98 and 4.12 bits, respectively) after long flat regions dominated by punctuation. Cosine trajectories within models support this: alignment remains low/negative across mid‑stack and jumps near the head (e.g., Qwen‑3‑8B from ~−0.30 mid‑stack to ≈1.0 at L36; Llama‑3‑8B from ~0.21 at L20 to ≈1.0 at L32), as KL simultaneously collapses to ≈0 only at the final row.

Calibration caveat. Treat Gemma family as final‑lens vs final‑head mismatched: “warn_high_last_layer_kl: true” and last‑row “kl_to_final_bits ≈ 1.01–1.14” in diagnostics mean final probabilities are head‑calibrated and not directly comparable to other families. Prefer rank thresholds for cross‑model conclusions and use KL depths only within‑model.

**Misinterpretations in Existing EVALS**
- Qwen2.5‑72B: “Cosine milestones (pure CSV): first cos_to_final ≥ 0.2 at layer 1 (cos = 0.3396) … ≥ 0.4 at layer 44 (0.4324) … ≥ 0.6 at layer 51 (0.6143)” — these numbers match Yi‑34B, not Qwen‑2.5‑72B. Qwen‑2.5‑72B has cos_to_final ≈ −0.021 at L1 and only reaches ≈1.0 at L80 (check pure CSV). Quote: “first cos_to_final ≥ 0.2 at layer 1 (cos = 0.3396) …” [evaluation‑Qwen2.5‑72B.md, “Cosine milestones” paragraph].
- Qwen2.5‑72B: “Berlin enters top‑5 around L72–74 …” — in the pure CSV the answer first reaches top‑5 much later: L78 (rank 5), then L79 (rank 2), and only rank‑1 at L80. Quote: “Important‑word trajectory: ‘Berlin’ enters top‑5 around L72–74 …” [evaluation‑Qwen2.5‑72B.md].
- Qwen3‑8B: “Rest_mass … suggests no precision loss from the norm lens” — over‑stated. Rest_mass summarizes top‑k coverage, not lens fidelity; lens distortion is addressed by `raw_lens_check` (risk=high; max_kl_norm_vs_raw ≈ 13.60). Quote: “Rest_mass falls toward the end; … suggesting no precision loss from the norm lens.” [evaluation‑Qwen3‑8B.md, “Limitations & Data Quirks”].
- Minor phrasing risk (Gemma‑2‑27B): “Confidence milestones: p_top1 > 0.30 at L0; p_top1 > 0.60 at L0 …” can be read as answer confidence; here it reflects generic top‑1 under a copy reflex on “simply”. Quote: “p_top1 > 0.30 at L 0; p_top1 > 0.60 at L 0 …” [evaluation‑gemma‑2‑27b.md]. Prefer explicit “generic top‑1” vs “p_answer”.

**Limitations**
- RMS‑lens can distort absolute probabilities; comparisons should stay within‑model, not across differing normalisation schemes.
- Single‑prompt probing may over‑fit tokenizer quirks; copy‑collapse depth can change if wording or punctuation shifts.
- Attention patterns and MLP activations are not inspected—only residual projections—so entropy bumps caused by internal gating may be mis‑attributed.
- Un‑embed weights may be promoted to FP32 in some models, slightly shrinking entropy gaps; keep comparisons qualitative.
- Final‑lens vs final‑head mismatches can keep last‑layer KL > 0 for some families/precisions; prefer rank thresholds for cross‑model conclusions and treat KL trends qualitatively within model.
- Layer counts differ (8B ≈ 32 layers, 34B ≈ 60–80); compare relative depths, not absolute indices.
- Correlational only; no causal patching here. Treat Δ/Δ̂ and entropy trends as descriptive, not mechanistic.

---
Produced by OpenAI GPT-5
