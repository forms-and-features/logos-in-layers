**1. Overview**
google/gemma-2-27b (27B) was probed with a norm-lens, recording per-layer next-token predictions, entropy, KL to final, and cosine-to-final direction. Run date: 2025-08-30 18:51:32 (timestamp-20250830-1851).
The probe targets the first unseen token after â€œThe capital of Germany is called simplyâ€, using ID-level gold alignment for â€œBerlinâ€.

**2. Method Sanity-Check**
Diagnostics confirm norm-lens use (RMSNorm) and positional encoding handling: â€œuse_norm_lens: true; unembed_dtype: torch.float32; layer0_position_info: token_only_rotary_modelâ€ [JSON L807â€“L817]. The context_prompt ends with â€œcalled simplyâ€ and no trailing space: â€œGive the city name only, plain text. The capital of Germany is called simplyâ€ [JSON L817]. Copy-collapse rule is the contiguous ID-level subsequence with Ï„=0.95, k=1, margin Î´=0.10 and no entropy fallback: â€œcopy_thresh: 0.95; copy_window_k: 1; copy_match_level: id_subsequenceâ€ [JSON L823â€“L826]. Gold alignment is ID-based and OK: â€œgold_alignment: "ok"â€ [JSON L831]. Negative control is present: `control_prompt` and `control_summary` [JSON L1018â€“L1036]. Ablation block exists with both variants in CSV: â€œL_copy_orig: 0; L_sem_orig: 46; L_copy_nf: 3; L_sem_nf: 46; delta_L_copy: 3; delta_L_sem: 0â€ [JSON L1011â€“L1016]; positive rows appear for `prompt_variant=orig` and `no_filler` in the pure CSV (e.g., lines 2 and 49). Summary indices: first_kl_below_0.5 = null, first_kl_below_1.0 = null, first_rank_le_1 = 46, first_rank_le_5 = 46, first_rank_le_10 = 46 [JSON L826â€“L830]. Units: KL and entropy are in bits (field name `kl_to_final_bits`, CSV `entropy`). Final-head calibration: last-layer KLâ‰ˆ1.135 bits, not ~0, with a `last_layer_consistency` snapshot present [JSON L833â€“L851]. Lens sanity (raw vs norm): mode=sample; â€œmax_kl_norm_vs_raw_bits: 80.100â€¦; lens_artifact_risk: "high"; first_norm_only_semantic_layer: nullâ€ [JSON L949â€“L950, L1005â€“L1007].
Copy-collapse flag check (pos, orig): first `copy_collapse=True` at layer 0 with top-1 â€œ simplyâ€ (p=0.99998) vs â€œ merelyâ€ (pâ‰ˆ7.5e-06) [CSV row 2] â€” âœ“ rule satisfied.

**3. Quantitative Findings**
Gold answer: â€œBerlinâ€. Rows filtered to prompt_id=pos, prompt_variant=orig from pure-next-token CSV.
- L 0 â€“ entropy 0.000 bits, top-1 ' simply'
- L 1 â€“ entropy 8.758 bits, top-1 ''
- L 2 â€“ entropy 8.764 bits, top-1 ''
- L 3 â€“ entropy 0.886 bits, top-1 ' simply'
- L 4 â€“ entropy 0.618 bits, top-1 ' simply'
- L 5 â€“ entropy 8.520 bits, top-1 'à¹²'
- L 6 â€“ entropy 8.553 bits, top-1 'ï•€'
- L 7 â€“ entropy 8.547 bits, top-1 'î«¤'
- L 8 â€“ entropy 8.529 bits, top-1 'ïŒ'
- L 9 â€“ entropy 8.524 bits, top-1 'ğ†£'
- L 10 â€“ entropy 8.345 bits, top-1 ' dieÅ¿em'
- L 11 â€“ entropy 8.493 bits, top-1 'ğ†£'
- L 12 â€“ entropy 8.324 bits, top-1 'î«¤'
- L 13 â€“ entropy 8.222 bits, top-1 'î«¤'
- L 14 â€“ entropy 7.877 bits, top-1 'î«¤'
- L 15 â€“ entropy 7.792 bits, top-1 'î«¤'
- L 16 â€“ entropy 7.975 bits, top-1 ' dieÅ¿em'
- L 17 â€“ entropy 7.786 bits, top-1 ' dieÅ¿em'
- L 18 â€“ entropy 7.300 bits, top-1 'Å¿icht'
- L 19 â€“ entropy 7.528 bits, top-1 ' dieÅ¿em'
- L 20 â€“ entropy 6.210 bits, top-1 'Å¿icht'
- L 21 â€“ entropy 6.456 bits, top-1 'Å¿icht'
- L 22 â€“ entropy 6.378 bits, top-1 ' dieÅ¿em'
- L 23 â€“ entropy 7.010 bits, top-1 ' dieÅ¿em'
- L 24 â€“ entropy 6.497 bits, top-1 ' dieÅ¿em'
- L 25 â€“ entropy 6.995 bits, top-1 ' dieÅ¿em'
- L 26 â€“ entropy 6.220 bits, top-1 ' dieÅ¿em'
- L 27 â€“ entropy 6.701 bits, top-1 ' dieÅ¿em'
- L 28 â€“ entropy 7.140 bits, top-1 ' dieÅ¿em'
- L 29 â€“ entropy 7.574 bits, top-1 ' dieÅ¿em'
- L 30 â€“ entropy 7.330 bits, top-1 ' dieÅ¿em'
- L 31 â€“ entropy 7.565 bits, top-1 ' dieÅ¿em'
- L 32 â€“ entropy 8.874 bits, top-1 ' zuÅ¿ammen'
- L 33 â€“ entropy 6.945 bits, top-1 ' dieÅ¿em'
- L 34 â€“ entropy 7.738 bits, top-1 ' dieÅ¿em'
- L 35 â€“ entropy 7.651 bits, top-1 ' dieÅ¿em'
- L 36 â€“ entropy 7.658 bits, top-1 ' dieÅ¿em'
- L 37 â€“ entropy 7.572 bits, top-1 ' dieÅ¿em'
- L 38 â€“ entropy 7.554 bits, top-1 ' ãƒ‘ãƒ³ãƒãƒ©'
- L 39 â€“ entropy 7.232 bits, top-1 ' dieÅ¿em'
- L 40 â€“ entropy 8.711 bits, top-1 ' å±•æ¿'
- L 41 â€“ entropy 7.082 bits, top-1 ' dieÅ¿em'
- L 42 â€“ entropy 7.057 bits, top-1 ' dieÅ¿em'
- L 43 â€“ entropy 7.089 bits, top-1 ' dieÅ¿em'
- L 44 â€“ entropy 7.568 bits, top-1 ' dieÅ¿em'
- L 45 â€“ entropy 7.141 bits, top-1 ' GeÅ¿ch'
- L 46 â€“ entropy 0.118 bits, top-1 ' Berlin'

Semantic collapse (first is_answer=True at ID-level): L 46 [CSV row 48].
Ablation (no-filler): L_copy_orig = 0, L_sem_orig = 46; L_copy_nf = 3, L_sem_nf = 46; Î”L_copy = 3, Î”L_sem = 0 [JSON L1011â€“L1016]. Interpretation: removing â€œsimplyâ€ delays copy-reflex by 3 layers but does not shift semantic collapse (stylistic-cue removal affects early copying, not semantics).

Î”H (bits) = entropy(L_copy) âˆ’ entropy(L_semantic) = 0.0005 âˆ’ 0.1180 = âˆ’0.118.
Confidence milestones: p_top1 > 0.30 at L 0; p_top1 > 0.60 at L 0; final-layer p_top1 = 0.984.
Rank milestones (diagnostics): rank â‰¤ 10 at L 46; rank â‰¤ 5 at L 46; rank â‰¤ 1 at L 46 [JSON L828â€“L830].
KL milestones (diagnostics): first_kl_below_1.0 = n/a; first_kl_below_0.5 = n/a [JSON L826â€“L827]. KL decreases only at the end and is not â‰ˆ 0 at final (1.135 bits), indicating finalâ€‘head calibration; `last_layer_consistency` provided [JSON L833â€“L851].
Cosine milestones: first cos_to_final â‰¥ 0.2 at L 1; â‰¥ 0.4 at L 46; â‰¥ 0.6 at L 46; final cos_to_final = 0.9994.

**4. Qualitative Patterns & Anomalies**
The model exhibits a strong copy-reflex at layer 0 (top-1 copy â€œ simplyâ€, p=0.99998) with high margin over alternatives [CSV row 2], while semantic collapse occurs only at the final layer: â€œ Berlin, 0.984â€ with is_answer=True and answer_rank=1 [CSV row 48]. Negative control behaves correctly: for â€œBerlin is the capital ofâ€, the topâ€‘5 are â€œ Germany (0.868), the (0.065), and (0.0065), a (0.0062), Europe (0.0056)â€ â€” no leakage of â€œBerlinâ€ [JSON L10â€“L16, L18â€“L30].
Records CSV shows the expected late emergence of the answer around the final block for key slots: at L46 the next token after â€œ isâ€/â€œ calledâ€/â€œ simplyâ€ is â€œ Berlinâ€ with probabilities near 1.0 (e.g., â€œâ€¦ (â€˜Berlinâ€™, 0.999998)â€ [L804]; â€œâ€¦ (â€˜Berlinâ€™, 0.999868)â€ [L805]; â€œâ€¦ (â€˜Berlinâ€™, 0.9841)â€ [L806]). Earlier layers for these positions do not include â€œBerlinâ€ in top-5, indicating genuinely late semantics rather than early leakage.
Rest-mass sanity: rest_mass falls to 1.99eâˆ’07 by L46 (minimum overall), consistent with concentrated mass on topâ€‘k at collapse; no spikes observed after L_semantic.
Rotation vs amplification: cos_to_final grows early to ~0.33 by L1 and remains moderate, but only jumps to â‰¥0.4 at the final layer while KL stays extremely high until the end (e.g., KL 41â€“43 bits midâ€‘stack; final 1.135 bits). This is an â€œearly direction, late calibrationâ€ pattern: the representation points toward the final direction early, but probabilities calibrate only at the last layer.
Head calibration (final layer): Gemmaâ€‘family signature is present. `last_layer_consistency` reports â€œkl_to_final_bits: 1.1352; top1_agree: true; p_top1_lens: 0.9841 vs p_top1_model: 0.4226; temp_est: 2.61; kl_after_temp_bits: 0.5665â€ [JSON L833â€“L841, L850]. Treat absolute probabilities cautiously; prefer rankâ€‘based statements within/between prompts.
Lens sanity: rawâ€‘vsâ€‘norm check reports mode="sample", â€œlens_artifact_risk: high; max_kl_norm_vs_raw_bits: 80.10; first_norm_only_semantic_layer: nullâ€ [JSON L949â€“L950, L1005â€“L1007]. Caution: early semantics may be lensâ€‘induced in some families, but here the first IDâ€‘level rankâ€‘1 is at the final layer.
Temperature robustness: Not explored (empty temperature_exploration). Finalâ€‘head snapshot shows â€œ Berlin, 0.423â€ at the model head [JSON L858â€“L860], consistent with the calibration gap above.
Important-word trajectory: â€œBerlinâ€ first enters any topâ€‘5 only at L46 for positions preceding the answer slot (e.g., â€œâ€¦ â€˜ isâ€™ â†’ Berlin 0.999998; â€˜ calledâ€™ â†’ Berlin 0.999868; â€˜ simplyâ€™ â†’ Berlin 0.9841)â€ [records.csv L804â€“L806]. Earlier layers emphasize orthographic/rare tokens and nonâ€‘Germanic artifacts (e.g., L38 topâ€‘1 â€œãƒ‘ãƒ³ãƒãƒ©â€; L40 â€œå±•æ¿â€), then progressively converge toward the final direction and finally the answer token.
Stylistic ablation: Removing â€œsimplyâ€ delays copy collapse (Î”L_copy=+3) but leaves semantic collapse unchanged (Î”L_sem=0) [JSON L1011â€“L1016], suggesting the adverb provides a stylistic anchor for early copying rather than affecting factual recall.

Checklist:
- RMS lens?: Yes â€” RMSNorm detected [JSON L810â€“L815].
- LayerNorm bias removed?: n.a. (RMS model: â€œnot_needed_rms_modelâ€) [JSON L812].
- Entropy rise at unembed?: n.a. (not explicitly instrumented; rely on CSV entropies).
- FP32 un-embed promoted?: No â€” `use_fp32_unembed: false`; unembed_dtype=torch.float32 via promotion logic [JSON L808â€“L809].
- Punctuation / markup anchoring?: Some midâ€‘stack odd tokens (e.g., L38 â€œãƒ‘ãƒ³ãƒãƒ©â€, L40 â€œå±•æ¿â€) before convergence.
- Copy-reflex?: âœ“ (layer 0 copy_collapse=True) [CSV row 2].
- Grammatical filler anchoring?: Not evident (layers 0â€“5 topâ€‘1 not in {â€œisâ€, â€œtheâ€, â€œaâ€, â€œofâ€}).

Quotes
> â€œuse_norm_lensâ€¦ unembed_dtypeâ€¦ layer0_position_info: token_only_rotary_modelâ€ [JSON L807â€“L817]
> â€œL_copy: 0â€¦ L_semantic: 46â€¦ copy_thresh: 0.95â€¦ copy_window_k: 1â€¦ copy_match_level: id_subsequenceâ€ [JSON L819â€“L826]
> â€œlast_layer_consistencyâ€¦ kl_to_final_bits: 1.1352â€¦ p_top1_lens: 0.9841â€¦ p_top1_model: 0.4226â€¦ temp_est: 2.61â€ [JSON L833â€“L841, L850]
> â€œraw_lens_checkâ€¦ mode: sampleâ€¦ max_kl_norm_vs_raw_bits: 80.100â€¦ lens_artifact_risk: highâ€ [JSON L949â€“L950, L1006â€“L1007]
> â€œBerlin is the capital of â€¦ topâ€‘5: â€˜ Germanyâ€™, 0.868; â€˜ theâ€™, 0.065; â€˜ andâ€™, 0.0065 â€¦â€ [JSON L10â€“L16, L18â€“L24]
> â€œpos,orig,46â€¦ â€˜ Berlinâ€™, 0.984â€¦ is_answer=Trueâ€¦ answer_rank=1â€ [CSV row 48]
> â€œpos,orig,0â€¦ â€˜ simplyâ€™, 0.99998â€¦ â€˜ merelyâ€™, 7.5eâˆ’06â€¦ copy_collapse=Trueâ€ [CSV row 2]
> â€œrecords L46: â€¦ (â€˜Berlinâ€™, 0.999998)â€¦ (â€˜Berlinâ€™, 0.999868)â€¦ (â€˜Berlinâ€™, 0.9841)â€ [records.csv L804â€“L806]

**5. Limitations & Data Quirks**
- Finalâ€‘head calibration: lastâ€‘layer KLâ‰ˆ1.135 bits with `warn_high_last_layer_kl=true`; treat final probabilities as familyâ€‘specific; prefer rank milestones and withinâ€‘model trends [JSON L833â€“L851].
- KL is lensâ€‘sensitive; rawâ€‘vsâ€‘norm â€œlens_artifact_risk: highâ€ and sampled mode only; treat KL thresholds qualitatively [JSON L949â€“L950, L1006â€“L1007].
- Rest_mass is low at collapse (â‰ˆ2eâˆ’07) and high midâ€‘stack; no spike after L_semantic was observed in CSV, but rawâ€‘vsâ€‘norm sampling is not exhaustive.
- Temperature exploration absent; robustness across T not assessed.

**6. Model Fingerprint**
Gemmaâ€‘2â€‘27B: semantic collapse at L 46; final entropy 0.118 bits; Berlin emerges only at the last layer with strong copyâ€‘reflex at L0.

---
Produced by OpenAI GPT-5
