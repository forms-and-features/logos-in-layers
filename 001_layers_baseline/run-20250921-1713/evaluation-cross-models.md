# Cross‑Model Evaluation — Logit‑Lens Baselines (run‑latest)

## 1) Result synthesis

Across ten open‑weight models, semantic commitment (answer rank ≤ 1) generally occurs late in depth, with a single early standout. Using `diagnostics.L_semantic` and `model_stats.num_layers` to gauge relative depth: Llama‑3‑70B is early (L40/80 ≈ 50%; 001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:835,955) while most others are late (≥70%): Mistral‑7B L25/32 ≈ 78% (001_layers_baseline/run-latest/output-Mistral-7B-v0.1.json:835,955), Llama‑3‑8B L25/32 ≈ 78% (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:835,955), Mistral‑Small‑24B L33/40 ≈ 82.5% (001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:835,955), Qwen‑3‑14B L36/40 ≈ 90% (001_layers_baseline/run-latest/output-Qwen3-14B.json:835,955), Qwen‑3‑8B L31/36 ≈ 86% (001_layers_baseline/run-latest/output-Qwen3-8B.json:835,955), Yi‑34B L44/60 ≈ 73% (001_layers_baseline/run-latest/output-Yi-34B.json:835,955), Qwen‑2.5‑72B L80/80 = 100% (001_layers_baseline/run-latest/output-Qwen2.5-72B.json:835,955), Gemma‑2‑9B L42/42 = 100% (001_layers_baseline/run-latest/output-gemma-2-9b.json:835,955), Gemma‑2‑27B L46/46 = 100% (001_layers_baseline/run-latest/output-gemma-2-27b.json:835,955). This “late consolidation” pattern aligns with tuned‑lens observations that direction and calibration tend to firm up near the head (cf. Tuned Lens; arXiv:2303.08112).

Copy‑reflex (ID‑level, τ=0.95, δ=0.10) fires only in Gemma. In the pure CSVs (pos/orig), Gemma‑2‑9B shows `copy_collapse=True` in layers 0–3 (e.g., L0 row; 001_layers_baseline/run-latest/output-gemma-2-9b-pure-next-token.csv:2), and Gemma‑2‑27B shows it at L0 (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2). All other models have `copy_collapse=False` in layers 0–3 (e.g., Llama‑3‑8B L0–L3: 001_layers_baseline/run-latest/output-Meta-Llama-3-8B-pure-next-token.csv:2–5; Mistral‑7B L0–L3: 001_layers_baseline/run-latest/output-Mistral-7B-v0.1-pure-next-token.csv:2–5). Under the stricter rule used here, this isolates Gemma as a copy‑reflex outlier.

Rank milestones provide robust cross‑model comparators. Using `diagnostics.first_rank_le_{10,5,1}` (within‑model): Llama‑3‑70B hits ≤10 by L38 and 1 by L40 (001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:842–844), whereas Qwen‑2.5‑72B remains late (≤10 at L74; 1 at L80; 001_layers_baseline/run-latest/output-Qwen2.5-72B.json:842–844). Smaller/medium models like Mistral‑7B and Llama‑3‑8B collapse at ≈25/32 (001_layers_baseline/run-latest/output-Mistral-7B-v0.1.json:842–844; 001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:842–844). Treat KL milestones as within‑model diagnostics only; exclude Gemma from KL‑based conclusions due to non‑zero last‑layer KL (see below).

Within‑model direction alignment (cosine to final logits) consistently rises with depth. Examples from early vs final rows (pos/orig):
- Llama‑3‑70B: cos 0.004 at L0 and 0.99999 at L80 (001_layers_baseline/run-latest/output-Meta-Llama-3-70B-pure-next-token.csv:2,82).
- Llama‑3‑8B: cos 0.168 at L0 and ≈1.0 at L32 (001_layers_baseline/run-latest/output-Meta-Llama-3-8B-pure-next-token.csv:2,34).
- Mistral‑7B: cos −0.330 at L0 and ≈1.0 at L32 (001_layers_baseline/run-latest/output-Mistral-7B-v0.1-pure-next-token.csv:2,34).
- Qwen‑3‑14B: cos −0.137 at L0 and ≈1.0 at L40 (001_layers_baseline/run-latest/output-Qwen3-14B-pure-next-token.csv:2,42).
- Qwen‑2.5‑72B: cos 0.588 at L0 and ≈1.0 at L80 (001_layers_baseline/run-latest/output-Qwen2.5-72B-pure-next-token.csv:2,82).
- Yi‑34B: cos 0.162 at L0 and ≈1.0 at L60 (001_layers_baseline/run-latest/output-Yi-34B-pure-next-token.csv:2,62).
Do not compare absolute cosine across families; treat only as within‑model evidence that the distributional direction converges toward the final head.

Lens sanity and calibration. From `raw_lens_check.summary`, Llama‑3‑8B and Yi‑34B include a norm‑only semantic layer flag, indicating lens‑induced early semantics risk: `first_norm_only_semantic_layer: 25` in Llama‑3‑8B (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:1019) and `46` in Yi‑34B (001_layers_baseline/run-latest/output-Yi-34B.json:1019). Artifact risk spans models: low for Llama‑3‑70B and Mistral‑Small‑24B (max_kl_norm_vs_raw_bits ≈ 0.043 and 0.179; 001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:1020–1021; 001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:1020–1021), high for others (e.g., Qwen‑2.5‑72B ≈ 19.91; 001_layers_baseline/run-latest/output-Qwen2.5-72B.json:1020–1021). For cross‑model claims, prefer rank thresholds and treat KL trends qualitatively.

Final‑head vs final‑lens calibration is clean in all but Gemma. `diagnostics.last_layer_consistency.kl_to_final_bits` is near‑zero for Llama‑3‑8B (0.0; 001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:847) and Llama‑3‑70B (0.00073; 001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:847), with `warn_high_last_layer_kl=false` in both (…:864). Gemma‑2‑9B and ‑27B have ≈1.01 and ≈1.14 bits with warnings (001_layers_baseline/run-latest/output-gemma-2-9b.json:847,864; 001_layers_baseline/run-latest/output-gemma-2-27b.json:847,864). Treat Gemma final probabilities as head‑calibration artifacts; rely on rank.

Entropy shape between copy and semantic collapse. Using `L_copy` (or `L_copy_H`) and `L_semantic`:
- Gemma‑2‑9B: L_copy=0 → L_sem=42; entropy rises from 1.67e‑05 bits at L0 to 0.370 bits at L42 (ΔH = −0.370 bits; 001_layers_baseline/run-latest/output-gemma-2-9b-pure-next-token.csv:2,44). Δ̂ = (42−0)/42 = 1.00 (late).
- Gemma‑2‑27B: L_copy=0 → L_sem=46; entropy rises from 0.00050 to 0.118 bits (ΔH = −0.118 bits; 001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2,48). Δ̂ = 1.00 (late).
- Qwen‑3‑8B: L_copy_H=31, L_sem=31; entropy 0.454 bits at both (ΔH = 0; 001_layers_baseline/run-latest/output-Qwen3-8B-pure-next-token.csv:33). Δ̂ = 0.0 (collapse coincides, but overall late at 86%).
- Qwen‑3‑14B: L_copy_H=32 → L_sem=36; entropy drops 0.816 → 0.312 bits (ΔH ≈ +0.504 bits; 001_layers_baseline/run-latest/output-Qwen3-14B-pure-next-token.csv:34,38). Δ̂ = (36−32)/40 = 0.10 (late semantics, modest copy→semantic gap).
For models with `L_copy = null` (Llama/Mistral/Yi/Qwen‑2.5), Δ and ΔH are undefined under this stricter copy rule; summarize their emergence via rank milestones and relative depth instead.

KL to final within model decreases mildly with depth in most families where final KL ≈ 0, consistent with late calibration. Using the pure CSV `kl_to_final_bits` at ~25/50/75% of depth: Llama‑3‑8B 11.81/11.73/11.32 bits at L8/16/24 (001_layers_baseline/run-latest/output-Meta-Llama-3-8B-pure-next-token.csv:10,18,26); Llama‑3‑70B 10.45/10.42/10.31 bits at L20/40/60 (001_layers_baseline/run-latest/output-Meta-Llama-3-70B-pure-next-token.csv:22,42,62); Mistral‑7B 10.25/10.33/9.05 bits at L8/16/24 (001_layers_baseline/run-latest/output-Mistral-7B-v0.1-pure-next-token.csv:10,18,26). Exclude Gemma from KL‑based comparisons (final KL ≈ 1 bit; 001_layers_baseline/run-latest/output-gemma-2-9b.json:847; 001_layers_baseline/run-latest/output-gemma-2-27b.json:847).

Within‑family contrasts
- Qwen: Qwen‑3‑8B (L_sem=31/36) and Qwen‑3‑14B (36/40) both collapse late; Qwen‑2.5‑72B is latest (80/80). Artifact risk is high for all three (001_layers_baseline/run-latest/output-Qwen3-8B.json:1020–1021; output-Qwen3-14B.json:1020–1021; output-Qwen2.5-72B.json:1020–1021). Entropy around collapse is flatter for Qwen‑3‑8B (ΔH=0; 001_layers_baseline/run-latest/output-Qwen3-8B-pure-next-token.csv:33) and sharper for Qwen‑3‑14B (ΔH≈0.50 bits; 001_layers_baseline/run-latest/output-Qwen3-14B-pure-next-token.csv:34,38). Copy milestones are absent or late only (`L_copy_H` at 31/32 for Qwen‑3 models; 001_layers_baseline/run-latest/output-Qwen3-8B.json:834–835; output-Qwen3-14B.json:834–835).
- Gemma: Both sizes exhibit a strong early copy‑reflex and very late semantic collapse (Δ̂≈1.0; 001_layers_baseline/run-latest/output-gemma-2-9b.json:833–835; output-gemma-2-27b.json:833–835). Both show non‑zero final KL with warnings (≈1.01/1.14 bits; 001_layers_baseline/run-latest/output-gemma-2-9b.json:847,864; output-gemma-2-27b.json:847,864), a known family trait where the final head is calibrated differently from the lens head.

Size and capacity. Early collapse is not monotonic in width/head count: both Llama‑3‑70B (8192 dims, 64 heads; early) and Qwen‑2.5‑72B (8192, 64; late) share similar capacity (001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:956–957; output-Qwen2.5-72B.json:956–957), implying architecture/training differences dominate over raw width/head count. No consistent relation emerges between wider `d_model`/more heads and sharper entropy changes.

Link to public scores (MMLU/ARC; provided table). High scorers include Yi‑34B (MMLU 76.3), Mistral‑Small‑24B (80.7), and Llama‑3‑70B (79.5). Of these, only Llama‑3‑70B shows early collapse (50%). Yi‑34B and Mistral‑Small collapse late (73%/82.5%). Conversely, Qwen‑2.5‑72B has the highest MMLU (86.1) but the latest collapse (100%). A steep ΔH does not consistently predict higher MMLU: Qwen‑3‑14B shows a modest positive ΔH (~0.50 bits; 001_layers_baseline/run-latest/output-Qwen3-14B-pure-next-token.csv:34,38) yet lags Llama‑3‑70B on MMLU; Gemma’s ΔH is negative (entropy rises from copy to semantic; 001_layers_baseline/run-latest/output-gemma-2-9b-pure-next-token.csv:2,44) and performance is mid‑tier. Overall, collapse depth appears family‑specific rather than score‑predictive.

Prism summary across models. Prism artifacts are present and compatible for all models with k=512 (e.g., Llama‑3‑8B `present=true, compatible=true, k=512, layers=[embed,7,15,23]`; 001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:819,822–825). Within‑model, we see little to no rank milestone improvement (no `answer_rank ≤ {10,5,1}` anywhere in the Prism pure CSVs for these runs), and KL deltas concentrate as increases at mid‑depth: e.g., Mistral‑7B ΔKL ≈ +12.8/+17.5/+17.5 bits at ~25/50/75% (baseline vs Prism at L8/16/24; 001_layers_baseline/run-latest/output-Mistral-7B-v0.1-pure-next-token.csv:10,18,26 vs …-prism.csv:10,18,26), Llama‑3‑8B ΔKL ≈ +5.37/+8.29/+9.78 bits at L8/16/24 (…/output-Meta-Llama-3-8B-*.csv:10,18,26), Llama‑3‑70B ΔKL ≈ +0.89/+1.00/+1.16 bits at L20/40/60 (…/output-Meta-Llama-3-70B-*.csv:22,42,62). Some exceptions show KL reductions (Gemma‑2‑27B ≈ −22.6/−23.7/−23.1; 001_layers_baseline/run-latest/output-gemma-2-27b-*.csv:14,25,36; Qwen‑2.5‑72B ≈ −3.16/−2.83/+0.54; 001_layers_baseline/run-latest/output-Qwen2.5-72B-*.csv:22,42,62). Gains do not correlate cleanly with `lens_artifact_risk` (e.g., high‑risk Mistral‑7B increases sharply; low‑risk Llama‑3‑70B increases slightly); overall, Prism appears regressive or neutral in these runs.

## 2) Misinterpretations in existing EVALS
- Gemma‑2‑27B layer count. The EVAL lists “47 layers” (001_layers_baseline/run-latest/evaluation-gemma-2-27b.md:2) but the JSON reports `"num_layers": 46` (001_layers_baseline/run-latest/output-gemma-2-27b.json:955). Use 46 blocks for relative‑depth calculations.

## 3) Limitations
- RMS‑lens can distort absolute probabilities; prefer within‑model trends and rank thresholds, not cross‑family probabilities.
- Single‑prompt probing can over‑fit tokenizer quirks; copy‑collapse depth can shift with minor phrasing.
- Residual‑only views omit attention/MLP internals; transient entropy bumps from gating can be mis‑attributed.
- Some models promote un‑embed to FP32 (`unembed_dtype: torch.float32`), potentially shrinking entropy gaps; treat gaps qualitatively.
- Final‑lens vs final‑head mismatches keep last‑layer KL > 0 in some families/precisions (e.g., Gemma); rely on rank thresholds for cross‑model conclusions and read KL trends qualitatively within model.
- Layer counts differ (e.g., 32/40/60/80); compare relative depths, not absolute indices.
- Correlational only; causal evidence (interventions/patches) is out of scope in this run.

— These limits aside, misleading patterns can arise when: (a) early “semantics” coincide with `first_norm_only_semantic_layer` (e.g., Llama‑3‑8B at 25; 001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:1019), (b) final KL is non‑zero (Gemma; 001_layers_baseline/run-latest/output-gemma-2-9b.json:847), or (c) Prism artifacts inflate KL mid‑stack without rank gains (e.g., Mistral‑7B; 001_layers_baseline/run-latest/output-Mistral-7B-v0.1-pure-next-token.csv:10,18,26 vs …-prism.csv:10,18,26). Prefer rank milestones and relative‑depth summaries (Δ̂) for robust comparisons.

---
Produced by OpenAI GPT-5
