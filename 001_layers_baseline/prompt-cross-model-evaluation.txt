ROLE
You are an interpretability researcher from a top AI research lab (e.g. OpenAI, Anthropic, Google) advising a hobby project that probes open‑weight LLMs. You are reviewing results of probes of multiple LLM models.

INPUTS
- SCRIPT – probing script: 001_layers_baseline/run.py
- JSON – structured results (one per model): 001_layers_baseline/run-latest/output-<MODEL>.json
  Each JSON is a compact summary; per‑token/per‑layer details live in CSVs.
  Read also:
  * diagnostics.last_layer_consistency (final‑head calibration)
  * diagnostics.raw_lens_window / diagnostics.raw_lens_full
  * diagnostics.normalization_provenance, diagnostics.numeric_health, diagnostics.copy_mask
  * measurement_guidance { prefer_ranks, suppress_abs_probs, reasons[], preferred_lens_for_reporting?, use_confirmed_semantics? }
  * tuned_lens.audit_summary (if present): rotation_vs_temperature, positional, head_mismatch, tuned_is_calibration_only, preferred_semantics_lens_hint
  * evaluation_pack (if present): milestones, artifact (…v2), repeatability, alignment, norm_trajectory, entropy, tuned_audit, citations
  * summary.semantic_margin { `delta_abs`, `p_uniform`, `L_semantic_margin_ok_norm`, `margin_ok_at_L_semantic_norm`, `p_answer_at_L_semantic_norm`, `L_semantic_confirmed_margin_ok_norm`? }
  * summary.micro_suite (if present): medians/IQR, `n_missing`, notes about the fact battery
  * evaluation_pack.micro_suite (if present): per‑fact milestones, aggregates (medians/IQR), and citations to CSV rows
- CSV – layer‑level results (one per model):
  * output-<MODEL>-records.csv
  * output-<MODEL>-pure-next-token.csv
    Contains: flags (`copy_collapse`, strict sweep `copy_strict@τ` where τ∈{0.70,0.80,0.90,0.95}, `copy_soft_k{1,2,3}@τ_soft`, `entropy_collapse`, `is_answer`),
    prob/calibration (`p_top1`, `p_top5`, `p_answer`, `answer_rank`, `kl_to_final_bits`),
    geometry (`cos_to_final`, `cos_to_answer`, `cos_to_prompt_max`, `geom_crossover`),
    surface mass (`echo_mass_prompt`, `answer_mass`, `answer_minus_echo_mass`, `mass_ratio_ans_over_prompt`),
    coverage (`topk_prompt_mass@50`),
    control (`control_margin`),
    entropy (`entropy_bits`) and teacher entropy (`teacher_entropy_bits`) for drift.
    Leading columns now include `fact_key`, `fact_index`, `prompt_id` (`pos`/`ctl`), and `prompt_variant` (`orig`/`no_filler`).
- CSV – Optional sidecars per model:
  * Prism: output-<MODEL>-records-prism.csv; output-<MODEL>-pure-next-token-prism.csv
  * Tuned‑Lens: output-<MODEL>-records-tuned.csv; output-<MODEL>-pure-next-token-tuned.csv
  * Raw‑vs‑Norm (window): output-<MODEL>-pure-next-token-rawlens-window.csv (includes `fact_key`/`fact_index` for each prompt)
  * Raw‑vs‑Norm (full): output-<MODEL>-pure-next-token-rawlens.csv
    (Augmented per §1.37–1.38 with: js_divergence, kl_raw_to_norm_bits, l1_prob_diff, topk_jaccard_raw_norm@50; leading columns include `fact_key`/`fact_index`)
  * Tuned‑variants (component ablations): output-<MODEL>-pure-next-token-tuned-variants.csv  (full tuned / rotation‑only / temperature‑only)
  * Tuned positions audit: output-<MODEL>-positions-tuned-audit.csv
  * Milestones (quick citation): output-<MODEL>-milestones.csv
  * Artifact audit (quick scan): output-<MODEL>-artifact-audit.csv

Also use your own expertise in latest LLM research.

RULES
- Treat all **cosine/coverage** as within‑model trends; avoid cross‑family absolute comparisons.
- Prefer **rank milestones** and **KL thresholds** over absolute probabilities; if `suppress_abs_probs=true` or risk tier = high, avoid absolute p entirely.
- If `preferred_semantics_lens_hint` or `preferred_lens_for_reporting` is set, report semantics under that lens by default; still include baseline for context.
- If `tuned_is_calibration_only=true`, treat tuned lens as calibration aid; prefer norm lens for semantics.
- If `warn_high_last_layer_kl=true`, do not infer final‑row probability regressions; focus on rank/KL thresholds.
- Do **not** recompute JS/Jaccard/KL/entropy from scratch; read values from the provided JSON/CSVs.

Final-row caveat (head calibration): When comparing final rows across models, prefer rank/KL thresholds. If a model’s JSON has `warn_high_last_layer_kl = true`, treat final probability differences as head‑calibration artefacts and do not infer regressions. Known pattern: the Gemma family often shows non‑zero last‑layer KL due to a calibrated final head; be on the lookout for similar signatures in other families. If `raw_lens_check.summary.lens_artifact_risk` is `high` or `first_norm_only_semantic_layer` is present, downgrade confidence in pre‑final “early semantics” and lean on rank milestones over probabilities.

Normalization gate: If norm strategies differ across models (e.g., post_ln2 vs next_ln1), restrict absolute timing claims; prefer normalized depth comparisons (e.g., fraction of total layers) and within‑family statements. Annotate mixed strategies explicitly.

Also honor `measurement_guidance`:
- If `prefer_ranks = true` or `suppress_abs_probs = true`, lead with rank thresholds and avoid absolute probability claims except as within‑model qualitative trends.
- Uniform‑margin gate: treat any `L_semantic_norm` with `margin_ok_at_L_semantic_norm=false` as **weak rank‑1 (near‑uniform)**. Prefer `L_semantic_confirmed_margin_ok_norm` when present. If neither exists, report the layer as tentative and avoid strong claims.
- When `evaluation_pack.micro_suite` exists, use **medians** (and IQR when helpful) across facts for timing metrics (copy depth, semantic onset, Δ̂). Still cite at least one concrete row per model via the provided citations.
- When `summary.micro_suite` or `evaluation_pack.micro_suite` exists, rely on the **medians/IQR** to discuss robustness and call out any large `n_missing`; highlight individual fact outliers only to explain deviations (cite the fact rows).
- If `suppress_abs_probs=true` or artefact tier = high, do **not** quote numeric probabilities anywhere (including `p_answer`, `p_top1`). Use ranks/KL and qualitative phrasing (“higher/lower”) only.

Gold‑token alignment caveat: Verify `gold_alignment` across models. For any model with `unresolved`, flag reduced comparability, rely on rank thresholds (`first_rank_le_{1,5,10}`), and avoid absolute probability claims. Stylistic ablation caveat: Filter depth summaries to `prompt_id = pos`, `prompt_variant = orig`. Compare `ΔL_sem`, `ΔL_copy`, and `ΔL_copy_soft[k]` (optionally normalized by `n_layers`); treat large positive shifts as stronger stylistic‑cue sensitivity and keep comparisons within-family when in doubt. When “top‑1” does not refer to the answer, label it as generic top‑1 (not `p_answer`), and include layer indices when citing any milestones (KL, cosine, rank, probabilities).

TASK
Write CROSS‑EVAL in GitHub‑flavoured Markdown answering the items below. Use SCRIPT, model JSONs, CSVs, and EVALS (single‑model write‑ups) as necessary; avoid broad philosophical claims.

Use tools to review these JSON and CSV files as needed.

Write concise but thorough paragraphs that correlate quantitative patterns across models, drawing on your extensive knowledge of LLM interpretability research; relate them to published interpretability findings (cite DOI/arXiv).

1. Result synthesis
   Correlate quantitative patterns across models. Use:
   - `first_rank_le_{10,5,1}` and `L_semantic_confirmed` (if present) for timing, **gated by** `summary.semantic_margin` (prefer the *_margin_ok_* variants when present).
   - Δ̂ = (L_sem − L_copy_variant)/n_layers; if `evaluation_pack.micro_suite` exists, report **median Δ̂** across facts.
   - Tuned audit: `delta_kl_rot_{p25,p50,p75}` and `pos_ood_gap`; note `tuned_is_calibration_only`.
   - Head mismatch: `tau_star_modelcal` and before/after final‑KL.

2. Copy reflex (early layers 0–3)
   Use pure‑CSV flags: `copy_collapse` OR `copy_soft_k1@τ_soft`=True in layers 0–3 → mark “copy‑reflex”. Call out outliers. Normalize timing by depth.

3. Lens artefact risk
   Report:
   - From diagnostics.raw_lens_full and artifact‑audit CSV: `lens_artifact_score_v2` (and legacy score), `js_divergence_p50`, `l1_prob_diff_p50`, `jaccard_raw_norm_p50`, `pct_layers_kl_ge_1.0`, `n_norm_only_semantics_layers`, earliest norm‑only layer.
   - Risk tier (low/medium/high). For high tier, restrict to rank/KL milestones and confirmed semantics.

4. Confirmed semantics
   If `summary.L_semantic_confirmed` exists, report its layer and `confirmed_source` (raw/tuned/both).
   If `summary.semantic_margin` exists, prefer `L_semantic_confirmed_margin_ok_norm` (or annotate `L_semantic_confirmed` as weak if the margin gate fails).
   If confirmed is absent, cite `L_semantic_norm`, explicitly noting when `margin_ok_at_L_semantic_norm=false`.
   When `evaluation_pack.micro_suite` exists, summarize medians across facts and cite at least one fact row per model.

5. Entropy & confidence
   Compare entropy drift `(entropy_bits − teacher_entropy_bits)` at representative depths (use summary percentiles or evaluation_pack.entropy). Summarize whether drift shrinks as rank improves and KL falls.

6. Normalization & numeric health
   Note norm strategy (post_ln2 vs next_ln1), early spikes (resid_norm_ratio / delta_resid_cos), and any numeric health flags overlapping candidate layers.

7. Repeatability
   Quote `diagnostics.repeatability` (max_rank_dev, p95_rank_dev, top1_flip_rate); if flagged high variance, treat near‑threshold rank differences cautiously.

8. Family patterns
   Within families (e.g., Qwen, Gemma), summarize consistencies/differences in collapse depth, artefact tier, tuned audit, and control margins.

10. Prism Summary Across Models (if present)
- Treat Prism as a shared‑decoder diagnostic. Report KL deltas at the sampled depths and any rank‑milestone shifts relative to baseline norm lens. Classify as Helpful / Neutral / Regressive.

11. Investigate and write a paragraph about notable similarities and differences between models of the same family (Qwen, gemma).

13. Misinterpretations in existing EVALS
Bullet each over‑statement or error you found; cite the exact EVAL line.

OUTPUT
- Write to: 001_layers_baseline/run-latest/evaluation-cross-models.md
- Include short quotes with CSV line numbers (use milestones.csv and evaluation_pack.citations when present).

At the end of the markdown file, add the fellowing line, separated by "---" lines:
**Produced by OpenAI GPT-5**


KNOWN LIMITATIONS
These limitations are known and accepted:
* RMS-lens can distort absolute probabilities; comparisons should stay within-model, not across differing normalisation schemes.
* Single-prompt probing may over-fit tokenizer quirks; copy-collapse depth can change if wording or punctuation shifts.
* Attention patterns and MLP activations are not inspected—only residual projections—so entropy bumps caused by internal gating may be mis-attributed.
* Un-embed weights may be promoted to FP32 ("use_fp32_unembed": true) in some models, slightly shrinking entropy gaps; keep comparisons qualitative.
* Final‑lens vs final‑head mismatches can keep last‑layer KL > 0 for some families/precisions; prefer rank thresholds for cross‑model conclusions and treat KL trends qualitatively within model.
* Layer counts differ (8 B ≈ 32 layers, 34 B ≈ 48); compare relative depths, not absolute indices.
* Current results are correlation-only; causal evidence (patching) awaits a later run.
So focus on concrete reasons - these limitations aside - the present data can mislead.

STYLE GUIDELINES
- Quote lines with L-numbers; keep quotes short.  
- Entropy in CSVs is already in bits; no conversion required.  
- Cite external papers only with DOI/arXiv; skip if unsure.  
- Avoid normative language; focus on actionable analysis.
- Ground every numeric milestone (layers, KL, cosine, probabilities) with a quoted CSV/JSON line from the model being discussed, and include the layer index. Do not reuse numbers across models.
