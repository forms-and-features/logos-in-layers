**Overview**

- Model: google/gemma-2-27b (pre_norm; 46 layers). The probe analyzes the first unseen token after the prompt and records per-layer entropy, calibration (KL to final), copy/collapse flags, cosine-to-final geometry, and answer rank. Artifacts: `output-gemma-2-27b.json`, `*-pure-next-token.csv`, `*-records.csv`, plus Prism/Tuned sidecars.

**Method Sanityâ€‘Check**

The context prompt uses the intended â€œcalled simplyâ€ ending with no trailing space: â€œGive the city name only, plain text. The capital of Germany is called simplyâ€ (diagnostics.context_prompt) [001_layers_baseline/run-latest/output-gemma-2-27b.json:716]. The run applies the RMS norm lens with fp32 unembed and rotary tokenâ€‘only posâ€‘embed handling: â€œuse_norm_lens: true; unembed_dtype: torch.float32; layer0_position_info: token_only_rotary_modelâ€ [001_layers_baseline/run-latest/output-gemma-2-27b.json:784].

Strict copy detector configuration is IDâ€‘level contiguous subsequence with Ï„=0.95, k=1, Î´=0.10 (script defaults), and softâ€‘copy uses Ï„_soft=0.5 with window_ks={1,2,3} as recorded in `copy_soft_config` and mirrored in flags: â€œcopy_soft_config: { threshold: 0.5, window_ks: [1,2,3], extra_thresholds: [] }â€ and â€œcopy_flag_columns: [copy_strict@0.95, copy_soft_k1@0.5, copy_soft_k2@0.5, copy_soft_k3@0.5]â€ [001_layers_baseline/run-latest/output-gemma-2-27b.json:1358,1387]. Gold alignment is OK: `gold_answer = { string: "Berlin", pieces: ["â–Berlin"], first_id: 12514 }` and `diagnostics.gold_alignment: "ok"` [001_layers_baseline/run-latest/output-gemma-2-27b.json:1775,1348]. Negative control is present with summary margins: `control_summary = { first_control_margin_pos: 0, max_control_margin: 0.9910899400710897 }` [001_layers_baseline/run-latest/output-gemma-2-27b.json:1416]. Ablation is present: `ablation_summary = { L_copy_orig: 0, L_sem_orig: 46, L_copy_nf: 3, L_sem_nf: 46, delta_L_copy: 3, delta_L_sem: 0 }` [001_layers_baseline/run-latest/output-gemma-2-27b.json:1393].

Copyâ€‘collapse flags fire strictly at layer 0: â€œcopy_collapse=True; copy_strict@0.95=True; copy_soft_k1@0.5=Trueâ€ [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2]. âœ“ rule satisfied (token is the prompt word â€œ simplyâ€). Softâ€‘copy earliest hit: layer 0 (k=1).

Summary indices (diagnostics): `first_kl_below_0.5 = null`, `first_kl_below_1.0 = null`, `first_rank_le_1 = 46`, `first_rank_le_5 = 46`, `first_rank_le_10 = 46` [001_layers_baseline/run-latest/output-gemma-2-27b.json:904]. Units for KL/entropy are bits; the pure CSV includes `teacher_entropy_bits` for drift. Finalâ€‘head calibration check shows nonâ€‘zero lastâ€‘layer KL and a temperature mismatch: `kl_to_final_bits=1.1352`, `top1_agree=true`, `p_top1_lens=0.9841` vs `p_top1_model=0.4226`, `temp_est=2.6102`, `kl_after_temp_bits=0.5665`, `warn_high_last_layer_kl=true` [001_layers_baseline/run-latest/output-gemma-2-27b.json:966]. Treat final probabilities cautiously (known Gemma pattern).

Lens sanity (raw vs norm): sampled `raw_lens_check.summary` reports `lens_artifact_risk: "high"` with `max_kl_norm_vs_raw_bits=80.10` and `first_norm_only_semantic_layer=null` [001_layers_baseline/run-latest/output-gemma-2-27b.json:1361]. Caution about early semantics; prefer rank milestones.

**Quantitative Findings**

- Table (pos, orig). One row per layer: L i â€” entropy (bits), topâ€‘1 token. Bold = semantic layer (first `is_answer=True`).

| Layer | Entropy (bits) | Topâ€‘1 |
|---|---:|---|
| L 0 | 0.000 | ' simply' |
| L 1 | 8.758 | '' |
| L 2 | 8.764 | '' |
| L 3 | 0.886 | ' simply' |
| L 4 | 0.618 | ' simply' |
| L 5 | 8.520 | 'à¹²' |
| L 6 | 8.553 | 'ï•€' |
| L 7 | 8.547 | 'î«¤' |
| L 8 | 8.529 | 'ïŒ' |
| L 9 | 8.524 | 'ğ†£' |
| L 10 | 8.345 | ' dieÅ¿em' |
| L 11 | 8.493 | 'ğ†£' |
| L 12 | 8.324 | 'î«¤' |
| L 13 | 8.222 | 'î«¤' |
| L 14 | 7.877 | 'î«¤' |
| L 15 | 7.792 | 'î«¤' |
| L 16 | 7.975 | ' dieÅ¿em' |
| L 17 | 7.786 | ' dieÅ¿em' |
| L 18 | 7.300 | 'Å¿icht' |
| L 19 | 7.528 | ' dieÅ¿em' |
| L 20 | 6.210 | 'Å¿icht' |
| L 21 | 6.456 | 'Å¿icht' |
| L 22 | 6.378 | ' dieÅ¿em' |
| L 23 | 7.010 | ' dieÅ¿em' |
| L 24 | 6.497 | ' dieÅ¿em' |
| L 25 | 6.995 | ' dieÅ¿em' |
| L 26 | 6.220 | ' dieÅ¿em' |
| L 27 | 6.701 | ' dieÅ¿em' |
| L 28 | 7.140 | ' dieÅ¿em' |
| L 29 | 7.574 | ' dieÅ¿em' |
| L 30 | 7.330 | ' dieÅ¿em' |
| L 31 | 7.565 | ' dieÅ¿em' |
| L 32 | 8.874 | ' zuÅ¿ammen' |
| L 33 | 6.945 | ' dieÅ¿em' |
| L 34 | 7.738 | ' dieÅ¿em' |
| L 35 | 7.651 | ' dieÅ¿em' |
| L 36 | 7.658 | ' dieÅ¿em' |
| L 37 | 7.572 | ' dieÅ¿em' |
| L 38 | 7.554 | ' ãƒ‘ãƒ³ãƒãƒ©' |
| L 39 | 7.232 | ' dieÅ¿em' |
| L 40 | 8.711 | ' å±•æ¿' |
| L 41 | 7.082 | ' dieÅ¿em' |
| L 42 | 7.057 | ' dieÅ¿em' |
| L 43 | 7.089 | ' dieÅ¿em' |
| L 44 | 7.568 | ' dieÅ¿em' |
| L 45 | 7.141 | ' GeÅ¿ch' |
| **L 46** | 0.118 | ' Berlin' |

Control margin (JSON): `first_control_margin_pos = 0`, `max_control_margin = 0.9911` [001_layers_baseline/run-latest/output-gemma-2-27b.json:1416].

Ablation (noâ€‘filler): `L_copy_orig = 0`, `L_sem_orig = 46`, `L_copy_nf = 3`, `L_sem_nf = 46`, so `Î”L_copy = +3`, `Î”L_sem = 0` [001_layers_baseline/run-latest/output-gemma-2-27b.json:1393].

Î”H (bits) = entropy(L_copy) âˆ’ entropy(L_semantic) = 0.00050 âˆ’ 0.11805 = âˆ’0.1176 [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2,48].
Soft Î”Hâ‚ (k=1) = entropy(L_copy_soft[1]) âˆ’ entropy(L_semantic) = 0.00050 âˆ’ 0.11805 = âˆ’0.1176 (soft k=2,3: null) [001_layers_baseline/run-latest/output-gemma-2-27b.json:1350; 001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2,48].

Confidence milestones (pure CSV): p_top1 > 0.30 at layer 0; p_top1 > 0.60 at layer 0; final p_top1 = 0.9841 [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2,48].

Rank milestones (diagnostics): rank â‰¤ 10 at layer 46; rank â‰¤ 5 at layer 46; rank â‰¤ 1 at layer 46 [001_layers_baseline/run-latest/output-gemma-2-27b.json:904].

KL milestones (diagnostics): first_kl_below_1.0 = null; first_kl_below_0.5 = null; KL decreases to a nonâ€‘zero value at final (1.135 bits) [001_layers_baseline/run-latest/output-gemma-2-27b.json:904,966]. Finalâ€‘head calibration warning present; prefer rank statements across families.

Cosine milestones (pure CSV): cos_to_final â‰¥ 0.2 at L 1; â‰¥ 0.4 at L 46; â‰¥ 0.6 at L 46; final cos_to_final = 0.9994 [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48].

Prism Sidecar Analysis

- Presence: Prism artifacts compatible=true [001_layers_baseline/run-latest/output-gemma-2-27b.json:725].
- Earlyâ€‘depth stability (KL vs final, baselineâ†’Prism): L0 16.85â†’19.43; Lâ‰ˆ25% (L11) 41.85â†’19.43; Lâ‰ˆ50% (L23) 43.15â†’19.42; Lâ‰ˆ75% (L34) 42.51â†’19.43 bits [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2; 001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token-prism.csv:1].
- Rank milestones (Prism): no rankâ‰¤10/5/1 before final; at L46 `answer_rank=165699` (no semantics) [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token-prism.csv:48].
- Topâ€‘1 agreement at sampled depths: topâ€‘1 differs at all sampled layers (e.g., L0 ' simply' vs ' assuredly') [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2; prism:1].
- Cosine drift: Prism cos_to_final is negative at early/mid layers (e.g., L0 âˆ’0.089, L23 âˆ’0.095) vs baseline positive (~0.33), indicating a different projection geometry; no earlier stabilization [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token-prism.csv:1].
- Copy flags: baseline strict copy fires at L0; Prism `copy_collapse=False` at L0, plausibly due to Prism transform changing the local topâ€‘1 neighborhood and probabilities [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2; prism:1].
- Verdict: Regressive for rank/semantics (no earlier `first_rankâ‰¤1` and altered surface behavior), though it substantially lowers KL at midâ€‘depths (~âˆ’22 bits).

**Qualitative Patterns & Anomalies**

Negative control (â€œBerlin is the capital ofâ€): topâ€‘5 are â€œ Germany (0.8676), the (0.0650), and (0.0065), a (0.0062), Europe (0.0056)â€ â€” Berlin does not appear (no leakage) [001_layers_baseline/run-latest/output-gemma-2-27b.json:6]. For stylistic rephrasings (e.g., â€œGermanyâ€™s capital city is called simplyâ€), Berlin is topâ€‘1 with pâ‰ˆ0.52â€“0.62 [001_layers_baseline/run-latest/output-gemma-2-27b.json:208,231,287].

Importantâ€‘word trajectory (records): early positions are dominated by literal/punctuation/filler tokens (e.g., pos 2: â€œ theâ€ pâ‰ˆ0.9999; pos 3: â€œ cityâ€ pâ‰ˆ0.996; pos 12: â€œ ofâ€ pâ‰ˆ0.99999) [001_layers_baseline/run-latest/output-gemma-2-27b-records.csv:3,4,13]. At NEXT, L0 predicts the copy token â€œ simplyâ€ with p=0.99998, while Berlin only becomes topâ€‘1 at the final head (L46, p=0.9841) [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2,48]. Midâ€‘stack NEXT topâ€‘1s frequently include nonâ€‘semantic artifacts (e.g., â€œ dieÅ¿emâ€, symbols, multilingual tokens), with `cos_to_final` rising gradually (â‰¥0.2 by L1) while KL stays very high, indicating â€œearly direction, late calibration.â€

Oneâ€‘word instruction ablation: removing â€œsimplyâ€ delays/relocates copy from L0â†’L3 (Î”L_copy=+3) but leaves semantics unchanged (L_semantic=46 both) [001_layers_baseline/run-latest/output-gemma-2-27b.json:1393]. This suggests stylistic anchoring affects surface reflexes, not answer emergence.

Restâ€‘mass sanity: For NEXT, rest_mass is tiny when the model is confident (L0 ~4.8eâˆ’06; L46 ~2.0eâˆ’07), and large during highâ€‘entropy midâ€‘layers (e.g., L29 rest_mass ~0.86), falling again near the final [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2,47,48]. No postâ€‘final spikes observed.

Rotation vs amplification: KL to final remains large across most depths and is not â‰ˆ0 at the final head (1.135 bits). Yet `cos_to_final` steadily increases to 0.9994 at L46, and `answer_rank` flips to 1 only at L46, consistent with meaningful direction emerging late relative to calibration. Finalâ€‘head calibration warning: `temp_est=2.6102`, `kl_after_temp=0.5665` [001_layers_baseline/run-latest/output-gemma-2-27b.json:966]. Given `raw_lens_check.summary.lens_artifact_risk="high"` with `max_kl_norm_vs_raw_bits=80.10` and `first_norm_only_semantic_layer=null` [001_layers_baseline/run-latest/output-gemma-2-27b.json:1361], treat any preâ€‘final â€œsemanticsâ€ as lensâ€‘sensitive and favor ranks.

Temperature robustness: At T=0.1, Berlin rank 1 with p=0.9898 and entropy=0.082 bits; at T=2.0, Berlin still rank 1 with p=0.0492 and entropy=12.631 bits [001_layers_baseline/run-latest/output-gemma-2-27b.json:670,737].

Checklist
- RMS lens? âœ“ [001_layers_baseline/run-latest/output-gemma-2-27b.json:778]
- LayerNorm bias removed? n.a. (RMS) [001_layers_baseline/run-latest/output-gemma-2-27b.json:776]
- Entropy rise at unembed? âœ“ (midâ€‘stack entropies 6â€“9 bits; final 0.118) [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:33,48]
- FP32 unâ€‘embed promoted? âœ“ (`unembed_dtype: torch.float32`) [001_layers_baseline/run-latest/output-gemma-2-27b.json:772]
- Punctuation / markup anchoring? âœ“ (multiple filler/punct topâ€‘1s midâ€‘stack) [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:29â€“41]
- Copyâ€‘reflex? âœ“ (strict/soft at L0) [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2]
- Grammatical filler anchoring? âœ“ (â€œisâ€, â€œtheâ€, punctuation dominate early positions) [001_layers_baseline/run-latest/output-gemma-2-27b-records.csv:2,9,14]

**Limitations & Data Quirks**

- Finalâ€‘head calibration is off for Gemmaâ€‘2â€‘27B (`warn_high_last_layer_kl=true; kl_to_final_bits=1.135; temp_est=2.610; kl_after_temp=0.567`), so treat final probabilities as familyâ€‘specific; rely on rank and withinâ€‘model trends [001_layers_baseline/run-latest/output-gemma-2-27b.json:966].
- `raw_lens_check` is `mode: sample` and flags `lens_artifact_risk: high` with very large `max_kl_norm_vs_raw_bits` (80.10), so early â€œsemanticsâ€ may be lensâ€‘induced; crossâ€‘lens differences are advisory [001_layers_baseline/run-latest/output-gemma-2-27b.json:1325].
- Surfaceâ€‘mass metrics depend on tokenizer; absolute masses are not comparable across families; use withinâ€‘model trends.

**Model Fingerprint**

â€œGemmaâ€‘2â€‘27B: collapse at Lâ€¯46; final entropy 0.118â€¯bits; â€˜Berlinâ€™ appears only at the final head.â€

---
Produced by OpenAI GPT-5 

