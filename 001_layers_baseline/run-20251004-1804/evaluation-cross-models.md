**Cross‑Model Evaluation (Logit‑Lens Baseline)**

1. Result synthesis

Across models, semantic collapse generally arrives late in depth, with family‑ and size‑specific deviations. Using rank milestones and confirmed semantics per measurement guidance, most models fall into the “late (≥70% depth)” bucket: Qwen3‑8B (L_sem=31/36 ⇒ 0.861) [001_layers_baseline/run-latest/output-Qwen3-8B.json:7178], Qwen3‑14B (36/40 ⇒ 0.90) [001_layers_baseline/run-latest/output-Qwen3-14B.json:7182], Qwen2.5‑72B (80/80 ⇒ 1.0) [001_layers_baseline/run-latest/output-Qwen2.5-72B.json:7354], Mistral‑7B (25/32 ⇒ 0.781) [001_layers_baseline/run-latest/output-Mistral-7B-v0.1.json:2241], Mistral‑Small‑24B (33/40 ⇒ 0.825) [001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:4001], Yi‑34B (44/60 ⇒ 0.733) [001_layers_baseline/run-latest/output-Yi-34B.json:2603], Gemma‑2‑9B (42/42 ⇒ 1.0) [001_layers_baseline/run-latest/output-gemma-2-9b.json:5740], Gemma‑2‑27B (46/46 ⇒ 1.0) [001_layers_baseline/run-latest/output-gemma-2-27b.json:5744]. A notable “early (<70%)” exception is Llama‑3‑70B (40/80 ⇒ 0.50) [001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:7131], consistent with stronger reasoning performance (MMLU 79.5% vs 62.1% for Llama‑3‑8B). Within‑family trends are mixed: Qwen 14B (0.90) is later than Qwen 8B (0.861) despite higher MMLU (66% vs 64%), while Llama scales earlier (70B at 0.50 vs 8B at 0.781). Overall, earlier normalized timing correlates with stronger scores in Llama but not reliably across families; treat this as suggestive, not causal (cf. logit‑lens caveats in Olsson et al., arXiv:2207.05221; Belrose et al., arXiv:2301.05217).

Copy‑reflex (layers 0–3) appears only in Gemma. We flag copy‑reflex when either `copy_collapse` or `copy_soft_k1@τ_soft` is True in layers 0–3 of the pure CSV. Gemma‑2‑9B has strict copy at L0 with ‘ simply’ (p≈0.9999993) [001_layers_baseline/run-latest/output-gemma-2-9b-pure-next-token.csv:2], and Gemma‑2‑27B likewise shows L0 copy collapse (p≈0.999976) [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2]. All others show no early strict/soft hits (e.g., Qwen3‑8B L0–3 False [001_layers_baseline/run-latest/output-Qwen3-8B-pure-next-token.csv:2], Llama‑3‑8B L0–3 False [001_layers_baseline/run-latest/output-Meta-Llama-3-8B-pure-next-token.csv:2–5], Mistral‑7B L0–3 False [001_layers_baseline/run-latest/output-Mistral-7B-v0.1-pure-next-token.csv:2–5]). Outlier: the Gemma family exhibits a strong early copy reflex.

Δ and Δ̂ timing. When strict `L_copy_strict` exists, we report Δ = L_semantic − L_copy and Δ̂ = Δ / n_layers; otherwise we use earliest `L_copy_soft[k]`. The Gemma family has L_copy=0 and L_semantic at the final layer: Gemma‑2‑9B Δ=42, Δ̂≈1.0 [001_layers_baseline/run-latest/output-gemma-2-9b.json:5636–5643]; Gemma‑2‑27B Δ=46, Δ̂≈1.0 [001_layers_baseline/run-latest/output-gemma-2-27b.json:5640–5649, 5658–5689]. For the other families, strict and soft detectors do not fire (e.g., Qwen3‑8B shows all‑null strict/soft with stability="none" [001_layers_baseline/run-latest/output-Qwen3-8B.json:7132–7159]). No consistent Δ̂–vs–width/heads pattern emerges: Llama‑3‑70B (d_model=8192, n_heads=64) is “early” (0.50) [001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:8486–8493, 7132], while Qwen2.5‑72B (8192/64) collapses only at the last layer (1.0) [001_layers_baseline/run-latest/output-Qwen2.5-72B.json:8690–8698, 7354].

Entropy shape between L_copy and L_semantic (pure NEXT). For Gemma‑2‑27B, entropy rises from near‑zero at L0 (H≈0.00050) to ≈0.118 bits at L46 [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2,48]. Gemma‑2‑9B similarly rises from ≈1.7e‑05 at L0 to ≈0.370 bits at L42 [001_layers_baseline/run-latest/output-gemma-2-9b-pure-next-token.csv:2,49]. For models without copy triggers, ΔH is n.a. (no L_copy); we report mid‑depth entropy drift against the teacher as a proxy: Llama‑3‑8B L≈16 shows entropy − teacher ≈13.89 bits [001_layers_baseline/run-latest/output-Meta-Llama-3-8B-pure-next-token.csv:20]; Qwen3‑8B L≈18 drift ≈13.79 bits [001_layers_baseline/run-latest/output-Qwen3-8B-pure-next-token.csv:20]; Qwen3‑14B L≈20 drift ≈13.35 bits [001_layers_baseline/run-latest/output-Qwen3-14B-pure-next-token.csv:22]; Mistral‑7B L≈16 drift ≈10.99 bits [001_layers_baseline/run-latest/output-Mistral-7B-v0.1-pure-next-token.csv:18]; Mistral‑Small‑24B L≈20 drift ≈13.56 bits [001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501-pure-next-token.csv:22]; Yi‑34B L≈24 drift ≈12.61 bits [001_layers_baseline/run-latest/output-Yi-34B-pure-next-token.csv:26]; Qwen2.5‑72B L≈40 drift ≈12.06 bits [001_layers_baseline/run-latest/output-Qwen2.5-72B-pure-next-token.csv:98]. These mid‑depth drifts shrink substantially under tuned lenses (see Tuned‑Lens summary), indicating improved calibration/translation.

Lens sanity and calibration. We lead with ranks due to lens sensitivities. Models with `warn_high_last_layer_kl=true` (Gemma‑2‑9B KL≈1.013 bits; Gemma‑2‑27B KL≈1.135 bits) show final‑head vs final‑lens calibration mismatches; use ranks not absolute p at the final row [001_layers_baseline/run-latest/output-gemma-2-9b.json:6401–6410; 001_layers_baseline/run-latest/output-gemma-2-27b.json:6461–6470]. Raw‑vs‑norm windows flag norm‑only semantics near claimed collapses for: Llama‑3‑8B (layers 25, 27–30; max window KL≈5.26 bits; tier=medium) [001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:7095–7119, 7168–7179], Llama‑3‑70B (79–80; ≈1.24 bits; tier=medium) [001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:7164–7180], Qwen3‑8B (high risk; max window KL≈13.60 bits) [001_layers_baseline/run-latest/output-Qwen3-8B.json:8294–8311], Qwen3‑14B (high risk; see sample/full summaries) [001_layers_baseline/run-latest/output-Qwen3-14B.json:8311–8328, 7214–7226], Qwen2.5‑72B (high risk; window max KL≈83.32 bits; earliest norm‑only semantics=80) [001_layers_baseline/run-latest/output-Qwen2.5-72B.json:7361–7388], Yi‑34B (high risk; window max KL≈90.47 bits; n_norm_only_semantics=14) [001_layers_baseline/run-latest/output-Yi-34B.json:2610–2656, 2644–2656], Mistral‑7B (high risk; window max KL≈8.56 bits; earliest norm‑only=32) [001_layers_baseline/run-latest/output-Mistral-7B-v0.1.json:2248–2280]. Mistral‑Small‑24B is notably cleaner (window max ≈5.98 bits; tier=low) [001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:4008–4045]. In line with prior work (e.g., nostalgebraist logit lens; arXiv:2305.01605), we interpret “early direction, late calibration”: cosines rise before KL falls.

Surface/geometry/coverage snapshots (norm, within‑model). At the surface→meaning boundary, answer mass exceeds echo mass across families: Llama‑3‑8B L_surface=32 with answer_mass≈0.520 and echo≈0.024 [001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:7065–7068]; Llama‑3‑70B L_surface=80 with answer_mass≈0.478 and echo≈0.0048 [001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:7113–7116]; Qwen3‑8B L_surface=31 with answer_mass≈0.936 and echo≈0.011 [001_layers_baseline/run-latest/output-Qwen3-8B.json:7159–7161]; Qwen3‑14B L_surface=36 with answer_mass≈0.953 and echo≈4.39e‑06 [001_layers_baseline/run-latest/output-Qwen3-14B.json:7163–7166]; Mistral‑7B L_surface=32 with answer_mass≈0.382 and echo≈0.061 [001_layers_baseline/run-latest/output-Mistral-7B-v0.1.json:2222–2224]; Mistral‑Small‑24B L_surface=40 with answer_mass≈0.455 and echo≈0.0042 [001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:3982–3984]; Yi‑34B L_surface=51 with answer_mass≈0.060 and echo≈0.0063 [001_layers_baseline/run-latest/output-Yi-34B.json:2584–2586]. Cosine milestones corroborate within‑model alignment increases (e.g., Llama‑3‑8B reaches cos≥0.6 by L32 [001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:7076–7083]); avoid cross‑family cosine comparisons.

Family similarities/differences. Qwen3‑8B and Qwen3‑14B both collapse very late (0.861 vs 0.90) with clean final heads (KL≈0) [001_layers_baseline/run-latest/output-Qwen3-8B.json:7756–7764; 001_layers_baseline/run-latest/output-Qwen3-14B.json:7816–7832]. Tuned lenses improve ranks modestly in both (Δle_1 ≈ +3) with large KL drops (see below). Gemma 9B/27B show strong early copy and calibrated final heads that disagree with the lens (warn_high_last_layer_kl=true), pushing L_semantic to the final layer. Mistral‑Small‑24B shows better lens faithfulness (low tier), while Mistral‑7B shows higher artefact risk (high tier). Llama aligns earlier at scale (70B vs 8B), but cosines at mid‑depth remain modest even as ranks improve, matching “early direction, late calibration.”

Prism Summary Across Models (diagnostic only). We gate “Helpful” as earlier/equal first_rank_le_1 and no KL inflation >0.2 bits at `~50%`. All compatible prisms in this run are Neutral or Regressive. Llama‑3‑8B has no rank milestones and higher KL under Prism at p50 (Δ≈−8.29 bits) [001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:860–906]; Regressive. Mistral‑7B similar (p50 Δ≈−17.54 bits) [001_layers_baseline/run-latest/output-Mistral-7B-v0.1.json:830–906]; Regressive. Qwen3‑8B (p50 Δ≈−0.59 bits) [001_layers_baseline/run-latest/output-Qwen3-8B.json:830–910]; Regressive. Qwen3‑14B (p50 Δ≈−0.25 bits) [001_layers_baseline/run-latest/output-Qwen3-14B.json:830–910]; Regressive. Yi‑34B shows mixed deltas (p50 Δ≈+1.36 bits, i.e., worse) with null ranks [001_layers_baseline/run-latest/output-Yi-34B.json:830–910]; Regressive. Gemma‑2‑9B and Gemma‑2‑27B show large KL shifts vs very high‑KL baselines (p50 Δ≈−10.33 and +23.73 bits, respectively) without rank milestones [001_layers_baseline/run-latest/output-gemma-2-9b.json:830–910; 001_layers_baseline/run-latest/output-gemma-2-27b.json:830–910]; Neutral/diagnostic at best. Llama‑3‑70B has Prism present but no rank improvements and small KL inflation (~+0.25 to +0.89 bits in text table); classify Regressive [001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:830–910]. Patterns: KL inflations concentrate at early/mid layers and are larger when lens‑artifact risk is high (Qwen, Gemma, Mistral‑7B), supporting Prism’s use as a robustness check rather than a translator.

Tuned‑Lens Summary Across Models (within‑model; preferred lens honored). We compare tuned vs norm and attribute ΔKL to temperature vs rotation using `kl_to_final_bits_norm_temp` (ΔKL_rot = ΔKL_tuned − ΔKL_temp). The models with sidecars show consistent rank improvements and KL reductions:
- Llama‑3‑8B (preferred=tuned): Δ first_rank_le_{10,5,1} = {+8,+7,+7}; ΔKL_tuned@{25,50,75}% ≈ {4.35, 4.31, 3.95} bits with ΔKL_rot ≈ {4.53, 4.54, 3.73} (positive rotation gains) [001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:8637–8660]. Last‑layer agreement is exact (KL≈0) [001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:7608–7624]. Mid‑depth entropy drift shrinks accordingly.
- Qwen3‑8B (preferred=tuned): Δ first_rank_le_{10,5,1} ≈ {+1,+2,+3}; ΔKL_tuned ≈ {4.14, 4.00, 1.40}; ΔKL_rot ≈ {4.66, 4.08, 2.64} [001_layers_baseline/run-latest/output-Qwen3-8B.json:8786–8898].
- Qwen3‑14B (preferred=tuned): Δ first_rank_le_{10,5,1} ≈ {+1,+1,+3}; ΔKL_tuned ≈ {4.68, 4.49, 3.90}; ΔKL_rot ≈ {4.76, 4.72, 3.66} [001_layers_baseline/run-latest/output-Qwen3-14B.json:8842–8916].
- Mistral‑7B (preferred=tuned): Δ first_rank_le_{10,5,1} ≈ {+2, 0, +7}; ΔKL_tuned ≈ {4.03, 3.75, 7.08}; ΔKL_rot ≈ {4.27, 4.03, 2.85} [001_layers_baseline/run-latest/output-Mistral-7B-v0.1.json:3790–3868].
- Mistral‑Small‑24B (preferred=tuned): Δ first_rank_le_{10,5,1} ≈ {+4,+5,+6}; ΔKL_tuned ≈ {4.19, 4.59, 5.35}; ΔKL_rot ≈ {3.87, 4.44, 11.50} [001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:5671–5769].
- Yi‑34B (preferred=tuned): Δ first_rank_le_{10,5,1} ≈ {+1, 0, +2}; ΔKL_tuned ≈ {5.88, 6.10, 8.39}; ΔKL_rot ≈ {4.97, 4.82, 4.00} [001_layers_baseline/run-latest/output-Yi-34B.json:4580–4672].
- Gemma‑2‑9B (preferred=norm): Ranks unchanged; ΔKL_tuned is negative at p50 (−10.52 bits), indicating regressions; prefer norm (and ranks) [001_layers_baseline/run-latest/output-gemma-2-9b.json:7360–7530].
- Gemma‑2‑27B (preferred=norm): Ranks unchanged; modest ΔKL_tuned (≤0.64 bits) with ΔKL_rot <0 across percentiles (pure calibration effects) [001_layers_baseline/run-latest/output-gemma-2-27b.json:7528–7588].

We honor each model’s `measurement_guidance.preferred_lens_for_reporting` (e.g., tuned for Llama‑3‑8B/Qwen/Mistral/Yi; norm for Gemma and Llama‑3‑70B/Qwen2.5‑72B) [001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:8701–8720; 001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:8606–8616; 001_layers_baseline/run-latest/output-Qwen2.5-72B.json:8816–8858].

Copy robustness (strict sweep; soft k1@0.5). Strict thresholds show stability="none" for most families (e.g., Llama‑3‑8B [001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:7020–7062], Mistral‑Small‑24B [001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:3955–3988]); Gemma has stability="mixed" with L_copy_strict=0 at all τ∈{0.70,0.95} [001_layers_baseline/run-latest/output-gemma-2-9b.json:5688–5725], reinforcing early copy. No models show `norm_only_flags[τ]=true` in the strict sweep, suggesting copy detection itself is not an artefact at these τ.

Linking collapse depth to public scores (informal). Within Llama, earlier normalized collapse (70B at 0.50) aligns with much higher MMLU/ARC than 8B (0.781), hinting at a size‑driven head‑calibration advantage. Within Qwen, 14B collapses later than 8B despite stronger MMLU, undercutting a simple monotonic relation. Across families, correlations do not hold (e.g., Qwen2.5‑72B collapses only at the last layer yet has the highest MMLU). Treat Δ̂ and normalized depth as descriptive, not predictive.

2. Misinterpretations in existing EVALS

- Qwen2.5‑72B: The EVAL states “control_summary is null” [001_layers_baseline/run-latest/evaluation-Qwen2.5-72B.md:25,116], but JSON records a populated control summary with `first_control_margin_pos=0` and `max_control_margin≈0.207` [001_layers_baseline/run-latest/output-Qwen2.5-72B.json:8806–8814].
- Minor emphasis drift: Some EVALs read absolute probabilities at collapse layers despite guidance to prefer ranks under high lens‑artifact risk (e.g., Qwen3‑14B highlights p≈0.953 at L36 [001_layers_baseline/run-latest/output-Qwen3-14B-pure-next-token.csv:38] while the model’s own head shows lower calibrated p at the final layer [001_layers_baseline/run-latest/output-Qwen3-14B.json:7816–7832]). This is acceptable narration but should be framed explicitly as within‑model, rank‑first.

3. Limitations

- RMS‑lens can distort absolute probabilities; keep comparisons within‑model and avoid cross‑family normalization differences.
- Single‑prompt probing may over‑fit tokenizer quirks; copy‑collapse depth can shift with wording/punctuation.
- Residual‑stream decoding only: attention/MLP dynamics are unobserved; entropy bumps from gating may be mis‑attributed.
- Some models promote FP32 un‑embed; this can slightly shrink entropy gaps; treat gaps qualitatively.
- Final‑lens vs final‑head mismatches (e.g., Gemma) keep last‑layer KL > 0; prefer ranks and qualitative KL trends.
- Layer counts differ widely; compare normalized depths, not raw indices.
- Correlation‑only: causal evidence (patching) awaits a later run.

---
Produced by OpenAI GPT-5

