# Evaluation Report: google/gemma-2-27b

*Run executed on: 2025-10-04 18:04:23*
**Overview**
This evaluation covers google/gemma-2-27b (46 layers), run artifacts dated 2025-10-04, analyzing layer-by-layer behavior on â€œGermany â†’ Berlinâ€ with a norm lens plus sidecars (Prism, Tuned-Lens). The probe records copy/filler reflexes, semantic collapse (rank milestones), KL-to-final alignment, cosine trajectories, and calibration diagnostics.

**Method Sanity-Check**
The context prompt ends with â€œcalled simplyâ€ (no trailing space) in both script and run: "context_prompt = \"â€¦ called simply\""  (001_layers_baseline/run.py:254) and "context_prompt": "Give the city name only, plain text. The capital of Germany is called simply"  (001_layers_baseline/run-latest/output-gemma-2-27b.json:4). Norm lens is enabled and used: "use_norm_lens": true (001_layers_baseline/run-latest/output-gemma-2-27b.json:808). Last-layer head calibration exists with non-zero KL and temperature fit: "kl_to_final_bits": 1.1352, "temp_est": 2.6102, "kl_after_temp_bits": 0.5665, "warn_high_last_layer_kl": true (001_layers_baseline/run-latest/output-gemma-2-27b.json:6462,6468-6470,6479). Measurement guidance: "prefer_ranks": true, "suppress_abs_probs": true; reasons include "warn_high_last_layer_kl", "norm_only_semantics_window", and "high_lens_artifact_risk" (001_layers_baseline/run-latest/output-gemma-2-27b.json:7566-7573). Copy detector configuration and flags are present and consistent across JSON/CSV: copy strict at Ï„ âˆˆ {0.95,0.7,0.8,0.9} and soft kâˆˆ{1,2,3} at Ï„_soft=0.5 are listed under "copy_flag_columns" (001_layers_baseline/run-latest/output-gemma-2-27b.json:7033-7042) and appear in the pure CSV header (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:1). Summary indices exist: "first_rank_le_1": 46, "first_rank_le_5": 46, "first_rank_le_10": 46; "first_kl_below_1.0": null; "first_kl_below_0.5": null (001_layers_baseline/run-latest/output-gemma-2-27b.json:5645-5649). Raw-vs-norm lens window/full checks report high artifact risk and norm-only semantics at the final layer: "max_kl_norm_vs_raw_bits_window": 99.54, "norm_only_semantics_layers": [46] (001_layers_baseline/run-latest/output-gemma-2-27b.json:5769-5773) and full summary "lens_artifact_score": 0.9872, tier "high" (001_layers_baseline/run-latest/output-gemma-2-27b.json:5775-5784). Normalizer provenance shows pre-norm strategy with next ln1 and correct epsilon placement: "arch": "pre_norm", "strategy": "next_ln1" (001_layers_baseline/run-latest/output-gemma-2-27b.json:5788-5789), with layer-0 ln_source "blocks[0].ln1" and final "ln_final" (001_layers_baseline/run-latest/output-gemma-2-27b.json:5792-5793,6160-6161). Per-layer normalizer effect metrics are present and stable (e.g., layer 0: "resid_norm_ratio": 0.7865, "delta_resid_cos": 0.5722; final: 0.0527, 0.6617) (001_layers_baseline/run-latest/output-gemma-2-27b.json:5796,6164). Unembedding bias is absent: "present": false, l2_norm=0 (001_layers_baseline/run-latest/output-gemma-2-27b.json:826-830). Environment/determinism: torch 2.8.0, device=cpu, deterministic_algorithms=true, seed=316 (001_layers_baseline/run-latest/output-gemma-2-27b.json:7019-7033). Numeric health is clean (no NaN/Inf) (001_layers_baseline/run-latest/output-gemma-2-27b.json:6426-6432). Copy mask is present with plausible ignored tokens (whitespace runs), size 4668 (001_layers_baseline/run-latest/output-gemma-2-27b.json:5618-5636). Gold alignment is OK (ID-based): "gold_alignment": "ok" (001_layers_baseline/run-latest/output-gemma-2-27b.json:6460). Control summary present: "first_control_margin_pos": 0, "max_control_margin": 0.9911 (001_layers_baseline/run-latest/output-gemma-2-27b.json:7065-7069). Ablation summary present with both variants: L_copy_orig=0, L_sem_orig=46, L_copy_nf=3, L_sem_nf=46 (001_layers_baseline/run-latest/output-gemma-2-27b.json:7043-7047).

Copy-reflex evidence: in the pure CSV, layer 0 shows copy_collapse=True with strict flags at all Ï„ and soft k1=True: "copy_collapse,copy_strict@0.95,copy_strict@0.7,copy_strict@0.8,copy_strict@0.9,copy_soft_k1@0.5=True" (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2). âœ“ rule satisfied for early copy-reflex.

**Quantitative Findings**
Table (prompt_id=pos, prompt_variant=orig). For each layer L: entropy (bits) and topâ€‘1 token at NEXT. Bold marks semantic collapse (rankâ‰¤1) under the norm lens.

- L 0 â€“ entropy 0.0005 bits, topâ€‘1 ' simply' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2)
- L 1 â€“ entropy 8.7582 bits, topâ€‘1 '' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:3)
- L 2 â€“ entropy 8.7645 bits, topâ€‘1 '' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:4)
- L 3 â€“ entropy 0.8857 bits, topâ€‘1 ' simply' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:5)
- L 4 â€“ entropy 0.6183 bits, topâ€‘1 ' simply' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:6)
- L 5 â€“ entropy 8.5203 bits, topâ€‘1 'à¹²' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:7)
- L 6 â€“ entropy 8.5531 bits, topâ€‘1 'ï•€' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:8)
- L 7 â€“ entropy 8.5470 bits, topâ€‘1 'î«¤' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:9)
- L 8 â€“ entropy 8.5287 bits, topâ€‘1 'ïŒ' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:10)
- L 9 â€“ entropy 8.5238 bits, topâ€‘1 'ğ†£' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:11)
- L 10 â€“ entropy 8.3452 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:12)
- L 11 â€“ entropy 8.4928 bits, topâ€‘1 'ğ†£' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:13)
- L 12 â€“ entropy 8.3244 bits, topâ€‘1 'î«¤' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:14)
- L 13 â€“ entropy 8.2225 bits, topâ€‘1 'î«¤' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:15)
- L 14 â€“ entropy 7.8766 bits, topâ€‘1 'î«¤' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:16)
- L 15 â€“ entropy 7.7925 bits, topâ€‘1 'î«¤' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:17)
- L 16 â€“ entropy 7.9748 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:18)
- L 17 â€“ entropy 7.7856 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:19)
- L 18 â€“ entropy 7.2999 bits, topâ€‘1 'Å¿icht' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:20)
- L 19 â€“ entropy 7.5278 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:21)
- L 20 â€“ entropy 6.2100 bits, topâ€‘1 'Å¿icht' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:22)
- L 21 â€“ entropy 6.4560 bits, topâ€‘1 'Å¿icht' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:23)
- L 22 â€“ entropy 6.3784 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:24)
- L 23 â€“ entropy 7.0104 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:25)
- L 24 â€“ entropy 6.4970 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:26)
- L 25 â€“ entropy 6.9949 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:27)
- L 26 â€“ entropy 6.2198 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:28)
- L 27 â€“ entropy 6.7007 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:29)
- L 28 â€“ entropy 7.1401 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:30)
- L 29 â€“ entropy 7.5741 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:31)
- L 30 â€“ entropy 7.3302 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:32)
- L 31 â€“ entropy 7.5652 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:33)
- L 32 â€“ entropy 8.8736 bits, topâ€‘1 ' zuÅ¿ammen' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:34)
- L 33 â€“ entropy 6.9447 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:35)
- L 34 â€“ entropy 7.7383 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:36)
- L 35 â€“ entropy 7.6507 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:37)
- L 36 â€“ entropy 7.6577 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:38)
- L 37 â€“ entropy 7.5724 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:39)
- L 38 â€“ entropy 7.5536 bits, topâ€‘1 'ãƒ‘ãƒ³ãƒãƒ©' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:40)
- L 39 â€“ entropy 7.2324 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:41)
- L 40 â€“ entropy 8.7105 bits, topâ€‘1 'å±•æ¿' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:42)
- L 41 â€“ entropy 7.0817 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:43)
- L 42 â€“ entropy 7.0565 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:44)
- L 43 â€“ entropy 7.0889 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:45)
- L 44 â€“ entropy 7.5683 bits, topâ€‘1 ' dieÅ¿em' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:46)
- L 45 â€“ entropy 7.1406 bits, topâ€‘1 ' GeÅ¿ch' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:47)
- L 46 â€“ entropy 0.1180 bits, topâ€‘1 ' Berlin' (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48)

Bold collapse layer: L_semantic_norm = 46, confirmed (tuned) with use_confirmed_semantics=true (001_layers_baseline/run-latest/output-gemma-2-27b.json:6840-6847,7576-7578).

Control margin: first_control_margin_pos=0; max_control_margin=0.9911 (001_layers_baseline/run-latest/output-gemma-2-27b.json:7065-7069).

Ablation (noâ€‘filler): L_copy_orig=0, L_sem_orig=46; L_copy_nf=3, L_sem_nf=46; Î”L_copy=3, Î”L_sem=0 (001_layers_baseline/run-latest/output-gemma-2-27b.json:7043-7047). Interpretation: removing â€œsimplyâ€ delays strict copy detection but leaves semantic collapse unchanged.

Î”H (bits) = entropy(L_copy) âˆ’ entropy(L_semantic) = 0.0005 âˆ’ 0.1180 â‰ˆ âˆ’0.1175 (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2,48). Soft Î”H_k (k=1 at Ï„_soft=0.5) is the same because L_copy_soft[k1]=0 here (001_layers_baseline/run-latest/output-gemma-2-27b.json:5656-5661).

Confidence milestones (p_top1, not necessarily the answer): p_top1 > 0.30 at L 0; p_top1 > 0.60 at L 0; final-layer p_top1 = 0.9841 (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2,48).

Rank milestones (norm): rank â‰¤ 10 at L 46; rank â‰¤ 5 at L 46; rank â‰¤ 1 at L 46 (001_layers_baseline/run-latest/output-gemma-2-27b.json:5647-5649). Preferred lens honored: measurement_guidance.preferred_lens_for_reporting="norm"; use_confirmed_semantics=true (001_layers_baseline/run-latest/output-gemma-2-27b.json:7576-7578).

KL milestones: first_kl_below_1.0 = null; first_kl_below_0.5 = null (001_layers_baseline/run-latest/output-gemma-2-27b.json:5645-5646). Final-layer KL is not â‰ˆ 0: 1.1352 bits (001_layers_baseline/run-latest/output-gemma-2-27b.json:6462); last-layer calibration warns with temp_est=2.61 and kl_after_temp_bits=0.5665 (001_layers_baseline/run-latest/output-gemma-2-27b.json:6468-6469).

Cosine milestones (norm): ge_0.2 at L 1; ge_0.4 at L 46; ge_0.6 at L 46 (001_layers_baseline/run-latest/output-gemma-2-27b.json:5736-5740). Final cos_to_final â‰ˆ 0.9994 at L 46 (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48).

Depth fractions: L_semantic_frac=1.0; first_rank_le_5_frac=1.0; L_copy_strict_frac=0.0 (001_layers_baseline/run-latest/output-gemma-2-27b.json:5744-5747).

Copy robustness (threshold sweep): stability="mixed"; earliest strict copy at Ï„=0.70 and Ï„=0.95 is L 0; norm_only_flags false at all Ï„ (001_layers_baseline/run-latest/output-gemma-2-27b.json:5698-5724). Copy configuration: "copy_thresh": 0.95, "copy_window_k": 1, "copy_match_level": "id_subsequence" (001_layers_baseline/run-latest/output-gemma-2-27b.json:5642-5644). Soft-copy config: Ï„_soft=0.5, window_ks=[1,2,3] (001_layers_baseline/run-latest/output-gemma-2-27b.json:889-899).

Prism Sidecar Analysis. Presence/compatibility: present=true, compatible=true (001_layers_baseline/run-latest/output-gemma-2-27b.json:834-846). Early-depth KL: baseline p25/p50/p75 â‰ˆ 42.01/43.15/42.51 bits vs Prism â‰ˆ 19.43/19.42/19.43 bits (drops â‰ˆ 22.6/23.7/23.1) (001_layers_baseline/run-latest/output-gemma-2-27b.json:862-888). Rank milestones under Prism remain null (no rank â‰¤ {10,5,1}) (001_layers_baseline/run-latest/output-gemma-2-27b.json:848-861). At the final layer, Prismâ€™s answer_rank remains large (e.g., 165,699) with very low p_top1 (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token-prism.csv:48). Verdict: Regressive (KL decreases substantially but ranks do not improve to match norm lens semantics).

**Qualitative Patterns & Anomalies**
Negative control shows clean behavior: for â€œBerlin is the capital ofâ€, top-5 are " Germany" (0.8676), " the" (0.0650), " and" (0.0065), " a" (0.0062), " Europe" (0.0056) (001_layers_baseline/run-latest/output-gemma-2-27b.json:14-36). No â€œBerlinâ€ appears in that top-5, so no obvious semantic leakage under this test.

Important-word trajectory (records). The model strongly favors â€œBerlinâ€ near the end of the context: at L46 pos=14 (â€œ isâ€), topâ€‘1 is â€œ Berlinâ€ (0.999998); at pos=15 (â€œ calledâ€), â€œ Berlinâ€ (0.999868); at NEXT (pos=16, â€œ simplyâ€) â€œ Berlinâ€ (0.9841) (001_layers_baseline/run-latest/output-gemma-2-27b-records.csv:804-806; 001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48). This suggests the lexical target is well formed preâ€‘final; the norm lens shows early directional alignment (cosâ‰¥0.2 by L1) but calibration remains highâ€‘KL until the very end.

Copy/filler dynamics. A strict copy-collapsed topâ€‘1 (â€œ simplyâ€) occurs at L0-L4, meeting Ï„=0.95 and Ï„âˆˆ{0.7,0.8,0.9} (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2,5,6). Ablation removes the â€œsimplyâ€ cue, shifting strict copy to L3 while leaving semantic collapse at L46 unchanged (001_layers_baseline/run-latest/output-gemma-2-27b.json:7043-7047). This points to stylistic anchoring influencing very early layers without affecting final semantics.

Rest-mass sanity. Rest_mass is tiny at the collapse layer (â‰ˆ1.99eâ€‘07) and does not spike postâ€‘collapse (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48). No evidence of precision loss from topâ€‘k truncation after semantics.

Rotation vs amplification. Cosine-to-final rises early (ge_0.2 at L1) while KL-to-final stays large across midâ€‘depths (p50â‰ˆ43 bits), indicating â€œearly direction, late calibrationâ€ (001_layers_baseline/run-latest/output-gemma-2-27b.json:5736-5740, 862-876). Final-layer calibration is imperfect: "kl_to_final_bits": 1.1352, with temperature fit reducing KL to 0.5665 (001_layers_baseline/run-latest/output-gemma-2-27b.json:6462,6469). Prefer rank-based milestones (per measurement_guidance) when comparing across families.

Lens sanity and artifacts. Rawâ€‘vsâ€‘norm checks show very large norm-vs-raw KL (up to â‰ˆ99.54 bits) and norm-only semantics at the final layer; risk tier "high" (001_layers_baseline/run-latest/output-gemma-2-27b.json:5769-5784). Treat any preâ€‘final semantics cautiously; here, rankâ€‘1 only arrives at L46 in norm and is marked confirmed. Copy strict norm_only_flags are false across Ï„ (001_layers_baseline/run-latest/output-gemma-2-27b.json:5717-5724).

Temperature robustness. At T=2.0 on the NEXT slot, â€œ Berlinâ€ remains top but much flatter: 0.0492 (001_layers_baseline/run-latest/output-gemma-2-27b.json:1104-1120). Entropy rises accordingly (teacher entropy bits listed as 2.8856 for the final head).

Head calibration (final). Family-typical Gemma signature: topâ€‘1 agrees but norm-lens vs final-head KL is high: "top1_agree": true, "p_top1_lens": 0.9841 vs "p_top1_model": 0.4226; "temp_est": 2.6102; "kl_after_temp_bits": 0.5665 (001_layers_baseline/run-latest/output-gemma-2-27b.json:6463-6469). Treat final probabilities as within-model only; rely on ranks for comparisons.

Checklist
- RMS lens? âœ“ (001_layers_baseline/run-latest/output-gemma-2-27b.json:812-816)
- LayerNorm bias removed? âœ“ (RMSNorm; no bias) (001_layers_baseline/run-latest/output-gemma-2-27b.json:812-816)
- Entropy rise at unembed? n.a.
- FP32 un-embed promoted? âœ“ ("unembed_dtype": "torch.float32") (001_layers_baseline/run-latest/output-gemma-2-27b.json:810-812)
- Punctuation / markup anchoring? âœ“ (early exotic tokens/punctuation; e.g., L5â€“L12 topâ€‘1 are nonâ€‘semantic codepoints)
- Copy-reflex? âœ“ (layer 0 strict) (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2)
- Grammatical filler anchoring? âœ— (L0â€“L5 topâ€‘1 not in {is,the,a,of})
- Preferred lens honored? âœ“ (preferred_lens_for_reporting="norm") (001_layers_baseline/run-latest/output-gemma-2-27b.json:7576)
- Confirmed semantics reported? âœ“ (L_semantic_confirmed=46; source=tuned) (001_layers_baseline/run-latest/output-gemma-2-27b.json:6840-6847)
- Full dualâ€‘lens metrics cited? âœ“ (raw_lens_full tier=high; pct_ge_1.0â‰ˆ0.979) (001_layers_baseline/run-latest/output-gemma-2-27b.json:5775-5784)
- Tunedâ€‘lens attribution done? âœ“ (Î”KL_tuned at percentiles small; prefer_tuned=false) (001_layers_baseline/run-latest/output-gemma-2-27b.json:7497-7562)
- normalization_provenance present? âœ“ (layer 0 ln1; final ln_final) (001_layers_baseline/run-latest/output-gemma-2-27b.json:5792-5793,6160-6161)
- per-layer normalizer metrics present? âœ“ (resid_norm_ratio, delta_resid_cos) (001_layers_baseline/run-latest/output-gemma-2-27b.json:5796,6164)
- unembed bias audited? âœ“ (present=false) (001_layers_baseline/run-latest/output-gemma-2-27b.json:826-830)
- deterministic_algorithms = true? âœ“ (001_layers_baseline/run-latest/output-gemma-2-27b.json:7027-7033)
- numeric_health clean? âœ“ (001_layers_baseline/run-latest/output-gemma-2-27b.json:6426-6432)
- copy_mask plausible? âœ“ (size=4668; whitespace runs) (001_layers_baseline/run-latest/output-gemma-2-27b.json:5618-5636)
- layer_map present? n.a.

**Limitations & Data Quirks**
Finalâ€‘head calibration is imperfect (warn_high_last_layer_kl=true; KLâ‰ˆ1.14 bits at final); follow measurement_guidance to prefer rank milestones and avoid absolute probability comparisons across families (001_layers_baseline/run-latest/output-gemma-2-27b.json:6462,6479,7566-7575). Rawâ€‘vsâ€‘norm checks show high lensâ€‘artifact risk (max norm-vs-raw KLâ‰ˆ99.5 bits; tier=high), so treat any preâ€‘final â€œearly semanticsâ€ as potentially lensâ€‘induced; rely on confirmed semantics and ranks (001_layers_baseline/run-latest/output-gemma-2-27b.json:5769-5784). Surfaceâ€‘mass and coverage metrics depend on tokenizer details; interpret withinâ€‘model trends rather than acrossâ€‘family magnitudes. Soft-copy threshold here is Ï„_soft=0.5 (not the typical 0.33); treat â€œsoftâ€ hits accordingly (001_layers_baseline/run-latest/output-gemma-2-27b.json:889-899).

**Model Fingerprint**
Gemmaâ€‘2â€‘27B: collapse at L 46; final KLâ‰ˆ1.14 bits with tempâ‰ˆ2.61; â€œBerlinâ€ is topâ€‘1 at NEXT with pâ‰ˆ0.984.

---
Produced by OpenAI GPT-5 
