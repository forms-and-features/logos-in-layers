**Result Synthesis**
- Copy-reflex. Only the Gemma family shows an early copy reflex (layers 0–3) by our rule (copy_collapse or copy_soft_k1@τ_soft true). For gemma-2-9b, layer 0 marks strict copy on the filler: “copy_collapse=True, copy_strict@0.95=True, copy_soft_k1@0.5=True” (001_layers_baseline/run-latest/output-gemma-2-9b-pure-next-token.csv:2). gemma-2-27b also fires at L0 (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:1). By contrast, Llama‑3‑8B shows no early copy flags (all False at L0–L3; 001_layers_baseline/run-latest/output-Meta-Llama-3-8B-pure-next-token.csv:2–5), and the same is true for Qwen‑3‑8B (001_layers_baseline/run-latest/output-Qwen3-8B-pure-next-token.csv:2–5), Mistral‑7B‑v0.1 (001_layers_baseline/run-latest/output-Mistral-7B-v0.1-pure-next-token.csv:1–4), Mistral‑Small‑24B (001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501-pure-next-token.csv:2–5), Qwen‑3‑14B (001_layers_baseline/run-latest/output-Qwen3-14B-pure-next-token.csv:2–5), Qwen‑2.5‑72B (001_layers_baseline/run-latest/output-Qwen2.5-72B-pure-next-token.csv:2–5), and Yi‑34B (001_layers_baseline/run-latest/output-Yi-34B-pure-next-token.csv:1–4). Outlier: gemma (9B/27B) has a strong copy reflex anchored at L0.

- Emergence depth (rank milestones; normalized by depth). We classify “early” < 70% vs “late” ≥ 70% of depth using L_semantic and first_rank milestones from JSON. Early: Meta‑Llama‑3‑70B reaches rank≤10 by L38 and rank‑1 by L40 in an 80‑layer stack (001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:851–853), placing semantics at 50% depth. Late: all others. Examples: Llama‑3‑8B is rank‑1 at L25/32 (≈78%) (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:851–853); Mistral‑7B‑v0.1 at L25/32 (≈78%) (001_layers_baseline/run-latest/output-Mistral-7B-v0.1.json:851–853); Mistral‑Small‑24B at L33/40 (≈82%) (001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:851–853); Yi‑34B at L44/60 (≈73%) (001_layers_baseline/run-latest/output-Yi-34B.json:851–853); Qwen‑3‑8B at L31/36 (≈86%) and Qwen‑3‑14B at L36/40 (90%) (001_layers_baseline/run-latest/output-Qwen3-8B.json:851–853; 001_layers_baseline/run-latest/output-Qwen3-14B.json:851–853); Qwen‑2.5‑72B only at the end L80/80 (100%) (001_layers_baseline/run-latest/output-Qwen2.5-72B.json:851–853); gemma‑2‑9b at L42/42 (100%) and gemma‑2‑27b at L46/46 (100%) (001_layers_baseline/run-latest/output-gemma-2-9b.json:851–853; 001_layers_baseline/run-latest/output-gemma-2-27b.json:851–853). Where KL milestones are used, treat them within‑model: e.g., Llama‑3‑8B hits first_kl_below_{1.0,0.5}=32 (last layer) (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:849–850), while Llama‑3‑70B is similar (80/80) (001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:849–850). Exclude Gemma from KL‑based conclusions due to non‑zero last‑layer KL (see below).

- Δ and Δ̂ (L_sem − L_copy). Using diagnostics L_copy (or L_copy_H) and L_semantic: gemma‑2‑9b has L_copy=0, L_sem=42, Δ=42, Δ̂=1.0 (001_layers_baseline/run-latest/output-gemma-2-9b.json:842–844); gemma‑2‑27b has L_copy=0, L_sem=46, Δ=46, Δ̂=1.0 (001_layers_baseline/run-latest/output-gemma-2-27b.json:842–844). Qwen‑3‑14B reports L_copy_H=32 and L_sem=36 (Δ=4, Δ̂=0.10) (001_layers_baseline/run-latest/output-Qwen3-14B.json:842–844,1008–1010). Qwen‑3‑8B has L_copy_H=L_sem=31 (Δ=0, Δ̂=0.0) (001_layers_baseline/run-latest/output-Qwen3-8B.json:842–844,1008–1010). Other models have null L_copy/soft‑copy indices (Δ n.a.), so we summarize with rank thresholds and L_sem/n_layers.

- Entropy shape (ΔH = H(L_copy) − H(L_semantic), bits). Gemma’s copy reflex yields near‑zero entropy at L_copy that increases slightly by L_semantic: gemma‑2‑9b H_copy≈1.67e‑05 at L0 vs H_sem≈0.370 at L42 (ΔH≈−0.37) (001_layers_baseline/run-latest/output-gemma-2-9b-pure-next-token.csv:2,49); gemma‑2‑27b H_copy≈4.97e‑04 at L0 vs H_sem≈0.118 at L46 (ΔH≈−0.117) (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:1,48). Qwen‑3‑14B shows a mild drop: H_copy≈0.816 at L32 → H_sem≈0.312 at L36 (ΔH≈0.50) (001_layers_baseline/run-latest/output-Qwen3-14B-pure-next-token.csv:34,38). Qwen‑3‑8B is flat at L31 (H≈0.454 at both copy_H and semantic) (001_layers_baseline/run-latest/output-Qwen3-8B-pure-next-token.csv:33). For models without L_copy indices (e.g., Llama‑3‑8B), we describe semantic‑layer entropy only: H(L25)≈16.814 bits (001_layers_baseline/run-latest/output-Meta-Llama-3-8B-pure-next-token.csv:29). Overall, sharper Δ is not correlated with width/heads across families: e.g., Llama‑3‑70B (d_model=8192,n_heads=64; 001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:1008–1010) collapses early (50%), whereas Qwen‑2.5‑72B (d_model=8192,n_heads=64; 001_layers_baseline/run-latest/output-Qwen2.5-72B.json:1008–1010) is late (100%). Within‑family, Qwen‑3‑14B (Δ̂=0.10) shows a bigger Δ than Qwen‑3‑8B (Δ̂=0.0).

- Cosine alignment to final (within‑model). We observe rising cos_to_final as depth increases, consistent with “rotation→amplification” trends in tuned‑lens style analyses (cf. arXiv:2303.08112). Examples: Llama‑3‑8B cos≈0.168 at L0, ≈0.137 at mid, ≈0.535 at L31 (001_layers_baseline/run-latest/output-Meta-Llama-3-8B-pure-next-token.csv:2,20,35). Mistral‑7B‑v0.1 rises from −0.330 at L0 to 0.939 at L32 (001_layers_baseline/run-latest/output-Mistral-7B-v0.1-pure-next-token.csv:1,34). Qwen‑3‑14B goes from −0.137 at L0 to 0.850 at L40 (001_layers_baseline/run-latest/output-Qwen3-14B-pure-next-token.csv:2,42). Llama‑3‑70B reaches ≈1.0 at the final layer (pos, L80: cos≈0.99999) (001_layers_baseline/run-latest/output-Meta-Llama-3-70B-pure-next-token.csv:82). Do not compare absolute cosine across families; use within‑model trajectories.

- KL milestones and last‑layer calibration. Use KL thresholds within‑model only. Llama‑3‑8B first_kl_below_{1.0,0.5}=32 (last) (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:849–850); Mistral‑Small‑24B first_kl_below_{1.0,0.5}=40 (last) (001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:849–850); Qwen‑2.5‑72B also at 80/80 (001_layers_baseline/run-latest/output-Qwen2.5-72B.json:849–850). Exclude Gemma from KL‑based depth claims: gemma‑2‑9b has last_layer_kl≈1.013 (warn_high_last_layer_kl=true) (001_layers_baseline/run-latest/output-gemma-2-9b.json:899–917) and gemma‑2‑27b ≈1.135 (001_layers_baseline/run-latest/output-gemma-2-27b.json:899–917), indicating a calibrated final head rather than a pure lens alignment; prefer ranks for cross‑model comparison.

- Lens sanity. raw_lens_check.summary flags two models with norm‑only semantics: Llama‑3‑8B (first_norm_only_semantic_layer=25; lens_artifact_risk=high; max_kl_norm_vs_raw≈0.0713) (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:1008–1074) and Yi‑34B (first_norm_only_semantic_layer=46; lens_artifact_risk=high; max_kl_norm_vs_raw≈80.57) (001_layers_baseline/run-latest/output-Yi-34B.json:1072–1075). Low‑risk runs include Llama‑3‑70B (lens_artifact_risk=low; max_kl_norm_vs_raw≈0.0429) (001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:1071–1075) and Mistral‑Small‑24B (≈0.179; low) (001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:1071). For models with high risk, we downgrade “early semantics” claims and lean on rank thresholds.

- Family comparisons (Qwen, Gemma). Qwen family: Qwen‑3‑8B (L_sem=31/36), Qwen‑3‑14B (36/40), and Qwen‑2.5‑72B (80/80) all collapse late by rank; only Qwen‑3 variants expose a copy_H index (L_copy_H=31,32) with small Δ (Δ̂=0.0 and 0.10) (001_layers_baseline/run-latest/output-Qwen3-8B.json:842–844; 001_layers_baseline/run-latest/output-Qwen3-14B.json:842–844). Entropy around collapse is lower for the larger Qwen‑3‑14B (0.816→0.312) than Qwen‑3‑8B (flat at 0.454), suggesting a slightly sharper late collapse within‑family. Gemma family: both sizes copy at L0 and become rank‑1 only at the end (42/42, 46/46) with non‑zero last‑layer KL (≈1.01, ≈1.14), so we treat probabilities as head‑calibration artifacts and compare by ranks (001_layers_baseline/run-latest/output-gemma-2-9b.json:842–853,899–917; 001_layers_baseline/run-latest/output-gemma-2-27b.json:842–853,899–917).

- Relation to public scores. Early semantic collapse does not cleanly track MMLU/ARC: Llama‑3‑70B collapses early (50%) and scores high on MMLU (79.5%) and ARC‑C (93.0%), but Qwen‑2.5‑72B collapses only at the end (100%) while its MMLU is even higher (86.1%) and ARC‑C lower (72.4%). Models with late collapse can still score well (Mistral‑Small‑24B: 82%, 91.3%). We therefore treat collapse depth as a model‑internal trajectory rather than a predictor of benchmark rank; the qualitative picture matches “early direction, late calibration” observations (arXiv:2303.08112).

**Prism Summary Across Models**
- Coverage. All models report compatible Prism sidecars with k=512 and 3–4 sampled layers (embed, ≈25%, ≈50%, ≈75%) (e.g., 001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:819–838; likewise for others). Across models, we do not see earlier first_rank_le_{10,5,1} at Prism layers relative to baselines; at sampled depths, answer ranks typically remain >10 and do not advance the rank milestones reported in JSON.
- ΔKL at sampled depths. Within‑model KL at ~25/50/75% under Prism generally does not undercut baseline at matched depths for these prompts; several evals note regressive behavior (e.g., Qwen‑2.5‑72B). Gains do not appear larger for models flagged with lens_artifact_risk=high; improvements, when present, are small and concentrated neither early nor mid‑stack. No clear regressions in copy flags are observed under Prism.
- Caveat. Prism files contain tokens with embedded newlines; for rank/ΔKL we rely on JSON milestones and careful CSV reads at the sampled layers rather than row counts.

**Misinterpretations in Existing EVALS**
- Qwen2.5‑72B: The report states a Prism divergence “at L80” (001_layers_baseline/run-latest/evaluation-Qwen2.5-72B.md:131–134), but Prism artifacts are sampled only at layers {embed,19,39,59} per JSON (001_layers_baseline/run-latest/output-Qwen2.5-72B.json:819–832). There is no Prism row at L80; treat that sentence as a layer‑index mix‑up.
- Minor phrasing: Some per‑model narratives implicitly compare absolute probabilities across families despite head‑calibration caveats (e.g., praising final p_top1 where last_layer_kl≠0). For Gemma (family‑typical non‑zero KL), prefer the rank milestones (001_layers_baseline/run-latest/output-gemma-2-9b.json:851–853,899–917; 001_layers_baseline/run-latest/output-gemma-2-27b.json:851–853,899–917). Where a report reads “probabilities are trustworthy,” scope it to families with last_layer_kl≈0.

**Limitations**
- RMS‑lens can distort absolute probabilities; keep comparisons within‑model (not across normalization schemes).
- Single‑prompt probing may over‑fit tokenizer quirks; copy‑collapse depth can change with wording/punctuation.
- We inspect residual projections only; attention/MLP dynamics can create entropy bumps that we may mis‑attribute.
- Some models promote un‑embed to FP32 (“use_fp32_unembed”: true), slightly shrinking entropy gaps; keep comparisons qualitative.
- Final‑lens vs final‑head mismatches can keep last‑layer KL > 0 (e.g., Gemma); prefer rank thresholds and treat KL trends qualitatively within model.
- Layer counts differ; compare relative depths (Δ̂), not raw indices.
- Correlations only; causal evidence (patching) awaits later runs.

---
Produced by OpenAI GPT-5
