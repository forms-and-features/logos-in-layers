## 1. Overview

This evaluation covers google/gemma-2-27b (27B). The probe tracks layer-by-layer next-token behavior with an RMSNorm lens, measuring copy collapse, entropy, rank milestones, KL to the final head, and cosine alignment to the final logits direction, with a negative control and an ablation (no-filler) variant.

## 2. Method sanity-check

Positional encoding and norm-lens usage are confirmed in diagnostics: â€œuse_norm_lens: true â€¦ layer0_position_info: token_only_rotary_model; â€¦ first_block_ln1_type: RMSNormâ€ [001_layers_baseline/run-latest/output-gemma-2-27b.json:807â€“814]. The `context_prompt` ends with â€œcalled simplyâ€ and has no trailing space: â€œGive the city name only, plain text. The capital of Germany is called simplyâ€ [001_layers_baseline/run-latest/output-gemma-2-27b.json:810].

Copy-collapse flags are present in CSV (strict and soft); earliest strict `copy_collapse=True` occurs at layer 0 with top-1/2: â€œ simplyâ€ (p=0.999976), â€œ merelyâ€ (pâ‰ˆ7.52e-06) [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2]. Strict rule parameters and labels are recorded in JSON: `copy_thresh=0.95`, `copy_window_k=1`, `copy_match_level="id_subsequence"`; soft-detector config `copy_soft_config` uses `threshold=0.5`, `window_ks=[1,2,3]`, `extra_thresholds=[]`; `copy_flag_columns` mirrors these labels: ["copy_strict@0.95","copy_soft_k1@0.5","copy_soft_k2@0.5","copy_soft_k3@0.5"] [001_layers_baseline/run-latest/output-gemma-2-27b.json:833â€“849,846â€“847,1049â€“1056]. Gold-token alignment is ID-based and OK: `gold_alignment: "ok"` [001_layers_baseline/run-latest/output-gemma-2-27b.json:866].

Summary indices (diagnostics): `first_kl_below_0.5=null`, `first_kl_below_1.0=null`, `first_rank_le_1=46`, `first_rank_le_5=46`, `first_rank_le_10=46` (units: KL/entropy in bits) [001_layers_baseline/run-latest/output-gemma-2-27b.json:838â€“857]. Last-layer head calibration exists and is not â‰ˆ0: `kl_to_final_bits=1.1352`, `top1_agree=true`, `p_top1_lens=0.9841` vs `p_top1_model=0.4226`, `temp_est=2.6102`, `kl_after_temp_bits=0.5665`, `warn_high_last_layer_kl=true` [001_layers_baseline/run-latest/output-gemma-2-27b.json:875â€“902]. Prefer rank-based statements over absolute probabilities.

Lens sanity (raw vs norm): mode=`sample`; `lens_artifact_risk: "high"`, `max_kl_norm_vs_raw_bits=80.10`, `first_norm_only_semantic_layer=null` [001_layers_baseline/run-latest/output-gemma-2-27b.json:1073â€“1086]. Treat any pre-final â€œearly semanticsâ€ cautiously and prefer rank milestones.

Negative control is present with `control_prompt` and `control_summary`: `first_control_margin_pos=0`, `max_control_margin=0.9910899` [001_layers_baseline/run-latest/output-gemma-2-27b.json:1091â€“1116]. Ablation summary exists and both variants appear in the CSV (orig/no_filler): `L_copy_orig=0`, `L_sem_orig=46`, `L_copy_nf=3`, `L_sem_nf=46`, `delta_L_copy=3`, `delta_L_sem=0` [001_layers_baseline/run-latest/output-gemma-2-27b.json:1083â€“1090; 001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:49â€“51]. For the main table below, rows are filtered to `prompt_id=pos`, `prompt_variant=orig`.

Copy-collapse flag check (strict): first `copy_collapse=True` at layer 0; tokenâ‚=â€œ simplyâ€ (p=0.999976), tokenâ‚‚=â€œ merelyâ€ (pâ‰ˆ7.52e-06) â€” âœ“ rule satisfied [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2]. Soft copy: earliest `copy_soft_k1@0.5=True` at layer 0; `k2`/`k3` never fire in pos/orig.

## 3. Quantitative findings

Per-layer (pos, orig). Format: â€œL n â€” entropy X bits, topâ€‘1 â€˜tokenâ€™â€. Bold indicates the semantic layer (first `is_answer=True`).

| Layer | Entropy (bits) | Topâ€‘1 |
|---:|---:|:---|
| 0 | 0.000 | â€˜ simplyâ€™ |
| 1 | 8.758 | â€˜â€™ |
| 2 | 8.764 | â€˜â€™ |
| 3 | 0.886 | â€˜ simplyâ€™ |
| 4 | 0.618 | â€˜ simplyâ€™ |
| 5 | 8.520 | â€˜à¹²â€™ |
| 6 | 8.553 | â€˜ï•€â€™ |
| 7 | 8.547 | â€˜î«¤â€™ |
| 8 | 8.529 | â€˜ïŒâ€™ |
| 9 | 8.524 | â€˜ğ†£â€™ |
| 10 | 8.345 | â€˜ dieÅ¿emâ€™ |
| 11 | 8.493 | â€˜ğ†£â€™ |
| 12 | 8.324 | â€˜î«¤â€™ |
| 13 | 8.222 | â€˜î«¤â€™ |
| 14 | 7.877 | â€˜î«¤â€™ |
| 15 | 7.792 | â€˜î«¤â€™ |
| 16 | 7.975 | â€˜ dieÅ¿emâ€™ |
| 17 | 7.786 | â€˜ dieÅ¿emâ€™ |
| 18 | 7.300 | â€˜Å¿ichtâ€™ |
| 19 | 7.528 | â€˜ dieÅ¿emâ€™ |
| 20 | 6.210 | â€˜Å¿ichtâ€™ |
| 21 | 6.456 | â€˜Å¿ichtâ€™ |
| 22 | 6.378 | â€˜ dieÅ¿emâ€™ |
| 23 | 7.010 | â€˜ dieÅ¿emâ€™ |
| 24 | 6.497 | â€˜ dieÅ¿emâ€™ |
| 25 | 6.995 | â€˜ dieÅ¿emâ€™ |
| 26 | 6.220 | â€˜ dieÅ¿emâ€™ |
| 27 | 6.701 | â€˜ dieÅ¿emâ€™ |
| 28 | 7.140 | â€˜ dieÅ¿emâ€™ |
| 29 | 7.574 | â€˜ dieÅ¿emâ€™ |
| 30 | 7.330 | â€˜ dieÅ¿emâ€™ |
| 31 | 7.565 | â€˜ dieÅ¿emâ€™ |
| 32 | 8.874 | â€˜ zuÅ¿ammenâ€™ |
| 33 | 6.945 | â€˜ dieÅ¿emâ€™ |
| 34 | 7.738 | â€˜ dieÅ¿emâ€™ |
| 35 | 7.651 | â€˜ dieÅ¿emâ€™ |
| 36 | 7.658 | â€˜ dieÅ¿emâ€™ |
| 37 | 7.572 | â€˜ dieÅ¿emâ€™ |
| 38 | 7.554 | â€˜ ãƒ‘ãƒ³ãƒãƒ©â€™ |
| 39 | 7.232 | â€˜ dieÅ¿emâ€™ |
| 40 | 8.711 | â€˜ å±•æ¿â€™ |
| 41 | 7.082 | â€˜ dieÅ¿emâ€™ |
| 42 | 7.057 | â€˜ dieÅ¿emâ€™ |
| 43 | 7.089 | â€˜ dieÅ¿emâ€™ |
| 44 | 7.568 | â€˜ dieÅ¿emâ€™ |
| 45 | 7.141 | â€˜ GeÅ¿châ€™ |
| **46** | **0.118** | **â€˜ Berlinâ€™** |

Semantic layer: L_semantic = 46 (first `is_answer=True`), with â€œ Berlinâ€ topâ€‘1, p_top1=0.9841 and `answer_rank=1` [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48].

Control margin (JSON): `first_control_margin_pos=0`, `max_control_margin=0.9910899` [001_layers_baseline/run-latest/output-gemma-2-27b.json:1110â€“1116].

Ablation (noâ€‘filler) [JSON]: L_copy_orig=0, L_sem_orig=46; L_copy_nf=3, L_sem_nf=46; Î”L_copy=+3, Î”L_sem=0 [001_layers_baseline/run-latest/output-gemma-2-27b.json:1083â€“1090].

Î”H (bits) = entropy(L_copy) âˆ’ entropy(L_semantic) = 0.00050 âˆ’ 0.11805 = âˆ’0.11755 [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2,48].

Soft Î”Hâ‚– (bits): k=1 uses L=0 â‡’ same as strict (âˆ’0.11755). k=2,3 are null (no soft copy firing) [001_layers_baseline/run-latest/output-gemma-2-27b.json:854â€“861].

Confidence milestones (pure CSV): p_top1>0.30 at L=3; p_top1>0.60 at L=3; final-layer p_top1=0.9841 [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2â€“6,48].

Rank milestones (JSON): rankâ‰¤10 at L=46; rankâ‰¤5 at L=46; rankâ‰¤1 at L=46 [001_layers_baseline/run-latest/output-gemma-2-27b.json:838â€“845].

KL milestones and final-head note: `first_kl_below_1.0=null`, `first_kl_below_0.5=null`; final KL is not â‰ˆ0 (1.1352 bits), consistent with family head calibration issues [001_layers_baseline/run-latest/output-gemma-2-27b.json:836â€“843,875â€“902]. Prefer ranks over final absolute probabilities.

Cosine milestones (pure CSV): first `cos_to_finalâ‰¥0.2` at L=1; `â‰¥0.4` at L=46; `â‰¥0.6` at L=46; final `cos_to_final=0.99939` [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48].

Prism Sidecar Analysis
- Presence: prism_summary.compatible=true, present=true, k=512, layers=[embed,10,22,33] [001_layers_baseline/run-latest/output-gemma-2-27b.json:817â€“833].
- Early-depth stability (KL to final): baseline vs prism at sampled depths (L=0,11,23,34):
  - Baseline KL: 16.85, 41.85, 43.15, 42.51 bits; Prism KL: 19.43, 19.43, 19.42, 19.43 bits [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv; 001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token-prism.csv]. No helpful early drop.
  - Final KL: baseline 1.135; prism 20.172 bits (miscalibrated) [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48; â€¦-prism.csv].
- Rank milestones (prism): never reaches rankâ‰¤10/5/1 (null) vs baseline all at L=46 [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token-prism.csv].
- Topâ€‘1 agreement: baseline disagrees with final at early depths; prism also disagrees and remains very low-confidence (p_top1â‰ˆ1.6eâ€‘5 at Lâˆˆ{0,11,23,34}) [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token-prism.csv].
- Cosine drift: prism cos_to_final is slightly negative (â‰ˆâˆ’0.09..âˆ’0.11) at sampled depths, indicating a poor alignment vs the final direction [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token-prism.csv].
- Copy flags: baseline strict copy at L0 flips to False under Prism (mass spread), plausibly from the transform reducing concentrated copy mass [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2; â€¦-prism.csv].
- Verdict: Regressive (higher KL, no earlier rank milestones, degraded calibration).

## 4. Qualitative patterns & anomalies

The model exhibits a clear copy-reflex at layer 0: â€œ simplyâ€ dominates (pâ‰ˆ0.99998), satisfying the strict copy rule, while soft copy (k=1) also fires at L0 [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2]. Despite early cosine alignment (`cosâ‰ˆ0.33` by midâ€‘stack), KL to final remains very high until the last layer, indicating â€œearly direction, late calibration.â€ Finalâ€‘head calibration issues are explicit for Gemmaâ€‘2â€‘27B: `top1_agree=true` but `p_top1_lens=0.9841` vs `p_top1_model=0.4226`, `temp_est=2.610`, `kl_after_temp_bits=0.5665`, and `warn_high_last_layer_kl=true` [001_layers_baseline/run-latest/output-gemma-2-27b.json:875â€“902].

Negative control: â€œBerlin is the capital ofâ€ produces a clean country prediction; topâ€‘5 are â€œ Germanyâ€ (0.8676), â€œ theâ€ (0.0650), â€œ andâ€ (0.0065), â€œ aâ€ (0.0062), â€œ Europeâ€ (0.0056) â€” no â€œBerlinâ€ appears in the topâ€‘5 [001_layers_baseline/run-latest/output-gemma-2-27b.json:13â€“31].

Records and importantâ€‘word trajectory (SCRIPT IMPORTANT_WORDS = ["Germany","Berlin","capital","Answer","word","simply"]). Within the context, â€œ Germanyâ€ is strongly salient early at its token position (e.g., L0 pos=13 topâ€‘1 â€œ Germanyâ€, pâ‰ˆ0.4365 [001_layers_baseline/run-latest/output-gemma-2-27b-records.csv:15]; L3 pos=13 topâ€‘1 â€œ Germanyâ€, pâ‰ˆ0.5790 [001_layers_baseline/run-latest/output-gemma-2-27b-records.csv:66]). At NEXT (pos=16), â€œ Berlinâ€ only becomes topâ€‘1 at the final layer: â€œ Berlin, 0.9841â€ (L46, pos=16) [001_layers_baseline/run-latest/output-gemma-2-27b-records.csv:806]. Semantically close city distractors appear only as minor mass in the final prediction (e.g., â€œ Munichâ€, pâ‰ˆ0.00581; also â€œ BERLIN/berlinâ€ variants) [001_layers_baseline/run-latest/output-gemma-2-27b.json:957â€“981].

Instructional variations (test prompts): removing the filler cue or â€œsimplyâ€ in phrasing still yields high-confidence â€œ Berlinâ€ (e.g., â€œGermanyâ€™s capital city is calledâ€, pâ‰ˆ0.598; â€œThe capital city of Germany is namedâ€, pâ‰ˆ0.376) [001_layers_baseline/run-latest/output-gemma-2-27b.json:214â€“231,240â€“254]. While these probe only final predictions (not layer-wise collapse), the ablation run shows Î”L_sem=0, Î”L_copy=+3, indicating stylistic filler mainly affects early copy behavior rather than semantic emergence [001_layers_baseline/run-latest/output-gemma-2-27b.json:1083â€“1090].

Restâ€‘mass sanity: no topâ€‘k coverage issues after semantics; at L=46, `rest_massâ‰ˆ2.0eâ€‘7` [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48].

Rotation vs amplification: KL decreases sharply only at the end (still 1.135 bits at final), while `answer_rank` jumps to 1 and `p_answer`/`p_top1` surge; cosine rises modestly early (â‰ˆ0.33 midâ€‘stack) before snapping to â‰ˆ1.0 at L=46. This is consistent with â€œearly direction, late calibration.â€ Given `warn_high_last_layer_kl=true`, prefer rank-based milestones for cross-family claims [001_layers_baseline/run-latest/output-gemma-2-27b.json:875â€“902,1073â€“1086].

Head calibration (final): `temp_est=2.610` reduces KL to 0.5665 bits (`kl_after_temp_bits`) but still not â‰ˆ0; `cfg_transform` fields are present but null for this run [001_layers_baseline/run-latest/output-gemma-2-27b.json:875â€“902].

Lens sanity: raw-vs-norm check (mode=sample) flags `lens_artifact_risk="high"` with `max_kl_norm_vs_raw_bits=80.10`; no `first_norm_only_semantic_layer` [001_layers_baseline/run-latest/output-gemma-2-27b.json:1079â€“1086]. Treat any apparent early semantics cautiously; rely on ranks and within-model cosine trends.

Temperature robustness (final head): at T=0.1, â€œ Berlinâ€ rank 1 (pâ‰ˆ0.9898; entropyâ‰ˆ0.082 bits); at T=2.0, â€œ Berlinâ€ rank 1 (pâ‰ˆ0.0492; entropyâ‰ˆ12.631 bits) [001_layers_baseline/run-latest/output-gemma-2-27b.json:669â€“697,736â€“760].

Checklist
- RMS lens? âœ“ (â€œRMSNormâ€ in first/final LN) [001_layers_baseline/run-latest/output-gemma-2-27b.json:811â€“814]
- LayerNorm bias removed? âœ“ (â€œnot_needed_rms_modelâ€) [001_layers_baseline/run-latest/output-gemma-2-27b.json:813]
- Entropy rise at unembed? âœ“ (pure L46â‰ˆ0.118 â†’ finalâ‰ˆ2.886 bits) [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48; 001_layers_baseline/run-latest/output-gemma-2-27b.json:922]
- FP32 unâ€‘embed promoted? âœ“ (`unembed_dtype:"torch.float32"`; `use_fp32_unembed:false`) [001_layers_baseline/run-latest/output-gemma-2-27b.json:808â€“809]
- Punctuation / markup anchoring? âœ“ (quotes/punctuation in finals) [001_layers_baseline/run-latest/output-gemma-2-27b.json:944â€“961]
- Copyâ€‘reflex? âœ“ (copy_collapse=True in L0) [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2]
- Grammatical filler anchoring? âœ— for {is,the,a,of} as topâ€‘1 in L0â€“5 (dominant cue is â€œ simplyâ€).

## 5. Limitations & data quirks

- Finalâ€‘layer KL is not â‰ˆ0 (`kl_to_final_bits=1.135`), and `warn_high_last_layer_kl=true`; treat final probabilities as familyâ€‘specific calibration and rely on rank milestones and withinâ€‘model trends [001_layers_baseline/run-latest/output-gemma-2-27b.json:875â€“902].
- Rawâ€‘vsâ€‘norm lens `mode=sample` with `lens_artifact_risk="high"` and `max_kl_norm_vs_raw_bits=80.10`; treat earlyâ€‘depth findings as sanity checks rather than exhaustive [001_layers_baseline/run-latest/output-gemma-2-27b.json:1079â€“1086].
- Rest_mass is low after semantics (â‰ˆ2eâ€‘7 at L=46); no evidence of precision loss from topâ€‘k coverage in this run [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48].

## 6. Model fingerprint (one sentence)

Gemmaâ€‘2â€‘27B: collapse at Lâ€¯46; final entropy â‰ˆ0.118â€¯bits (lens) with â€œ Berlinâ€ topâ€‘1; strong early copy of â€œ simplyâ€ at Lâ€¯0; finalâ€‘head calibration gap persists (KLâ‰ˆ1.135â€¯bits).

---
Produced by OpenAI GPT-5 

