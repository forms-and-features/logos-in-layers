**Overview**
- Model: google/gemma-2-27b (47 layers; pre_norm; RMSNorm). Run timestamp: 2025-09-21 19:45 local (file mtime of JSON: 001_layers_baseline/run-latest/output-gemma-2-27b.json).
- Probe captures layer-wise entropy, top-1 trajectories, ID-level answer alignment, KL-to-final, cosine-to-final, copy collapse, and ablation (no-filler) effects on the Germanyâ†’Berlin task. Final-layer lens shows Berlin as top-1 with high confidence while last-layer head calibration diverges (KL â‰ˆ 1.14 bits).

**Method Sanity-Check**
The prompt and norm lens configuration are as intended. The context prompt ends with â€œcalled simplyâ€ (no trailing space):
> "Give the city name only, plain text. The capital of Germany is called simply"  [001_layers_baseline/run-latest/output-gemma-2-27b.json:4]
Norm lens is enabled with FP32 un-embed dtype: 
> "use_norm_lens": true, "unembed_dtype": "torch.float32"  [001_layers_baseline/run-latest/output-gemma-2-27b.json:807â€“809]
Positional encoding info is recorded: 
> "layer0_position_info": "token_only_rotary_model"  [001_layers_baseline/run-latest/output-gemma-2-27b.json:816]
Copy rule parameters are present and use ID-level subsequence matching (k=1, Ï„=0.95, Î´=0.10):
> "copy_thresh": 0.95, "copy_window_k": 1, "copy_match_level": "id_subsequence"  [001_layers_baseline/run-latest/output-gemma-2-27b.json:837â€“839]
Summary indices are present (bits):
> "first_kl_below_0.5": null, "first_kl_below_1.0": null, "first_rank_le_1": 46, "first_rank_le_5": 46, "first_rank_le_10": 46  [001_layers_baseline/run-latest/output-gemma-2-27b.json:840â€“844]
Gold alignment uses ID-level tokens and is OK:
> "gold_alignment": "ok"  [001_layers_baseline/run-latest/output-gemma-2-27b.json:845]; gold token: "â–Berlin" (first_id 12514)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1052â€“1059]
Negative control is present:
> "control_prompt" â€¦ Franceâ†’Paris  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1032â€“1046]; "control_summary": {"first_control_margin_pos": 0, "max_control_margin": 0.9910899400710897}  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1047â€“1050]
Ablation exists and both variants appear in CSVs (`prompt_variant = orig` and `no_filler`):
> "ablation_summary": {"L_copy_orig": 0, "L_sem_orig": 46, "L_copy_nf": 3, "L_sem_nf": 46, "delta_L_copy": 3, "delta_L_sem": 0}  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1024â€“1030]
Lens sanity check indicates high artifact risk (sample mode):
> summary: {"first_norm_only_semantic_layer": null, "max_kl_norm_vs_raw_bits": 80.10008036401692, "lens_artifact_risk": "high"}  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1018â€“1022]
Last-layer head calibration diverges (expected for Gemma):
> last_layer_consistency: {"kl_to_final_bits": 1.1352, "top1_agree": true, "p_top1_lens": 0.9841, "p_top1_model": 0.4226, "temp_est": 2.6102, "kl_after_temp_bits": 0.5665, "warn_high_last_layer_kl": true}  [001_layers_baseline/run-latest/output-gemma-2-27b.json:846â€“865]
Copyâ€‘collapse flag check (pure CSV; pos/orig): first `copy_collapse = True` is at layer 0 with a copied token from the prompt: 
> (layer 0, topâ€‘1 = â€˜simplyâ€™, p = 0.99998; topâ€‘2 = â€˜merelyâ€™, p = 7.5eâ€‘06)  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2]  âœ“ rule satisfied

**Quantitative Findings**
Main table (pos, orig). Each row: L i â€“ entropy X bits, topâ€‘1 â€˜tokenâ€™. Bold marks the first semantic layer (is_answer=True). Units: bits.

- L 0 â€“ entropy 0.000 bits, top-1 'simply'
- L 1 â€“ entropy 8.758 bits, top-1 ''
- L 2 â€“ entropy 8.764 bits, top-1 ''
- L 3 â€“ entropy 0.886 bits, top-1 'simply'
- L 4 â€“ entropy 0.618 bits, top-1 'simply'
- L 5 â€“ entropy 8.520 bits, top-1 'à¹²'
- L 6 â€“ entropy 8.553 bits, top-1 'ï•€'
- L 7 â€“ entropy 8.547 bits, top-1 'î«¤'
- L 8 â€“ entropy 8.529 bits, top-1 'ïŒ'
- L 9 â€“ entropy 8.524 bits, top-1 'ğ†£'
- L 10 â€“ entropy 8.345 bits, top-1 'dieÅ¿em'
- L 11 â€“ entropy 8.493 bits, top-1 'ğ†£'
- L 12 â€“ entropy 8.324 bits, top-1 'î«¤'
- L 13 â€“ entropy 8.222 bits, top-1 'î«¤'
- L 14 â€“ entropy 7.877 bits, top-1 'î«¤'
- L 15 â€“ entropy 7.792 bits, top-1 'î«¤'
- L 16 â€“ entropy 7.975 bits, top-1 'dieÅ¿em'
- L 17 â€“ entropy 7.786 bits, top-1 'dieÅ¿em'
- L 18 â€“ entropy 7.300 bits, top-1 'Å¿icht'
- L 19 â€“ entropy 7.528 bits, top-1 'dieÅ¿em'
- L 20 â€“ entropy 6.210 bits, top-1 'Å¿icht'
- L 21 â€“ entropy 6.456 bits, top-1 'Å¿icht'
- L 22 â€“ entropy 6.378 bits, top-1 'dieÅ¿em'
- L 23 â€“ entropy 7.010 bits, top-1 'dieÅ¿em'
- L 24 â€“ entropy 6.497 bits, top-1 'dieÅ¿em'
- L 25 â€“ entropy 6.995 bits, top-1 'dieÅ¿em'
- L 26 â€“ entropy 6.220 bits, top-1 'dieÅ¿em'
- L 27 â€“ entropy 6.701 bits, top-1 'dieÅ¿em'
- L 28 â€“ entropy 7.140 bits, top-1 'dieÅ¿em'
- L 29 â€“ entropy 7.574 bits, top-1 'dieÅ¿em'
- L 30 â€“ entropy 7.330 bits, top-1 'dieÅ¿em'
- L 31 â€“ entropy 7.565 bits, top-1 'dieÅ¿em'
- L 32 â€“ entropy 8.874 bits, top-1 'zuÅ¿ammen'
- L 33 â€“ entropy 6.945 bits, top-1 'dieÅ¿em'
- L 34 â€“ entropy 7.738 bits, top-1 'dieÅ¿em'
- L 35 â€“ entropy 7.651 bits, top-1 'dieÅ¿em'
- L 36 â€“ entropy 7.658 bits, top-1 'dieÅ¿em'
- L 37 â€“ entropy 7.572 bits, top-1 'dieÅ¿em'
- L 38 â€“ entropy 7.554 bits, top-1 'ãƒ‘ãƒ³ãƒãƒ©'
- L 39 â€“ entropy 7.232 bits, top-1 'dieÅ¿em'
- L 40 â€“ entropy 8.711 bits, top-1 'å±•æ¿'
- L 41 â€“ entropy 7.082 bits, top-1 'dieÅ¿em'
- L 42 â€“ entropy 7.057 bits, top-1 'dieÅ¿em'
- L 43 â€“ entropy 7.089 bits, top-1 'dieÅ¿em'
- L 44 â€“ entropy 7.568 bits, top-1 'dieÅ¿em'
- L 45 â€“ entropy 7.141 bits, top-1 'GeÅ¿ch'
- **L 46 â€“ entropy 0.118 bits, top-1 'Berlin'**

Control margin (JSON): first_control_margin_pos = 0; max_control_margin = 0.9910899400710897  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1047â€“1050]

Ablation (noâ€‘filler): L_copy_orig = 0, L_sem_orig = 46; L_copy_nf = 3, L_sem_nf = 46; Î”L_copy = 3, Î”L_sem = 0  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1024â€“1030]

Î”H (bits) = entropy(L_copy) âˆ’ entropy(L_semantic) = 0.00050 âˆ’ 0.11805 = âˆ’0.11755

Confidence milestones (pure CSV, pos/orig):
- p_top1 > 0.30 at layer 0; p_top1 > 0.60 at layer 0; final-layer p_top1 = 0.9841  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2,48]

Rank milestones (diagnostics):
- rank â‰¤ 10 at layer 46; rank â‰¤ 5 at layer 46; rank â‰¤ 1 at layer 46  [001_layers_baseline/run-latest/output-gemma-2-27b.json:842â€“844]

KL milestones (diagnostics + CSV):
- first_kl_below_1.0: null; first_kl_below_0.5: null  [001_layers_baseline/run-latest/output-gemma-2-27b.json:840â€“841]; final-layer kl_to_final_bits = 1.1352 (not â‰ˆ 0), consistent with last-layer calibration divergence  [001_layers_baseline/run-latest/output-gemma-2-27b.json:846â€“865]. KL generally decreases sharply only at the final layer but remains >1 bit at final.

Cosine milestones (pure CSV, pos/orig):
- first cos_to_final â‰¥ 0.2 at layer 1; â‰¥ 0.4 at layer 46; â‰¥ 0.6 at layer 46; final cos_to_final = 0.99939  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48]

Prism Sidecar Analysis
- Presence: prism sidecar present and compatible (mode "auto", k=512)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:819â€“825].
- Early-depth stability (KL at layers 0/âŒŠn/4âŒ‹/âŒŠn/2âŒ‹/âŒŠ3n/4âŒ‹/final): baseline â‰ˆ {16.85, 41.85, 43.15, 42.51, 1.14} vs Prism â‰ˆ {19.43, 19.43, 19.42, 19.43, 20.17} bits  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv, 001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token-prism.csv]. Prism reduces midâ€‘stack KL substantially but does not approach final.
- Rank milestones (Prism): first_rank_le_{10,5,1} = none observed (answer never reaches topâ€‘10). Baseline achieved all at layer 46.
- Topâ€‘1 agreement: at sampled depths, Prism topâ€‘1 tokens do not agree with the final (â€˜Berlinâ€™), including at the final layer (e.g., layer 46 Prism topâ€‘1 â€˜furiouslyâ€™, pâ‰ˆ1.7eâˆ’4)  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token-prism.csv:48].
- Cosine drift: Prism cos_to_final is negative at early/mid layers (e.g., L0 â‰ˆ âˆ’0.089, L23 â‰ˆ âˆ’0.095) and remains negative at final (â‰ˆ âˆ’0.070), indicating no alignment with the final direction.
- Copy flags: baseline fires copy at L0 (âœ“), Prism does not (all False at L0â€“3), consistent with Prism removing the early copy prior.
- Verdict: Regressive â€” despite lower midâ€‘stack KL, Prism fails to recover rank milestones or topâ€‘1 agreement and remains far from the final distribution.

**Qualitative Patterns & Anomalies**
Early layers show a strong copy reflex: layer 0 topâ€‘1 copies the trailing word â€˜simplyâ€™ with pâ‰ˆ0.99998  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2]. Midâ€‘stack is dominated by orthographic/punctuation and historicalâ€‘orthography artifacts (e.g., â€˜dieÅ¿emâ€™, â€˜Å¿ichtâ€™, nonâ€‘Latin tokens), while cosine to final creeps up slowly and only snaps into alignment at the last layer (cos â‰ˆ 0.999 at L46). KL remains very high throughout the stack and drops only at the end, but not to â‰ˆ0; this is a classic â€œearly direction, late calibrationâ€ pattern under a norm lens with a misâ€‘calibrated final head.

Negative control (â€œBerlin is the capital ofâ€): topâ€‘5 are " Germany" (0.8676), " the" (0.0650), " and" (0.0065), " a" (0.0062), " Europe" (0.0056); Berlin does not appear  [001_layers_baseline/run-latest/output-gemma-2-27b.json:10â€“31]. No semantic leakage.

Importantâ€‘word trajectory (records CSV; IMPORTANT_WORDS = ["Germany", "Berlin", "capital", "Answer", "word", "simply"]): â€˜Germanyâ€™ is consistently salient around its token (e.g., layer 3 at pos=13: topâ€‘1 â€˜Germanyâ€™, pâ‰ˆ0.579)  [001_layers_baseline/run-latest/output-gemma-2-27b-records.csv:66]. The answer â€˜Berlinâ€™ enters decisively only at the end and saturates across positions (e.g., L46 pos=14: â€˜Berlinâ€™, pâ‰ˆ0.999998; pos=15: pâ‰ˆ0.999868; NEXT pos=16: pâ‰ˆ0.9841)  [001_layers_baseline/run-latest/output-gemma-2-27b-records.csv:804â€“806]. The grammatical cue â€˜simplyâ€™ is topâ€‘1 in early layers at the NEXT position (L0/L3/L4), then vanishes as semantics consolidate  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2â€“6]. Closest semantic distractors (e.g., â€˜Munichâ€™, â€˜Bonnâ€™) appear only as minor mass at final (e.g., â€˜Munichâ€™ pâ‰ˆ0.0058 in final prediction)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:904â€“909].

Collapseâ€‘layer instruction sensitivity: Removing â€œsimplyâ€ delays copy by +3 layers (L_copy: 0â†’3) but leaves semantics unchanged (L_sem: 46â†’46), indicating stylistic anchoring affects the copy reflex but not answer formation  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1024â€“1030].

Restâ€‘mass sanity: Rest_mass is tiny at the semantic layer (â‰ˆ2.0eâˆ’07 at L46), consistent with concentrated probability mass and no precision loss  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48].

Rotation vs amplification: cos_to_final rises late (â‰¥0.4 only at L46) while KL stays high until the end; p_answer and rank jump abruptly only at L46 (rank 1, p_answerâ‰ˆ0.984). This is â€œearly direction, late calibrationâ€; given final KL â‰ˆ1.14 bits and warn_high_last_layer_kl = true, prefer rank milestones over absolute probabilities for crossâ€‘family comparisons  [001_layers_baseline/run-latest/output-gemma-2-27b.json:846â€“865].

Head calibration (final layer): temp_est â‰ˆ 2.61 reduces KL but leaves it at â‰ˆ0.57 bits (kl_after_temp_bits) and `warn_high_last_layer_kl = true`  [001_layers_baseline/run-latest/output-gemma-2-27b.json:853â€“854,864]. Treat final-layer probabilities as familyâ€‘specific; rely on rank milestones and withinâ€‘model trends.

Lens sanity: rawâ€‘vsâ€‘norm sample indicates high artifact risk with max_kl_norm_vs_raw_bits â‰ˆ 80.10 and no â€œnormâ€‘only semanticsâ€ layer; early â€œsemanticsâ€ should be treated cautiously and rankâ€‘based milestones preferred  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1018â€“1022].

Temperature robustness: At T = 0.1, Berlin rank 1 (p â‰ˆ 0.9898); at T = 2.0, Berlin remains rank 1 (p â‰ˆ 0.0492). Entropy rises from â‰ˆ0.082 bits to â‰ˆ12.63 bits as T increases  [001_layers_baseline/run-latest/output-gemma-2-27b.json:670â€“676,737â€“745,738â€“739].

Checklist
- RMS lens? âœ“ (RMSNorm; pre_norm)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:810â€“816,960â€“961]
- LayerNorm bias removed? âœ“ ("not_needed_rms_model")  [001_layers_baseline/run-latest/output-gemma-2-27b.json:812]
- Entropy rise at unembed? âœ“ (final prediction entropy â‰ˆ 2.886 bits vs L46 â‰ˆ 0.118)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:869; 001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48]
- FP32 unâ€‘embed promoted? âœ— (`use_fp32_unembed`: false), but `unembed_dtype` is torch.float32  [001_layers_baseline/run-latest/output-gemma-2-27b.json:808â€“809,815]
- Punctuation / markup anchoring? âœ“ (early topâ€‘1 tokens like â€˜à¹²â€™, â€˜ï•€â€™, â€˜î«¤â€™)  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:7â€“9]
- Copyâ€‘reflex? âœ“ (copy_collapse = True at L0)  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2]
- Grammatical filler anchoring? âœ— (early topâ€‘1 not in {â€œisâ€, â€œtheâ€, â€œaâ€, â€œofâ€})

**Limitations & Data Quirks**
- Final KL-to-final â‰ˆ 1.135 bits with `warn_high_last_layer_kl = true`; treat final probabilities as calibrationâ€‘specific and prefer rank milestones for crossâ€‘model claims  [001_layers_baseline/run-latest/output-gemma-2-27b.json:846â€“865].
- `raw_lens_check` ran in sample mode and flags high lensâ€‘artifact risk (max_kl_norm_vs_raw_bits â‰ˆ 80.10); early â€œsemanticsâ€ may be lensâ€‘induced  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1018â€“1022].
- Rest_mass is not a fidelity metric; its nearâ€‘zero final value only reflects topâ€‘k coverage. KL/entropy reported are in bits.
- Prism sidecar appears misaligned with final logits (negative cosine, no rank milestones); treat its outputs as exploratory diagnostics only.

**Model Fingerprint**
â€œGemmaâ€‘2â€‘27B: collapse at L 46; final entropy (lens) 0.118 bits; â€˜Berlinâ€™ becomes topâ€‘1 only at the last layer; strong copy reflex at L0.â€

---
Produced by OpenAI GPT-5

