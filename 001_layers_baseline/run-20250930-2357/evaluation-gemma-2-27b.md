# Evaluation Report: google/gemma-2-27b

*Run executed on: 2025-09-30 23:57:21*
**Overview**

Google Gemmaâ€‘2â€‘27B (46 layers; preâ€‘norm) was probed layerâ€‘byâ€‘layer on the â€œGermany â†’ Berlinâ€ prompt. The run captures copyâ€‘reflex at the input token and a very late semantic collapse at the final layer. Confirmed semantics (tuned corroboration) occur at Lâ€¯46.

**Method Sanityâ€‘Check**

Diagnostics indicate the intended norm lens and rotary positional encoding path were used: â€œuse_norm_lens: true â€¦ layer0_position_info: token_only_rotary_modelâ€  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1080â€“1090, 1097â€“1103]. The context prompt ends exactly with â€œcalled simplyâ€ (no trailing space): â€œcontext_prompt â€¦ The capital of Germany is called simplyâ€  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1099â€“1103].

Copy detection configuration and outputs are present: â€œcopy_thresh: 0.95, copy_window_k: 1, copy_match_level: id_subsequenceâ€ with strict sweep entries and softâ€‘copy config/window_ks recorded  [001_layers_baseline/run-latest/output-gemma-2-27b.json:941â€“951, 966â€“983, 994â€“1012]. The JSON lists `copy_flag_columns` that mirror the CSV flags  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1630â€“1638]. Goldâ€‘token alignment is OK  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1083] and ablation summary is present with both `orig` and `no_filler` variants  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1639â€“1646]. For the main table and milestones below, rows are filtered to `prompt_id = pos`, `prompt_variant = orig` in the pure CSV.

Measurement guidance requests rankâ€‘first reporting: â€œprefer_ranks: true, suppress_abs_probs: true â€¦ reasons: [â€˜warn_high_last_layer_klâ€™, â€˜norm_only_semantics_windowâ€™, â€˜high_lens_artifact_riskâ€™, â€˜high_lens_artifact_scoreâ€™] â€¦ preferred_lens_for_reporting: "norm" â€¦ use_confirmed_semantics: trueâ€  [001_layers_baseline/run-latest/output-gemma-2-27b.json:2150â€“2162].

Rawâ€‘vsâ€‘Norm window and full checks flag normâ€‘only semantics and high lens artefact risk: â€œcenter_layers: [0, 46], radius: 4 â€¦ norm_only_semantics_layers: [46] â€¦ max_kl_norm_vs_raw_bits_window: 99.54â€  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1047â€“1069]; and in full: â€œpct_layers_kl_ge_1.0: 0.9787 â€¦ n_norm_only_semantics_layers: 1 â€¦ earliest_norm_only_semantic: 46 â€¦ max_kl_norm_vs_raw_bits: 99.54 â€¦ tier: "high"â€  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1071â€“1081]. Treat early semantics cautiously and prefer rank milestones.

Strict copy thresholds sweep shows earliest `L_copy_strict` at Ï„âˆˆ{0.70,0.95} equals 0 (stability: â€œmixedâ€; norm_only_flags false)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1001â€“1019]. The pure CSV confirms strict copy at L0 with contiguous IDâ€‘level subsequence: â€œlayer 0 â€¦ copy_collapse = True â€¦ copy_strict@0.95 = True â€¦ copy_soft_k1@0.5 = Trueâ€  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2].

Lastâ€‘layer head calibration is nonâ€‘negligible (Gemma family pattern): â€œkl_to_final_bits: 1.1352 â€¦ top1_agree: true; p_top1_lens: 0.9841 vs p_top1_model: 0.4226; temp_est: 2.61; kl_after_temp_bits: 0.5665; warn_high_last_layer_kl: trueâ€  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1084â€“1103]. The pure CSVâ€™s final row corroborates nonâ€‘zero final KL  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48]. Prefer ranks over absolute probabilities across families.

Prism sidecar is compatible but diagnosticâ€‘only: â€œk: 512; layers: [embed, 10, 22, 33]; KL delta at percentiles â‰ˆ +22.6/+23.7/+23.1 bits (baseline minus Prism)â€ with no rank milestones achieved by Prism  [001_layers_baseline/run-latest/output-gemma-2-27b.json:825â€“870].

Confirmed semantics are present and preferred for reporting: â€œL_semantic_confirmed: 46 (confirmed_source: "tuned")â€  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1451â€“1458].

**Quantitative Findings**

Layerâ€‘byâ€‘layer (pos/orig; entropy in bits; topâ€‘1 token). Bold indicates the confirmed semantic layer.

| Layer | Entropy | Topâ€‘1 |
|---:|---:|:---|
| 0 | 0.00050 | simply |
| 1 | 8.75823 |  |
| 2 | 8.76449 |  |
| 3 | 0.88567 | simply |
| 4 | 0.61827 | simply |
| 5 | 8.52026 | à¹² |
| 6 | 8.55309 | ï•€ |
| 7 | 8.54697 | î«¤ |
| 8 | 8.52874 | ïŒ |
| 9 | 8.52380 | ğ†£ |
| 10 | 8.34524 | dieÅ¿em |
| 11 | 8.49276 | ğ†£ |
| 12 | 8.32442 | î«¤ |
| 13 | 8.22249 | î«¤ |
| 14 | 7.87661 | î«¤ |
| 15 | 7.79248 | î«¤ |
| 16 | 7.97484 | dieÅ¿em |
| 17 | 7.78555 | dieÅ¿em |
| 18 | 7.29993 | Å¿icht |
| 19 | 7.52777 | dieÅ¿em |
| 20 | 6.20999 | Å¿icht |
| 21 | 6.45600 | Å¿icht |
| 22 | 6.37844 | dieÅ¿em |
| 23 | 7.01041 | dieÅ¿em |
| 24 | 6.49704 | dieÅ¿em |
| 25 | 6.99488 | dieÅ¿em |
| 26 | 6.21981 | dieÅ¿em |
| 27 | 6.70072 | dieÅ¿em |
| 28 | 7.14012 | dieÅ¿em |
| 29 | 7.57415 | dieÅ¿em |
| 30 | 7.33021 | dieÅ¿em |
| 31 | 7.56517 | dieÅ¿em |
| 32 | 8.87356 | zuÅ¿ammen |
| 33 | 6.94474 | dieÅ¿em |
| 34 | 7.73832 | dieÅ¿em |
| 35 | 7.65066 | dieÅ¿em |
| 36 | 7.65774 | dieÅ¿em |
| 37 | 7.57239 | dieÅ¿em |
| 38 | 7.55355 | ãƒ‘ãƒ³ãƒãƒ© |
| 39 | 7.23244 | dieÅ¿em |
| 40 | 8.71052 | å±•æ¿ |
| 41 | 7.08169 | dieÅ¿em |
| 42 | 7.05652 | dieÅ¿em |
| 43 | 7.08893 | dieÅ¿em |
| 44 | 7.56833 | dieÅ¿em |
| 45 | 7.14057 | GeÅ¿ch |
| **46** | **0.11805** | **Berlin** |

Control margin (ctl JSON): first_control_margin_pos = 0; max_control_margin = 0.9911  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1662â€“1665].

Ablation (noâ€‘filler): L_copy_orig = 0; L_sem_orig = 46; L_copy_nf = 3; L_sem_nf = 46; Î”L_copy = 3; Î”L_sem = 0  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1639â€“1646].

Î”H (bits) = entropy(L_copy) âˆ’ entropy(L_semantic) = 0.00050 âˆ’ 0.11805 = âˆ’0.11755  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2, 48]. Soft Î”Hâ‚ (k=1) = âˆ’0.11755 (L_copy_soft[1] = 0)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:956â€“963].

Confidence milestones (generic topâ€‘1 from pure CSV): p_top1 > 0.30 at Lâ€¯0; p_top1 > 0.60 at Lâ€¯0; finalâ€‘layer p_top1 = 0.9841 (Berlin)  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2, 48].

Rank milestones (norm lens; diagnostics): rank â‰¤ 10 at Lâ€¯46; rank â‰¤ 5 at Lâ€¯46; rank â‰¤ 1 at Lâ€¯46  [001_layers_baseline/run-latest/output-gemma-2-27b.json:947â€“949].

KL milestones: first_kl_below_1.0 = null; first_kl_below_0.5 = null; final KL â‰ˆ 1.135 bits (nonâ€‘zero), decreasing is not the primary signal here  [001_layers_baseline/run-latest/output-gemma-2-27b.json:945â€“947, 1084â€“1087].

Cosine milestones (norm): first cos_to_final â‰¥ 0.2 at Lâ€¯1; â‰¥ 0.4 at Lâ€¯46; â‰¥ 0.6 at Lâ€¯46; final cos_to_final = 0.9994  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1032â€“1037; 001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48].

Copy robustness (strict sweep): stability = â€œmixedâ€; earliest L_copy_strict at Ï„=0.70 â†’ 0; at Ï„=0.95 â†’ 0; norm_only_flags all false  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1001â€“1019].

Prism sidecar analysis: compatible=true, but rank milestones not achieved (null). Earlyâ€‘depth KL is much lower under Prism (baseline p25/p50/p75 â‰ˆ 42.01/43.15/42.51 bits vs Prism â‰ˆ 19.43/19.42/19.43; deltas â‰ˆ 22.57/23.73/23.08) with no earlier ranks and no copy flips to strict True  [001_layers_baseline/run-latest/output-gemma-2-27b.json:825â€“870]. Verdict: Regressive for semantics (KL lower but no rankâ€‘1), consistent with Prismâ€™s diagnostic role.

**Qualitative Patterns & Anomalies**

The model exhibits a strong copyâ€‘reflex on the adverb â€œsimplyâ€ at L0 (strict Ï„=0.95) with softâ€‘copy at early layers, e.g., â€œlayer 0 â€¦ copy_collapse = True â€¦ copy_strict@0.95 = True â€¦ copy_soft_k1@0.5 = Trueâ€  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2]. Berlin only becomes rankâ€‘1 at the final layer with high withinâ€‘lens confidence but a nonâ€‘zero final KL vs the modelâ€™s head: â€œL46 â€¦ p_answer = 0.9841 â€¦ kl_to_final_bits = 1.1352â€  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48]. This is a familyâ€‘typical finalâ€‘head calibration gap for Gemma; follow measurement guidance and prefer rankâ€‘based claims.

Negative control (â€œBerlin is the capital ofâ€): the model correctly predicts the country without leakage; topâ€‘5 begins with â€œGermany (0.868), the (0.065), and (0.0065), a (0.0062), Europe (0.0056)â€  [001_layers_baseline/run-latest/output-gemma-2-27b.json:11â€“20]. No â€œBerlinâ€ appears, so no semantic leakage.

Importantâ€‘word trajectory (records CSV): early layers are dominated by filler/copy (â€œsimplyâ€ is pervasive up to midâ€‘stack) and later by odd orthographic tokens (â€œdieÅ¿emâ€, â€œÅ¿ichtâ€), before collapsing to â€œBerlinâ€ at L46: â€œâ€¦ L10 topâ€‘1 â€˜dieÅ¿emâ€™ â€¦ L20 â€˜Å¿ichtâ€™ â€¦ L45 â€˜GeÅ¿châ€™ â€¦ L46 â€˜Berlinâ€™ (0.984)â€  [001_layers_baseline/run-latest/output-gemma-2-27b-records.csv:188, 358, 528, 806]. This suggests surfaceâ€‘form anchoring and orthographyâ€‘heavy features prior to semantic collapse.

Prompt ablation (no â€œsimplyâ€) delays strict copy from L0â†’L3 without affecting semantics (L_sem remains 46): â€œL_copy_orig: 0 â€¦ L_copy_nf: 3 â€¦ L_sem_nf: 46 â€¦ Î”L_copy = 3; Î”L_sem = 0â€  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1639â€“1646]. This supports the interpretation of grammaticalâ€‘style anchoring rather than changed semantics.

Restâ€‘mass sanity: final rest_mass is â‰ˆ 2.0eâ€‘07 at L46, consistent with concentrated mass after collapse  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48].

Rotation vs amplification: cosine to final rises early (â‰¥0.2 by L1) while KL remains very high until the end (first_kl_below_1.0 = null)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1032â€“1037, 945â€“947]. This is â€œearly direction, late calibrationâ€; finalâ€‘head calibration remains nonâ€‘trivial (warn_high_last_layer_kl=true; temp_estâ‰ˆ2.61; kl_after_temp_bitsâ‰ˆ0.567)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1084â€“1103].

Lens sanity: rawâ€‘vsâ€‘norm indicates high artefact risk and normâ€‘only semantics at L46 (â€œmax_kl_norm_vs_raw_bits: 99.54 â€¦ earliest_norm_only_semantic: 46 â€¦ tier: highâ€)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1071â€“1081]. Accordingly, statements emphasize ranks and withinâ€‘model trends.

Temperature robustness (teacherâ€‘head): at T=0.1, Berlin rankâ€¯1 (pâ‰ˆ0.990; entropyâ‰ˆ0.082); at T=2.0, Berlin remains rankâ€‘1 with much lower margin (pâ‰ˆ0.049; entropyâ‰ˆ12.63)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:669â€“744].

Checklist: RMS lens âœ“; LayerNorm bias removed n/a (RMS model)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1087â€“1091]; Entropy rise at unembed âœ“ (midâ€‘stack entropies â‰« teacher entropy)  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:10â€“32]; FP32 unâ€‘embed promoted âœ“ (â€œunembed_dtype": "torch.float32")  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1086â€“1088]; Punctuation/filler anchoring âœ“ (â€œsimplyâ€ early; orthographic tokens mid)  [001_layers_baseline/run-latest/output-gemma-2-27b-records.csv:69, 188]; Copyâ€‘reflex âœ“ (strict at L0; soft at L0â€“4)  [001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2, 5â€“6]; Preferred lens honored âœ“ (norm; confirmed semantics used)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:2150â€“2162]; Confirmed semantics reported âœ“ (source: tuned)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1451â€“1458]; Full dualâ€‘lens metrics cited âœ“ (pct_layers_kl_ge_1.0, n_norm_only_semantics_layers, earliest_norm_only_semantic, tier)  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1071â€“1081]; Tunedâ€‘lens attribution âœ“ (Î”KL_tuned/Î”KL_temp/Î”KL_rot at 25/50/75%) shows negative Î”KL_rot  [001_layers_baseline/run-latest/output-gemma-2-27b.json:2129â€“2147].

**Limitations & Data Quirks**

Final KL is not â‰ˆ0 (warn_high_last_layer_kl=true); treat final probabilities as familyâ€‘specific calibration and prefer rankâ€‘based milestones  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1084â€“1103]. Rawâ€‘vsâ€‘norm artefact risk is high with normâ€‘only semantics at L46; rely on confirmed semantics and rank milestones for onset  [001_layers_baseline/run-latest/output-gemma-2-27b.json:1047â€“1081]. Surfaceâ€‘mass metrics depend on tokenizer idiosyncrasies; crossâ€‘model mass comparisons are not advised.

**Model Fingerprint**

Gemmaâ€‘2â€‘27B: collapse at Lâ€¯46; final entropy â‰ˆâ€¯0.118â€¯bits; â€œBerlinâ€ first appears rankâ€¯1 only at the last layer.

---
Produced by OpenAI GPT-5 
