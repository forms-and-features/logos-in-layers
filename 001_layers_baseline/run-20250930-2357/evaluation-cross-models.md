**Result Synthesis**
- Copy-reflex appears only in the Gemma family at the very bottom layers. In gemma‑2‑9b, the pure next‑token CSV flags strict and soft copy at L0–3 (e.g., L0: “copy_collapse=True, copy_strict@0.95=True, copy_soft_k1@0.5=True” in 001_layers_baseline/run-latest/output-gemma-2-9b-pure-next-token.csv:2). Gemma‑2‑27B shows the same pattern (L0 strict and soft true; 001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2). All other models show no early copy flags (layers 0–3) under the configured detectors.
- Rank milestones (preferred cross‑model comparator) cluster late for most bases, with one clear early outlier. Examples: Qwen3‑8B baseline first_rank_le_{10,5,1} = 29/29/31 (001_layers_baseline/run-latest/output-Qwen3-8B.json:2068–2076); Qwen3‑14B = 32/33/36 (001_layers_baseline/run-latest/output-Qwen3-14B.json:2075–2084); Yi‑34B = le_10/le_5/le_1 not exposed in metrics block but L_semantic=44 (001_layers_baseline/run-latest/output-Yi-34B.json:954) and depth fraction 0.733 (001_layers_baseline/run-latest/output-Yi-34B.json:1054) indicate a mid‑late collapse. In contrast, Meta‑Llama‑3‑70B collapses much earlier by depth (L_semantic=40 of 80; fraction 0.50 at 001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:1073–1074). Treat KL percentiles as within‑model diagnostics: they generally decline with depth alongside rank improvements (e.g., Qwen3‑8B p25/p50/p75 KL deltas tuned vs norm ≈ +4.14/+4.00/+1.40 bits in 001_layers_baseline/run-latest/output-Qwen3-8B.json:2091–2108; interpret as better calibration/translation within model, not across).
- Confirmed semantics: report L_semantic_confirmed when present and the corroborating source. Examples: Meta‑Llama‑3‑8B confirmed at L25 from raw (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:1447–1450); Mistral‑7B‑v0.1 confirmed at L25 from raw (001_layers_baseline/run-latest/output-Mistral-7B-v0.1.json:1443–1446); Qwen3‑8B confirmed at L31 from raw (001_layers_baseline/run-latest/output-Qwen3-8B.json:1446–1449); Qwen3‑14B confirmed at L36 from raw (001_layers_baseline/run-latest/output-Qwen3-14B.json:1449–1452); Yi‑34B confirmed at L44 from tuned (001_layers_baseline/run-latest/output-Yi-34B.json:1479–1482); Gemma‑2‑9B confirmed at L42 from tuned (001_layers_baseline/run-latest/output-gemma-2-9b.json:1451–1454); Gemma‑2‑27B confirmed at L46 from tuned (001_layers_baseline/run-latest/output-gemma-2-27b.json:1455–1458). Honor measurement_guidance.preferred_lens_for_reporting per model (e.g., tuned for Qwen3‑8B at 001_layers_baseline/run-latest/output-Qwen3-8B.json:2139; norm for Gemma family at 001_layers_baseline/run-latest/output-gemma-2-9b.json:2152 and 001_layers_baseline/run-latest/output-gemma-2-27b.json:2160).
- Lens sanity (raw vs norm) strongly varies by family and should gate “early semantics” claims. High‑risk: Qwen3‑8B window max KL 38.096 (001_layers_baseline/run-latest/output-Qwen3-8B.json:1059–1066), full pct_layers_kl_ge_1.0 ≈ 0.757, tier=high (001_layers_baseline/run-latest/output-Qwen3-8B.json:1062–1070); Qwen3‑14B window max KL ≈ 98.58 and tier=high (001_layers_baseline/run-latest/output-Qwen3-14B.json:1041–1069); Yi‑34B window max KL ≈ 90.47 with seven norm‑only semantics in window and tier=high (001_layers_baseline/run-latest/output-Yi-34B.json:1061–1099). Medium‑risk: Meta‑Llama‑3‑8B has several window norm‑only layers near L25 and max window KL ≈ 5.26; full tier=medium (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:1025–1076). Low‑risk: Mistral‑Small‑24B shows empty window norm‑only, small max window KL ≈ 5.98, tier=low (001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:1041–1079). Where risk is high, rely on ranks and confirmed semantics and treat pre‑final “early semantics” as lens‑induced if they disappear under the raw lens within ±4–8 layers (cf. Yi‑34B window norm‑only at L44–48; 001_layers_baseline/run-latest/output-Yi-34B.json:1061–1090).
- Last‑layer head calibration: Gemma family shows non‑zero last‑layer KL (gemma‑2‑9b kl_to_final_bits ≈ 1.013 with warn_high_last_layer_kl=true at 001_layers_baseline/run-latest/output-gemma-2-9b.json:1090–1100; gemma‑2‑27b ≈ 1.135 at 001_layers_baseline/run-latest/output-gemma-2-27b.json:1084–1092). Treat across‑model final‑row probability differences as head‑calibration artifacts, not regressions; prefer rank thresholds. Others are ≈0 (e.g., Qwen3‑8B 0.0 at 001_layers_baseline/run-latest/output-Qwen3-8B.json:1075–1083; Mistral‑Small‑24B 0.0 at 001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:1081–1089).
- Family comparisons. Gemma‑2‑9B and 27B both exhibit a pronounced bottom‑layer copy reflex (CSV flags at L0) and very late semantics (L42/L46; 001_layers_baseline/run-latest/output-gemma-2-9b.json:936; 001_layers_baseline/run-latest/output-gemma-2-27b.json:940). Qwen (8B/14B/2.5‑72B) shows no early copy flags and later semantic fractions (0.861/0.900/1.000 at 001_layers_baseline/run-latest/output-Qwen3-8B.json:1029–1037; 001_layers_baseline/run-latest/output-Qwen3-14B.json:1033–1036; 001_layers_baseline/run-latest/output-Qwen2.5-72B.json:1073–1076). Within Llama, 70B collapses earlier by depth than 8B (0.50 vs 0.781; 001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:1073–1074 vs 001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:1121–1126). These patterns align with established logit‑lens observations that semantics typically emerge mid‑to‑late in depth and sharpen as layers combine residual features [Distill DOI:10.23915/distill.00033; arXiv:2012.14913].
- Δ and Δ̂ (normalized) for emergence timing: using strict copy where present, Δ̂ = (L_semantic − L_copy)/n_layers is maximal for Gemma (gemma‑2‑9b: L_copy=0 at 001_layers_baseline/run-latest/output-gemma-2-9b.json:1001–1012; n_layers=42 at 001_layers_baseline/run-latest/output-gemma-2-9b.json:1556–1561; L_semantic=42 at 001_layers_baseline/run-latest/output-gemma-2-9b.json:936 → Δ̂=1.0). Gemma‑2‑27B is similar (L_copy=0 at 001_layers_baseline/run-latest/output-gemma-2-27b.json:998–1012; n_layers=46 at 001_layers_baseline/run-latest/output-gemma-2-27b.json:1561–1565; L_semantic=46 at 001_layers_baseline/run-latest/output-gemma-2-27b.json:940 → Δ̂=1.0). Other models lack a strict/soft L_copy; Δ̂ is not defined and we lean on rank milestones and confirmed semantics.
- Entropy drop shape between L_copy and L_semantic: Gemma’s copy is near‑deterministic at L0 and semantics remain low‑entropy at the top, yielding small ΔH = H(L_copy) − H(L_semantic). For gemma‑2‑9b, H(L0)=1.67e‑05 bits vs H(L42)=0.3701 bits (001_layers_baseline/run-latest/output-gemma-2-9b-pure-next-token.csv:2 and 001_layers_baseline/run-latest/output-gemma-2-9b-pure-next-token.csv:48), a narrow −0.37‑bit step rather than a plunge. For gemma‑2‑27b, H(L0)=0.00050 vs H(L46)=0.1180 (001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:2 and 001_layers_baseline/run-latest/output-gemma-2-27b-pure-next-token.csv:48), ΔH≈−0.118 bits. For models without a copy layer, entropy trends taper down toward semantics (e.g., Meta‑Llama‑3‑8B H(L25)=16.81 bits at 001_layers_baseline/run-latest/output-Meta-Llama-3-8B-pure-next-token.csv:28), but we avoid constructing ΔH without L_copy.
- Cosine alignment (within‑model only): rising cos_to_final milestones corroborate direction alignment as depth increases. Examples: Meta‑Llama‑3‑8B reaches cos ≥ 0.2/0.4/0.6 at L20/L30/L32 (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:1115–1127); Mistral‑Small‑24B at ~35/40/40 (001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:1015–1026). Absolute cosine magnitudes are not compared across families.
- Early (< 70% depth) vs late (≥ 70%) semantics: only Meta‑Llama‑3‑70B is clearly early (0.50; 001_layers_baseline/run-latest/output-Meta-Llama-3-70B.json:1073–1074). All others are late: Llama‑3‑8B and Mistral‑7B at 0.781 (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:1121–1126; 001_layers_baseline/run-latest/output-Mistral-7B-v0.1.json:1025–1028), Mistral‑Small‑24B at 0.825 (001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:1033–1036), Qwen3‑8B at 0.861 (001_layers_baseline/run-latest/output-Qwen3-8B.json:1029–1036), Qwen3‑14B at 0.900 (001_layers_baseline/run-latest/output-Qwen3-14B.json:1033–1036), Qwen2.5‑72B at 1.000 (001_layers_baseline/run-latest/output-Qwen2.5-72B.json:1073–1076), Yi‑34B at 0.733 (001_layers_baseline/run-latest/output-Yi-34B.json:1053–1056), Gemma‑2‑9B/27B at 1.000 (001_layers_baseline/run-latest/output-gemma-2-9b.json:1035–1039; 001_layers_baseline/run-latest/output-gemma-2-27b.json:1039–1042). There is no consistent link between “sharper” collapse and wider d_model or more heads (contrast Mistral‑Small‑24B d_model=5120,n_heads=32 at 001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:1558–1560 vs Qwen3‑14B d_model=5120,n_heads=40 at 001_layers_baseline/run-latest/output-Qwen3-14B.json:1555–1557 and the absence of a copy reflex in both).
- Relation to public scores. Within the Llama family, earlier collapse coincides with higher published scores (Llama‑3‑70B: early 50% depth and strong MMLU/ARC; table supplied). Across families, Qwen2.5‑72B collapses latest (1.0 fraction) despite top MMLU, so any monotonic Δ̂→score claim fails. Use within‑family trends only.

Prism Summary Across Models
- Prism artifacts are compatible for all listed models, but rank milestones under Prism are typically null and KL deltas tend to be regressive vs norm. Examples: Qwen3‑8B Prism rank milestones null with KL deltas −0.36/−0.59/−7.03 bits at ~25/50/75% (001_layers_baseline/run-latest/output-Qwen3-8B.json:838–867). Gemma‑2‑9B shows −11.63/−10.33/−24.42 bits with null ranks (001_layers_baseline/run-latest/output-gemma-2-9b.json:838–867). Mistral‑Small‑24B shows −1.93/−5.98/−4.97 (001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:838–867). Llama‑3‑70B similarly reports Prism metrics but does not improve ranks. These patterns are strongest in families with high raw‑vs‑norm artifact risk (Qwen, Gemma, Yi), suggesting Prism’s shared‑decoder is not a substitute for the model head and should be treated as diagnostic only, with effects concentrated at early/mid layers but not translating to earlier ranks.

Tuned‑Lens Summary Across Models
- Tuned‑Lens improves rank milestones and reduces KL within models where measurement_guidance prefers tuned. Qwen3‑8B: tuned first_rank_le_{10,5,1} shift by +1/+2/+3 layers later (31→34 for le_1) with KL deltas ≈ +4.14/+4.00/+1.40 bits (001_layers_baseline/run-latest/output-Qwen3-8B.json:2068–2108) and ΔKL_rot ≈ +4.66/+4.08/+2.64 at 25/50/75% (001_layers_baseline/run-latest/output-Qwen3-8B.json:2128–2136), indicating rotation/translation contributes beyond per‑layer temperature (ΔKL_temp slightly negative). Qwen3‑14B: tuned deltas ≈ +1/+1/+3 layers later; KL deltas ≈ +4.68/+4.49/+3.90 bits; ΔKL_rot ≈ +4.76/+4.72/+3.66 (001_layers_baseline/run-latest/output-Qwen3-14B.json:2075–2146). Mistral‑7B‑v0.1: tuned pulls le_1 much later (25→32) with KL deltas ≈ +4.03/+3.75/+7.08; ΔKL_rot ≈ +4.27/+4.03/+2.85 and ΔKL_temp mixed (001_layers_baseline/run-latest/output-Mistral-7B-v0.1.json:2061–2140). Meta‑Llama‑3‑8B: tuned moves milestones later (25→32) with KL gains ≈ +4.35/+4.31/+3.95 and ΔKL_rot ≈ +4.53/+4.54/+3.73 (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:2064–2350). Where measurement_guidance prefers norm (Gemma family, Qwen2.5‑72B, Llama‑3‑70B), tuned either regresses or is not preferred; report norm baselines.
- Entropy drift (entropy − teacher_entropy_bits) at mid depth is positive for most models (e.g., Meta‑Llama‑3‑70B drift ≈ +14.34 bits at L40; 001_layers_baseline/run-latest/output-Meta-Llama-3-70B-pure-next-token.csv:42), indicating higher uncertainty under intermediate lenses than the teacher, consistent with logit‑lens expectations.
- Surface/Geometry/Coverage: report within‑model only. Example: Qwen3‑8B L_surface_to_meaning_norm=31 with answer_mass≈0.936 vs echo_mass≈0.011 (001_layers_baseline/run-latest/output-Qwen3-8B.json:1011–1016); L_geom_norm=34 with modest cosines (001_layers_baseline/run-latest/output-Qwen3-8B.json:1016–1024). Meta‑Llama‑3‑8B L_surface_to_meaning_norm=32 with answer_mass≈0.520 and echo_mass≈0.024 (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:1092–1100); L_geom_norm=26 (001_layers_baseline/run-latest/output-Meta-Llama-3-8B.json:1100–1110). These trends support a surface→meaning transition before or near confirmed semantics.

Copy Robustness (strict sweep)
- Stability and norm_only_flags: Gemma family is “mixed” but with norm_only_flags=false at all τ (001_layers_baseline/run-latest/output-gemma-2-9b.json:990–1017; 001_layers_baseline/run-latest/output-gemma-2-27b.json:994–1021). Others are “none” with no strict L_copy across τ and no norm‑only flags (e.g., Qwen3‑8B at 001_layers_baseline/run-latest/output-Qwen3-8B.json:984–1011; Mistral‑Small‑24B at 001_layers_baseline/run-latest/output-Mistral-Small-24B-Base-2501.json:988–1015). This reinforces that only Gemma exhibits an early copy reflex under our detectors.

Emergence Timing vs Scores
- Using the provided score table, earlier semantics within family correlate with higher scores for Llama (70B earlier than 8B). Across families, no consistent relation: Qwen2.5‑72B collapses latest by depth (1.0) while leading MMLU. Softer detectors (using earliest L_copy_soft[k]) do not alter this ranking materially (no soft detections outside Gemma);
entropy steepness (ΔH) is small where copy exists (Gemma), and non‑comparable elsewhere due to absent L_copy.

Interpretation Notes
- Use rank thresholds for cross‑model comparisons and treat KL as within‑model diagnostics. For families with high raw‑vs‑norm divergence or non‑zero final KL, avoid absolute probability comparisons across models and focus on milestones and within‑model trends, consistent with logit‑/tuned‑lens literature (e.g., feature aggregation through residual pathways; Distill DOI:10.23915/distill.00033; MLP key‑value memory formation arXiv:2012.14913).

**Misinterpretations in Existing EVALS**
- evaluation-gemma-2-27b.md: The Prism paragraph describes “KL delta at percentiles ≈ +22.6/+23.7/+23.1 bits (baseline minus Prism)” (001_layers_baseline/run-latest/evaluation-gemma-2-27b.md:96). Since Prism is diagnostic and not a teacher, calling this “Regressive for semantics” could be misread as a model regression. Suggest clarifying it as “Prism is regressive relative to the norm lens baseline (higher KL, no rank improvements), not relative to the final head.”
- evaluation-Meta-Llama-3-8B.md: The line “tuned collapse: first_rank_le_1 = 32 (later than baseline 25)” (001_layers_baseline/run-latest/evaluation-Meta-Llama-3-8B.md:37) is correct, but it risks implying tuned “worsens” the model. Tuned‑Lens is a translator; prefer phrasing “tuned ranks appear later under this translator,” keeping comparison within the lens space.
- evaluation-Qwen3-8B.md: The layer snapshots table lists non‑ASCII tokens without clarifying tokenizer artifacts; readers may over‑interpret early noise. Add a one‑liner that surface tokens at early layers are tokenizer‑dependent and not meaningful across families (001_layers_baseline/run-latest/evaluation-Qwen3-8B.md:19–42).

**Limitations**
- RMS‑lens can distort absolute probabilities; keep comparisons within model, not across differing normalisation schemes.
- Single‑prompt probing may over‑fit tokenizer quirks; copy‑collapse depth can change if wording or punctuation shifts.
- Attention patterns and MLP activations are not inspected—only residual projections—so entropy bumps caused by internal gating may be mis‑attributed.
- Un‑embed weights may be promoted to FP32 ("use_fp32_unembed": true) in some models, slightly shrinking entropy gaps; keep comparisons qualitative.
- Final‑lens vs final‑head mismatches (e.g., Gemma) can keep last‑layer KL > 0; prefer rank thresholds for cross‑model conclusions and treat KL trends qualitatively within model.
- Layer counts differ (8B ≈ 32 layers, 34B ≈ 60–80); compare relative depths, not absolute indices.
- Current results are correlation‑only; causal evidence (patching) awaits a later run.

---
Produced by OpenAI GPT-5

