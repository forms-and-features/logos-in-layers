
============================================================
EVALUATING MODEL: google/gemma-2-9b
============================================================
Loading model: google/gemma-2-9b...
Loaded pretrained model google/gemma-2-9b into HookedTransformer

=== NORMALIZATION ANALYSIS ========
Block LayerNorm type: RMSNormPre
⚠️  Non-vanilla norm detected (RMSNormPre) - norm-lens will be skipped to avoid distortion
Final LayerNorm type: RMSNormPre
=== END NORMALIZATION ANALYSIS ====


=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['<bos>', 'Question', ':', ' What', ' is', ' the', ' capital', ' of', ' Germany', '?', ' Answer', ':']
Computing layer-wise predictions (memory-efficient targeted caching)...

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using RAW residual stream (non-vanilla norms detected, skipping normalization to avoid distortion)
Note: Shown probabilities are softmax over top-k only (don't sum to 1)
------------------------------------------------------------
Layer  0 (embeddings):
  (entropy: -0.000):
   1. ':' (1.000000)
   2. ',' (0.000000)
   3. ' ' (0.000000)
   4. '.' (0.000000)
   5. '-' (0.000000)
   6. ' (' (0.000000)
   7. '

' (0.000000)
   8. '
' (0.000000)
   9. ';' (0.000000)
  10. ' :' (0.000000)
  11. '(' (0.000000)
  12. '/' (0.000000)
  13. '1' (0.000000)
  14. ''' (0.000000)
  15. '  ' (0.000000)
  16. ' -' (0.000000)
  17. '2' (0.000000)
  18. ')' (0.000000)
  19. ' "' (0.000000)
  20. '?' (0.000000)

Layer  1 (after block 0) (entropy: 0.000):
   1. ':' (1.000000)
   2. ' :' (0.000000)
   3. '：' (0.000000)
   4. '):' (0.000000)
   5. '.:' (0.000000)
   6. ';' (0.000000)
   7. ':' (0.000000)
   8. '':' (0.000000)
   9. '<bos>' (0.000000)
  10. '":' (0.000000)
  11. ':}' (0.000000)
  12. ':.' (0.000000)
  13. ':"' (0.000000)
  14. ':-' (0.000000)
  15. '}:' (0.000000)
  16. ']:' (0.000000)
  17. '():' (0.000000)
  18. ',' (0.000000)
  19. '?:' (0.000000)
  20. '::' (0.000000)

Layer  2 (after block 1) (entropy: 0.000):
   1. ':' (1.000000)
   2. '’:' (0.000000)
   3. ':' (0.000000)
   4. ' :' (0.000000)
   5. ':}' (0.000000)
   6. ':</' (0.000000)
   7. '!:' (0.000000)
   8. '】:' (0.000000)
   9. ':',' (0.000000)
  10. '：' (0.000000)
  11. '»:' (0.000000)
  12. ':");' (0.000000)
  13. '®:' (0.000000)
  14. ':",' (0.000000)
  15. '}}:' (0.000000)
  16. '+:' (0.000000)
  17. ':')' (0.000000)
  18. '”:' (0.000000)
  19. ' _:' (0.000000)
  20. '}:' (0.000000)

Layer  3 (after block 2) (entropy: 0.000):
   1. ':' (1.000000)
   2. '：' (0.000000)
   3. '}:' (0.000000)
   4. ' :' (0.000000)
   5. '✨:' (0.000000)
   6. '’:' (0.000000)
   7. '»:' (0.000000)
   8. ']:' (0.000000)
   9. '%:' (0.000000)
  10. ':}' (0.000000)
  11. '!:' (0.000000)
  12. '】:' (0.000000)
  13. '︰' (0.000000)
  14. '﹕' (0.000000)
  15. ' ：' (0.000000)
  16. '*:' (0.000000)
  17. '**:' (0.000000)
  18. ':',' (0.000000)
  19. ':",' (0.000000)
  20. '':' (0.000000)

Layer  4 (after block 3) (entropy: 0.000):
   1. ':' (1.000000)
   2. ']:' (0.000000)
   3. '’:' (0.000000)
   4. ':}' (0.000000)
   5. '：' (0.000000)
   6. ' :' (0.000000)
   7. '】:' (0.000000)
   8. '»:' (0.000000)
   9. '}:' (0.000000)
  10. '':' (0.000000)
  11. '):' (0.000000)
  12. '":' (0.000000)
  13. '”:' (0.000000)
  14. ':',' (0.000000)
  15. ':",' (0.000000)
  16. ':]' (0.000000)
  17. ' Umum' (0.000000)
  18. '.:' (0.000000)
  19. '$:' (0.000000)
  20. ':\' (0.000000)

Layer  5 (after block 4) (entropy: 0.000):
   1. ':' (1.000000)
   2. ' :' (0.000000)
   3. '：' (0.000000)
   4. '’:' (0.000000)
   5. '):' (0.000000)
   6. '":' (0.000000)
   7. '}:' (0.000000)
   8. '$:' (0.000000)
   9. '*:' (0.000000)
  10. '»:' (0.000000)
  11. '':' (0.000000)
  12. ':}' (0.000000)
  13. ':",' (0.000000)
  14. ':"' (0.000000)
  15. '<td>' (0.000000)
  16. '.:' (0.000000)
  17. ' Geheimnis' (0.000000)
  18. ']:' (0.000000)
  19. '»' (0.000000)
  20. '{' (0.000000)

Layer  6 (after block 5) (entropy: 0.000):
   1. ':' (1.000000)
   2. ' :' (0.000000)
   3. ' menjawab' (0.000000)
   4. ' Batalla' (0.000000)
   5. '：' (0.000000)
   6. ':}' (0.000000)
   7. '

' (0.000000)
   8. '':' (0.000000)
   9. ' Wassers' (0.000000)
  10. ' Grecs' (0.000000)
  11. ''' (0.000000)
  12. '":' (0.000000)
  13. ' Mexique' (0.000000)
  14. ' selaku' (0.000000)
  15. ' Gebir' (0.000000)
  16. ':\' (0.000000)
  17. ' Respuesta' (0.000000)
  18. ':.' (0.000000)
  19. ' Antwort' (0.000000)
  20. ' Münch' (0.000000)

Layer  7 (after block 6) (entropy: 0.000):
   1. ':' (1.000000)
   2. ' :' (0.000000)
   3. ' Grecs' (0.000000)
   4. ' Batalla' (0.000000)
   5. '’' (0.000000)
   6. ':}' (0.000000)
   7. ' Mexique' (0.000000)
   8. '：' (0.000000)
   9. '':' (0.000000)
  10. ' menjawab' (0.000000)
  11. '’:' (0.000000)
  12. ' Jesucristo' (0.000000)
  13. ' explica' (0.000000)
  14. ' =' (0.000000)
  15. '":' (0.000000)
  16. ' hjemmeside' (0.000000)
  17. ':",' (0.000000)
  18. ' resposta' (0.000000)
  19. ' japonais' (0.000000)
  20. ' chêne' (0.000000)

Layer  8 (after block 7) (entropy: 0.000):
   1. ':' (1.000000)
   2. ' answer' (0.000000)
   3. 'answer' (0.000000)
   4. ' Antwort' (0.000000)
   5. ' answers' (0.000000)
   6. 'Answer' (0.000000)
   7. ' resposta' (0.000000)
   8. ' Batalla' (0.000000)
   9. ' :' (0.000000)
  10. ' =' (0.000000)
  11. '：' (0.000000)
  12. ' Answer' (0.000000)
  13. ' menjawab' (0.000000)
  14. '2' (0.000000)
  15. 'resposta' (0.000000)
  16. '

' (0.000000)
  17. '  ' (0.000000)
  18. ' Respuesta' (0.000000)
  19. '":' (0.000000)
  20. ' Münch' (0.000000)

Layer  9 (after block 8) (entropy: 0.001):
   1. ':' (0.999895)
   2. ' answer' (0.000095)
   3. 'Answer' (0.000008)
   4. 'answer' (0.000001)
   5. '  ' (0.000000)
   6. '

' (0.000000)
   7. '1' (0.000000)
   8. '
' (0.000000)
   9. ' answers' (0.000000)
  10. '2' (0.000000)
  11. 's' (0.000000)
  12. ' :' (0.000000)
  13. ' =' (0.000000)
  14. ' Antwort' (0.000000)
  15. ' ' (0.000000)
  16. '<strong>' (0.000000)
  17. 'A' (0.000000)
  18. '' (0.000000)
  19. '3' (0.000000)
  20. '#' (0.000000)

Layer 10 (after block 9) (entropy: 0.118):
   1. ' answer' (0.975117)
   2. 'Answer' (0.024572)
   3. ':' (0.000265)
   4. 'answer' (0.000027)
   5. ' answers' (0.000018)
   6. ' Antwort' (0.000001)
   7. '
' (0.000000)
   8. 'A' (0.000000)
   9. '<strong>' (0.000000)
  10. ' Answer' (0.000000)
  11. '1' (0.000000)
  12. '

' (0.000000)
  13. 's' (0.000000)
  14. '  ' (0.000000)
  15. 'Yes' (0.000000)
  16. '2' (0.000000)
  17. 'a' (0.000000)
  18. '//' (0.000000)
  19. '#' (0.000000)
  20. ' =' (0.000000)

Layer 11 (after block 10) (entropy: 0.013):
   1. ':' (0.998299)
   2. ' answer' (0.001689)
   3. 'Answer' (0.000007)
   4. 'answer' (0.000005)
   5. ' answers' (0.000001)
   6. ' Answer' (0.000000)
   7. '
' (0.000000)
   8. ' =' (0.000000)
   9. 's' (0.000000)
  10. ' Antwort' (0.000000)
  11. 'Solution' (0.000000)
  12. 'a' (0.000000)
  13. 'e' (0.000000)
  14. ' ' (0.000000)
  15. ' :' (0.000000)
  16. 't' (0.000000)
  17. ' Hidup' (0.000000)
  18. 'A' (0.000000)
  19. 'B' (0.000000)
  20. 'i' (0.000000)

Layer 12 (after block 11) (entropy: 0.002):
   1. ':' (0.999864)
   2. 'A' (0.000066)
   3. 'Answer' (0.000027)
   4. 'a' (0.000025)
   5. '
' (0.000015)
   6. '1' (0.000002)
   7. 's' (0.000000)
   8. '2' (0.000000)
   9. 'E' (0.000000)
  10. 'B' (0.000000)
  11. ' ' (0.000000)
  12. 'i' (0.000000)
  13. '

' (0.000000)
  14. 'e' (0.000000)
  15. ' answer' (0.000000)
  16. '3' (0.000000)
  17. ' =' (0.000000)
  18. '  ' (0.000000)
  19. '' (0.000000)
  20. 'I' (0.000000)

Layer 13 (after block 12) (entropy: 0.078):
   1. ':' (0.985737)
   2. '
' (0.013378)
   3. 'a' (0.000811)
   4. 'A' (0.000041)
   5. '

' (0.000019)
   6. 's' (0.000009)
   7. '2' (0.000001)
   8. ' ' (0.000001)
   9. '1' (0.000001)
  10. 'Answer' (0.000000)
  11. 'e' (0.000000)
  12. '3' (0.000000)
  13. ' =' (0.000000)
  14. 'd' (0.000000)
  15. 'i' (0.000000)
  16. '  ' (0.000000)
  17. 'I' (0.000000)
  18. 'B' (0.000000)
  19. '//' (0.000000)
  20. '' (0.000000)

Layer 14 (after block 13) (entropy: 0.018):
   1. 'a' (0.997779)
   2. ':' (0.001198)
   3. 'A' (0.000649)
   4. '
' (0.000300)
   5. '

' (0.000020)
   6. '2' (0.000020)
   7. 's' (0.000017)
   8. '1' (0.000011)
   9. 'i' (0.000006)
  10. ' ' (0.000000)
  11. '3' (0.000000)
  12. 'The' (0.000000)
  13. 'B' (0.000000)
  14. 'I' (0.000000)
  15. 't' (0.000000)
  16. 'd' (0.000000)
  17. 'in' (0.000000)
  18. 'e' (0.000000)
  19. 'C' (0.000000)
  20. ' =' (0.000000)

Layer 15 (after block 14) (entropy: 0.382):
   1. 'a' (0.906525)
   2. 's' (0.070176)
   3. ':' (0.015800)
   4. 'A' (0.006463)
   5. '
' (0.000538)
   6. '

' (0.000264)
   7. '1' (0.000096)
   8. 'i' (0.000086)
   9. '2' (0.000041)
  10. 't' (0.000003)
  11. ' =' (0.000002)
  12. 'The' (0.000002)
  13. ' ' (0.000002)
  14. 'B' (0.000000)
  15. ' a' (0.000000)
  16. 'd' (0.000000)
  17. 'M' (0.000000)
  18. '3' (0.000000)
  19. 'an' (0.000000)
  20. 'x' (0.000000)

Layer 16 (after block 15) (entropy: 0.176):
   1. 'a' (0.958738)
   2. 'A' (0.040665)
   3. ':' (0.000300)
   4. 's' (0.000214)
   5. 'The' (0.000060)
   6. '1' (0.000013)
   7. 'M' (0.000003)
   8. ' ' (0.000002)
   9. ' a' (0.000002)
  10. '
' (0.000001)
  11. '2' (0.000001)
  12. '3' (0.000000)
  13. ' =' (0.000000)
  14. 't' (0.000000)
  15. 'i' (0.000000)
  16. 'B' (0.000000)
  17. 'C' (0.000000)
  18. 'x' (0.000000)
  19. 'L' (0.000000)
  20. 'd' (0.000000)

Layer 17 (after block 16) (entropy: 0.601):
   1. 's' (0.820125)
   2. 'a' (0.115485)
   3. 'A' (0.061596)
   4. ' ' (0.002518)
   5. '1' (0.000129)
   6. ':' (0.000101)
   7. ' a' (0.000044)
   8. ' the' (0.000001)
   9. 'The' (0.000000)
  10. '2' (0.000000)
  11. 'y' (0.000000)
  12. 'i' (0.000000)
  13. 'C' (0.000000)
  14. '3' (0.000000)
  15. 'M' (0.000000)
  16. '
' (0.000000)
  17. 't' (0.000000)
  18. 'B' (0.000000)
  19. ' to' (0.000000)
  20. ''' (0.000000)

Layer 18 (after block 17) (entropy: 1.074):
   1. ' ' (0.468759)
   2. 'a' (0.305792)
   3. 's' (0.221849)
   4. ' a' (0.003182)
   5. ' the' (0.000107)
   6. '1' (0.000107)
   7. ':' (0.000102)
   8. 'The' (0.000075)
   9. 'A' (0.000016)
  10. '’' (0.000006)
  11. ''' (0.000004)
  12. '2' (0.000001)
  13. 'do' (0.000000)
  14. ' to' (0.000000)
  15. '3' (0.000000)
  16. ' =' (0.000000)
  17. 'i' (0.000000)
  18. 'M' (0.000000)
  19. ' The' (0.000000)
  20. 'is' (0.000000)

Layer 19 (after block 18) (entropy: 1.057):
   1. ' ' (0.561628)
   2. ' the' (0.293448)
   3. ' a' (0.106469)
   4. 's' (0.035925)
   5. 'The' (0.002411)
   6. 'a' (0.000060)
   7. ':' (0.000046)
   8. ' to' (0.000011)
   9. ' =' (0.000000)
  10. ''' (0.000000)
  11. '’' (0.000000)
  12. 'A' (0.000000)
  13. ' The' (0.000000)
  14. ' is' (0.000000)
  15. '1' (0.000000)
  16. ' of' (0.000000)
  17. ' answer' (0.000000)
  18. 'i' (0.000000)
  19. '2' (0.000000)
  20. 'do' (0.000000)

Layer 20 (after block 19) (entropy: 0.026):
   1. ' the' (0.996267)
   2. ' a' (0.003173)
   3. ' ' (0.000559)
   4. 'The' (0.000000)
   5. 's' (0.000000)
   6. 'a' (0.000000)
   7. ':' (0.000000)
   8. ' to' (0.000000)
   9. ' is' (0.000000)
  10. '’' (0.000000)
  11. 'i' (0.000000)
  12. ' Gonçalves' (0.000000)
  13. ''' (0.000000)
  14. ' =' (0.000000)
  15. ' of' (0.000000)
  16. ' not' (0.000000)
  17. ' answer' (0.000000)
  18. ' The' (0.000000)
  19. 'A' (0.000000)
  20. 'as' (0.000000)

Layer 21 (after block 20) (entropy: 0.001):
   1. ' the' (0.999942)
   2. ' ' (0.000058)
   3. ' a' (0.000000)
   4. 'The' (0.000000)
   5. 's' (0.000000)
   6. ' The' (0.000000)
   7. ' to' (0.000000)
   8. ':' (0.000000)
   9. ''' (0.000000)
  10. ' is' (0.000000)
  11. 'a' (0.000000)
  12. ' of' (0.000000)
  13. ' It' (0.000000)
  14. ' "' (0.000000)
  15. ' Gonçalves' (0.000000)
  16. '’' (0.000000)
  17. 'i' (0.000000)
  18. '1' (0.000000)
  19. ' “' (0.000000)
  20. ' you' (0.000000)

Layer 22 (after block 21) (entropy: 0.016):
   1. ' the' (0.997757)
   2. ' ' (0.002163)
   3. 'The' (0.000079)
   4. ' a' (0.000000)
   5. ' The' (0.000000)
   6. ':' (0.000000)
   7. ' is' (0.000000)
   8. 's' (0.000000)
   9. 'a' (0.000000)
  10. ' to' (0.000000)
  11. ' Gonçalves' (0.000000)
  12. 'No' (0.000000)
  13. ',' (0.000000)
  14. 'It' (0.000000)
  15. ' you' (0.000000)
  16. '-' (0.000000)
  17. ''' (0.000000)
  18. 'One' (0.000000)
  19. ' of' (0.000000)
  20. 'A' (0.000000)

Layer 23 (after block 22) (entropy: 0.005):
   1. ' the' (0.999449)
   2. ' ' (0.000463)
   3. 'The' (0.000087)
   4. ' a' (0.000001)
   5. ' The' (0.000000)
   6. ':' (0.000000)
   7. ' is' (0.000000)
   8. ',' (0.000000)
   9. '-' (0.000000)
  10. 'a' (0.000000)
  11. ' “' (0.000000)
  12. ' to' (0.000000)
  13. ' in' (0.000000)
  14. 's' (0.000000)
  15. '.' (0.000000)
  16. ' "' (0.000000)
  17. '’' (0.000000)
  18. ' of' (0.000000)
  19. '1' (0.000000)
  20. ' It' (0.000000)

Layer 24 (after block 23) (entropy: 0.004):
   1. ' the' (0.999570)
   2. 'The' (0.000395)
   3. ' ' (0.000035)
   4. ' a' (0.000000)
   5. ' The' (0.000000)
   6. ' is' (0.000000)
   7. ',' (0.000000)
   8. '-' (0.000000)
   9. ' to' (0.000000)
  10. '.' (0.000000)
  11. ' Gonçalves' (0.000000)
  12. ' “' (0.000000)
  13. ' in' (0.000000)
  14. '“' (0.000000)
  15. ' It' (0.000000)
  16. 's' (0.000000)
  17. '1' (0.000000)
  18. 'It' (0.000000)
  19. 'a' (0.000000)
  20. ' ‘' (0.000000)

Layer 25 (after block 24) (entropy: 0.690):
   1. 'The' (0.556863)
   2. ' the' (0.442765)
   3. ' ' (0.000369)
   4. ' a' (0.000003)
   5. ' The' (0.000000)
   6. ',' (0.000000)
   7. ' is' (0.000000)
   8. '.' (0.000000)
   9. 'It' (0.000000)
  10. 'the' (0.000000)
  11. ' in' (0.000000)
  12. ' "' (0.000000)
  13. '-' (0.000000)
  14. 'You' (0.000000)
  15. 's' (0.000000)
  16. ' It' (0.000000)
  17. 'M' (0.000000)
  18. ':' (0.000000)
  19. ' “' (0.000000)
  20. '1' (0.000000)

Layer 26 (after block 25) (entropy: 0.011):
   1. 'The' (0.998562)
   2. ' the' (0.001437)
   3. ' ' (0.000001)
   4. ' The' (0.000000)
   5. ' a' (0.000000)
   6. ',' (0.000000)
   7. 'the' (0.000000)
   8. 'It' (0.000000)
   9. 's' (0.000000)
  10. 'a' (0.000000)
  11. '.' (0.000000)
  12. ' in' (0.000000)
  13. ' it' (0.000000)
  14. ' is' (0.000000)
  15. ' It' (0.000000)
  16. 'What' (0.000000)
  17. 'You' (0.000000)
  18. '-' (0.000000)
  19. 'A' (0.000000)
  20. '1' (0.000000)

Layer 27 (after block 26) (entropy: 0.000):
   1. 'The' (0.999999)
   2. ' the' (0.000001)
   3. ' The' (0.000000)
   4. ' ' (0.000000)
   5. ' a' (0.000000)
   6. 'It' (0.000000)
   7. 'the' (0.000000)
   8. ' It' (0.000000)
   9. '.' (0.000000)
  10. 'What' (0.000000)
  11. ' it' (0.000000)
  12. ',' (0.000000)
  13. 'A' (0.000000)
  14. 'You' (0.000000)
  15. ' is' (0.000000)
  16. '1' (0.000000)
  17. ' "' (0.000000)
  18. 'M' (0.000000)
  19. 's' (0.000000)
  20. '-' (0.000000)

Layer 28 (after block 27) (entropy: 0.005):
   1. 'The' (0.999367)
   2. ' The' (0.000633)
   3. ' the' (0.000001)
   4. ' ' (0.000000)
   5. ' a' (0.000000)
   6. ' it' (0.000000)
   7. 'It' (0.000000)
   8. ' It' (0.000000)
   9. ' is' (0.000000)
  10. 'What' (0.000000)
  11. ',' (0.000000)
  12. 'the' (0.000000)
  13. ' "' (0.000000)
  14. ' in' (0.000000)
  15. 'Great' (0.000000)
  16. 'You' (0.000000)
  17. ' I' (0.000000)
  18. 'A' (0.000000)
  19. 'Don' (0.000000)
  20. 'We' (0.000000)

Layer 29 (after block 28) (entropy: 0.000):
   1. 'The' (0.999998)
   2. ' The' (0.000002)
   3. ' the' (0.000000)
   4. ' ' (0.000000)
   5. ' a' (0.000000)
   6. 'It' (0.000000)
   7. ' "' (0.000000)
   8. 'We' (0.000000)
   9. 'You' (0.000000)
  10. ' it' (0.000000)
  11. 'A' (0.000000)
  12. ' in' (0.000000)
  13. ' It' (0.000000)
  14. ' “' (0.000000)
  15. 's' (0.000000)
  16. 'the' (0.000000)
  17. 'What' (0.000000)
  18. ' is' (0.000000)
  19. '

' (0.000000)
  20. ',' (0.000000)

Layer 30 (after block 29) (entropy: 0.011):
   1. ' The' (0.998576)
   2. 'The' (0.001420)
   3. ' ' (0.000003)
   4. ' the' (0.000001)
   5. ' a' (0.000000)
   6. ' in' (0.000000)
   7. 'We' (0.000000)
   8. 'the' (0.000000)
   9. ' it' (0.000000)
  10. ' It' (0.000000)
  11. 'What' (0.000000)
  12. ' I' (0.000000)
  13. 'It' (0.000000)
  14. ',' (0.000000)
  15. ' (' (0.000000)
  16. ' city' (0.000000)
  17. ' is' (0.000000)
  18. ' "' (0.000000)
  19. ' “' (0.000000)
  20. ' City' (0.000000)

Layer 31 (after block 30) (entropy: 0.080):
   1. ' The' (0.984417)
   2. 'The' (0.015583)
   3. ' the' (0.000000)
   4. ' ' (0.000000)
   5. ' a' (0.000000)
   6. ' Hauptstadt' (0.000000)
   7. ' capital' (0.000000)
   8. ' in' (0.000000)
   9. ' London' (0.000000)
  10. 'We' (0.000000)
  11. ' I' (0.000000)
  12. ' city' (0.000000)
  13. 'It' (0.000000)
  14. ' it' (0.000000)
  15. ' Capital' (0.000000)
  16. ' It' (0.000000)
  17. ' Berlin' (0.000000)
  18. 'the' (0.000000)
  19. ' (' (0.000000)
  20. ' Washington' (0.000000)

Layer 32 (after block 31) (entropy: 0.000):
   1. ' The' (1.000000)
   2. 'The' (0.000000)
   3. ' ' (0.000000)
   4. ' the' (0.000000)
   5. ' a' (0.000000)
   6. ' Berlin' (0.000000)
   7. ' I' (0.000000)
   8. ' It' (0.000000)
   9. ' Germany' (0.000000)
  10. 'We' (0.000000)
  11. ' We' (0.000000)
  12. ' (' (0.000000)
  13. ' in' (0.000000)
  14. ' capital' (0.000000)
  15. ' it' (0.000000)
  16. ',' (0.000000)
  17. ' Washington' (0.000000)
  18. 'Berlin' (0.000000)
  19. 'It' (0.000000)
  20. ' London' (0.000000)

Layer 33 (after block 32) (entropy: 0.000):
   1. ' The' (0.999965)
   2. ' ' (0.000035)
   3. 'The' (0.000000)
   4. ' the' (0.000000)
   5. ' Berlin' (0.000000)
   6. ' capital' (0.000000)
   7. ' Capital' (0.000000)
   8. ' I' (0.000000)
   9. 'Berlin' (0.000000)
  10. ' a' (0.000000)
  11. ' Hauptstadt' (0.000000)
  12. 'Capital' (0.000000)
  13. ' (' (0.000000)
  14. '  ' (0.000000)
  15. ' It' (0.000000)
  16. '1' (0.000000)
  17. ' Washington' (0.000000)
  18. 'We' (0.000000)
  19. ',' (0.000000)
  20. ' in' (0.000000)

Layer 34 (after block 33) (entropy: 0.010):
   1. ' Berlin' (0.998710)
   2. ' The' (0.001290)
   3. ' ' (0.000000)
   4. 'The' (0.000000)
   5. 'Berlin' (0.000000)
   6. ' the' (0.000000)
   7. ' Germany' (0.000000)
   8. ' capital' (0.000000)
   9. ' We' (0.000000)
  10. ' London' (0.000000)
  11. ' It' (0.000000)
  12. ' I' (0.000000)
  13. 'We' (0.000000)
  14. ' Washington' (0.000000)
  15. ' a' (0.000000)
  16. ' Hauptstadt' (0.000000)
  17. ' Capital' (0.000000)
  18. 'It' (0.000000)
  19. ' Paris' (0.000000)
  20. 'Capital' (0.000000)

Layer 35 (after block 34) (entropy: 0.000):
   1. ' Berlin' (1.000000)
   2. 'Berlin' (0.000000)
   3. ' The' (0.000000)
   4. ' ' (0.000000)
   5. 'The' (0.000000)
   6. ' Washington' (0.000000)
   7. ' the' (0.000000)
   8. ' Hauptstadt' (0.000000)
   9. ' Germany' (0.000000)
  10. ' London' (0.000000)
  11. ' capital' (0.000000)
  12. 'Washington' (0.000000)
  13. 'We' (0.000000)
  14. ' I' (0.000000)
  15. 'Capital' (0.000000)
  16. '1' (0.000000)
  17. ' Paris' (0.000000)
  18. ' a' (0.000000)
  19. '  ' (0.000000)
  20. 'This' (0.000000)

Layer 36 (after block 35) (entropy: 0.000):
   1. ' Berlin' (1.000000)
   2. 'Berlin' (0.000000)
   3. ' The' (0.000000)
   4. ' ' (0.000000)
   5. 'The' (0.000000)
   6. ' the' (0.000000)
   7. ' Hauptstadt' (0.000000)
   8. ' London' (0.000000)
   9. ' capital' (0.000000)
  10. ' Germany' (0.000000)
  11. ' Washington' (0.000000)
  12. ' Berlín' (0.000000)
  13. '1' (0.000000)
  14. 'We' (0.000000)
  15. ' I' (0.000000)
  16. 'It' (0.000000)
  17. '

' (0.000000)
  18. 'London' (0.000000)
  19. 'This' (0.000000)
  20. '  ' (0.000000)

Layer 37 (after block 36) (entropy: 0.000):
   1. ' Berlin' (1.000000)
   2. ' The' (0.000000)
   3. 'Berlin' (0.000000)
   4. ' ' (0.000000)
   5. 'The' (0.000000)
   6. ' Hauptstadt' (0.000000)
   7. ' the' (0.000000)
   8. '

' (0.000000)
   9. ' capital' (0.000000)
  10. ' London' (0.000000)
  11. ' Berlín' (0.000000)
  12. ' I' (0.000000)
  13. '1' (0.000000)
  14. ' Washington' (0.000000)
  15. ' (' (0.000000)
  16. 'We' (0.000000)
  17. '  ' (0.000000)
  18. 'Ber' (0.000000)
  19. 'This' (0.000000)
  20. 'New' (0.000000)

Layer 38 (after block 37) (entropy: 0.000):
   1. ' Berlin' (1.000000)
   2. 'Berlin' (0.000000)
   3. ' The' (0.000000)
   4. ' Berlín' (0.000000)
   5. 'Ber' (0.000000)
   6. ' ' (0.000000)
   7. ' Hauptstadt' (0.000000)
   8. 'The' (0.000000)
   9. ' Germany' (0.000000)
  10. ' capital' (0.000000)
  11. ' Ber' (0.000000)
  12. ' BERLIN' (0.000000)
  13. ' Paris' (0.000000)
  14. ' Washington' (0.000000)
  15. ' London' (0.000000)
  16. 'berlin' (0.000000)
  17. ' the' (0.000000)
  18. 'Capital' (0.000000)
  19. ' Capital' (0.000000)
  20. '

' (0.000000)

Layer 39 (after block 38) (entropy: 0.000):
   1. ' Berlin' (1.000000)
   2. 'Berlin' (0.000000)
   3. ' The' (0.000000)
   4. ' Berlín' (0.000000)
   5. 'Ber' (0.000000)
   6. ' Ber' (0.000000)
   7. 'The' (0.000000)
   8. ' ' (0.000000)
   9. ' It' (0.000000)
  10. ' Hauptstadt' (0.000000)
  11. ' This' (0.000000)
  12. ' I' (0.000000)
  13. '

' (0.000000)
  14. ' the' (0.000000)
  15. ' BERLIN' (0.000000)
  16. ' We' (0.000000)
  17. ' capital' (0.000000)
  18. ' There' (0.000000)
  19. 'This' (0.000000)
  20. 'BER' (0.000000)

Layer 40 (after block 39) (entropy: -0.000):
   1. ' Berlin' (1.000000)
   2. ' The' (0.000000)
   3. 'Berlin' (0.000000)
   4. ' ' (0.000000)
   5. '

' (0.000000)
   6. ' It' (0.000000)
   7. ' This' (0.000000)
   8. '  ' (0.000000)
   9. 'The' (0.000000)
  10. ' Ber' (0.000000)
  11. ' “' (0.000000)
  12. ' I' (0.000000)
  13. 'Ber' (0.000000)
  14. ' A' (0.000000)
  15. ' There' (0.000000)
  16. ' What' (0.000000)
  17. ' (' (0.000000)
  18. ' "' (0.000000)
  19. '
' (0.000000)
  20. ' We' (0.000000)

Layer 41 (after block 40) (entropy: -0.000):
   1. ' Berlin' (1.000000)
   2. 'Berlin' (0.000000)
   3. ' The' (0.000000)
   4. ' Germany' (0.000000)
   5. ' It' (0.000000)
   6. ' BERLIN' (0.000000)
   7. ' ' (0.000000)
   8. ' I' (0.000000)
   9. '

' (0.000000)
  10. ' Bonn' (0.000000)
  11. ' This' (0.000000)
  12. ' What' (0.000000)
  13. ' German' (0.000000)
  14. ' We' (0.000000)
  15. '  ' (0.000000)
  16. ' A' (0.000000)
  17. ' “' (0.000000)
  18. ' You' (0.000000)
  19. ' There' (0.000000)
  20. ' In' (0.000000)

Layer 42 (after block 41) (entropy: -0.000):
   1. ' Berlin' (1.000000)
   2. ' The' (0.000000)
   3. ' It' (0.000000)
   4. ' Bonn' (0.000000)
   5. ' Germany' (0.000000)
   6. ' BERLIN' (0.000000)
   7. '

' (0.000000)
   8. ' I' (0.000000)
   9. ' ' (0.000000)
  10. 'Berlin' (0.000000)
  11. ' What' (0.000000)
  12. ' This' (0.000000)
  13. ' “' (0.000000)
  14. ' Hamburg' (0.000000)
  15. ' A' (0.000000)
  16. '  ' (0.000000)
  17. ' Munich' (0.000000)
  18. ' "' (0.000000)
  19. ' There' (0.000000)
  20. ' You' (0.000000)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 1.388):
   1. ' Berlin' (0.850448)
   2. ' The' (0.022042)
   3. ' It' (0.017281)
   4. ' Bonn' (0.014459)
   5. ' Germany' (0.011866)
   6. ' BERLIN' (0.010677)
   7. '

' (0.009764)
   8. ' I' (0.008592)
   9. ' ' (0.008246)
  10. 'Berlin' (0.006725)
  11. ' What' (0.006601)
  12. ' This' (0.005325)
  13. ' “' (0.004984)
  14. ' Hamburg' (0.003755)
  15. ' A' (0.003557)
  16. '  ' (0.003520)
  17. ' Munich' (0.003347)
  18. ' "' (0.003067)
  19. ' There' (0.002914)
  20. ' You' (0.002827)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 3.692)
   1. ' a' (0.343119)
   2. ' one' (0.143769)
   3. ' the' (0.122252)
   4. ' home' (0.094358)
   5. ' known' (0.072611)
   6. ' not' (0.066297)
   7. ' an' (0.050883)
   8. ' full' (0.040089)
   9. ' famous' (0.036301)
  10. ' set' (0.030321)

Prompt: 'Berlin is the capital of'
  (entropy: 0.667)
   1. ' Germany' (0.899384)
   2. ' the' (0.071754)
   3. ' modern' (0.007902)
   4. ' a' (0.005432)
   5. ' ' (0.003482)
   6. ' Europe' (0.003160)
   7. ' German' (0.002913)
   8. ' both' (0.002491)
   9. ' Berlin' (0.001919)
  10. ' germany' (0.001564)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 3.686)
   1. '

' (0.264715)
   2. ' If' (0.185901)
   3. ' Berlin' (0.174559)
   4. ' Of' (0.076018)
   5. ' The' (0.075707)
   6. ' You' (0.055997)
   7. '
' (0.050243)
   8. ' That' (0.044829)
   9. ' Yes' (0.037684)
  10. ' And' (0.034347)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000)
   1. ' Berlin' (1.000000)
   2. ' The' (0.000000)
   3. ' It' (0.000000)
   4. ' Bonn' (0.000000)
   5. ' Germany' (0.000000)
   6. ' BERLIN' (0.000000)
   7. '

' (0.000000)
   8. ' I' (0.000000)
   9. ' ' (0.000000)
  10. 'Berlin' (0.000000)
  11. ' What' (0.000000)
  12. ' This' (0.000000)
  13. ' “' (0.000000)
  14. ' Hamburg' (0.000000)
  15. ' A' (0.000000)

Temperature 2.0:
  (entropy: 7.632)
   1. ' Berlin' (0.410835)
   2. ' The' (0.066141)
   3. ' It' (0.058563)
   4. ' Bonn' (0.053570)
   5. ' Germany' (0.048529)
   6. ' BERLIN' (0.046032)
   7. '

' (0.044021)
   8. ' I' (0.041294)
   9. ' ' (0.040455)
  10. 'Berlin' (0.036534)
  11. ' What' (0.036195)
  12. ' This' (0.032510)
  13. ' “' (0.031451)
  14. ' Hamburg' (0.027300)
  15. ' A' (0.026570)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 42
Model dimension: 3584
Number of heads: 16
Vocab size: 256000
Context length: 8192
=== END OF MODEL STATS ========

