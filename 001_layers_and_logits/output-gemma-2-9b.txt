
============================================================
EVALUATING MODEL: google/gemma-2-9b
============================================================
Loading model: google/gemma-2-9b...
Loaded pretrained model google/gemma-2-9b into HookedTransformer

=== NORMALIZATION ANALYSIS ========
Block normalization type: RMSNormPre
Using NORMALISED residual stream (RMS, no learnable scale)
Final normalization type: RMSNormPre
=== END NORMALIZATION ANALYSIS ====


=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['<bos>', 'Question', ':', ' What', ' is', ' the', ' capital', ' of', ' Germany', '?', ' Answer', ':']
Computing layer-wise predictions (memory-efficient targeted caching)...

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using NORMALIZED residual stream (RMS, no learnable scale)
Note: Shown probabilities are from full softmax (calibrated and comparable)
------------------------------------------------------------
Layer  0 (embeddings):
[diagnostic] No separate positional embedding hook; using only token embeddings for layer 0 residual.
  (entropy: 0.000 bits):
   1. ':' (1.000000)
   2. ',' (0.000000)
   3. ' ' (0.000000)
   4. '.' (0.000000)
   5. '-' (0.000000)
   6. ' (' (0.000000)
   7. '

' (0.000000)
   8. '
' (0.000000)
   9. ';' (0.000000)
  10. ' :' (0.000000)
  11. '(' (0.000000)
  12. '/' (0.000000)
  13. '1' (0.000000)
  14. ''' (0.000000)
  15. '  ' (0.000000)
  16. ' -' (0.000000)
  17. '2' (0.000000)
  18. ')' (0.000000)
  19. ' "' (0.000000)
  20. '?' (0.000000)

Layer  1 (after transformer block 0) (entropy: 0.000 bits):
   1. ':' (1.000000)
   2. ' :' (0.000000)
   3. '：' (0.000000)
   4. '):' (0.000000)
   5. '.:' (0.000000)
   6. ';' (0.000000)
   7. ':' (0.000000)
   8. '':' (0.000000)
   9. '<bos>' (0.000000)
  10. '":' (0.000000)
  11. ':}' (0.000000)
  12. ':.' (0.000000)
  13. ':"' (0.000000)
  14. ':-' (0.000000)
  15. '}:' (0.000000)
  16. ']:' (0.000000)
  17. '():' (0.000000)
  18. ',' (0.000000)
  19. '?:' (0.000000)
  20. '::' (0.000000)

Layer  2 (after transformer block 1) (entropy: 0.000 bits):
   1. ':' (1.000000)
   2. '’:' (0.000000)
   3. ':' (0.000000)
   4. ' :' (0.000000)
   5. ':}' (0.000000)
   6. ':</' (0.000000)
   7. '!:' (0.000000)
   8. '】:' (0.000000)
   9. ':',' (0.000000)
  10. '：' (0.000000)
  11. '»:' (0.000000)
  12. ':");' (0.000000)
  13. '®:' (0.000000)
  14. ':",' (0.000000)
  15. '}}:' (0.000000)
  16. '+:' (0.000000)
  17. ':')' (0.000000)
  18. '”:' (0.000000)
  19. ' _:' (0.000000)
  20. '}:' (0.000000)

Layer  3 (after transformer block 2) (entropy: 0.000 bits):
   1. ':' (1.000000)
   2. '：' (0.000000)
   3. '}:' (0.000000)
   4. ' :' (0.000000)
   5. '✨:' (0.000000)
   6. '’:' (0.000000)
   7. '»:' (0.000000)
   8. ']:' (0.000000)
   9. '%:' (0.000000)
  10. ':}' (0.000000)
  11. '!:' (0.000000)
  12. '】:' (0.000000)
  13. '︰' (0.000000)
  14. '﹕' (0.000000)
  15. ' ：' (0.000000)
  16. '*:' (0.000000)
  17. '**:' (0.000000)
  18. ':',' (0.000000)
  19. ':",' (0.000000)
  20. '':' (0.000000)

Layer  4 (after transformer block 3) (entropy: 0.000 bits):
   1. ':' (1.000000)
   2. ']:' (0.000000)
   3. '’:' (0.000000)
   4. ':}' (0.000000)
   5. '：' (0.000000)
   6. ' :' (0.000000)
   7. '】:' (0.000000)
   8. '»:' (0.000000)
   9. '}:' (0.000000)
  10. '':' (0.000000)
  11. '):' (0.000000)
  12. '":' (0.000000)
  13. '”:' (0.000000)
  14. ':',' (0.000000)
  15. ':",' (0.000000)
  16. ':]' (0.000000)
  17. ' Umum' (0.000000)
  18. '.:' (0.000000)
  19. '$:' (0.000000)
  20. ':\' (0.000000)

Layer  5 (after transformer block 4) (entropy: 0.000 bits):
   1. ':' (1.000000)
   2. ' :' (0.000000)
   3. '：' (0.000000)
   4. '’:' (0.000000)
   5. '):' (0.000000)
   6. '":' (0.000000)
   7. '}:' (0.000000)
   8. '$:' (0.000000)
   9. '*:' (0.000000)
  10. '»:' (0.000000)
  11. '':' (0.000000)
  12. ':}' (0.000000)
  13. ':",' (0.000000)
  14. ':"' (0.000000)
  15. '<td>' (0.000000)
  16. '.:' (0.000000)
  17. ' Geheimnis' (0.000000)
  18. ']:' (0.000000)
  19. '»' (0.000000)
  20. '{' (0.000000)

Layer  6 (after transformer block 5) (entropy: 0.000 bits):
   1. ':' (0.999997)
   2. ' :' (0.000001)
   3. ' menjawab' (0.000001)
   4. ' Batalla' (0.000000)
   5. '：' (0.000000)
   6. ':}' (0.000000)
   7. '

' (0.000000)
   8. '':' (0.000000)
   9. ' Wassers' (0.000000)
  10. ' Grecs' (0.000000)
  11. ''' (0.000000)
  12. '":' (0.000000)
  13. ' Mexique' (0.000000)
  14. ' selaku' (0.000000)
  15. ' Gebir' (0.000000)
  16. ':\' (0.000000)
  17. ' Respuesta' (0.000000)
  18. ':.' (0.000000)
  19. ' Antwort' (0.000000)
  20. ' Münch' (0.000000)

Layer  7 (after transformer block 6) (entropy: 0.000 bits):
   1. ':' (0.999995)
   2. ' :' (0.000002)
   3. ' Grecs' (0.000001)
   4. ' Batalla' (0.000001)
   5. '’' (0.000000)
   6. ':}' (0.000000)
   7. ' Mexique' (0.000000)
   8. '：' (0.000000)
   9. '':' (0.000000)
  10. ' menjawab' (0.000000)
  11. '’:' (0.000000)
  12. ' Jesucristo' (0.000000)
  13. ' explica' (0.000000)
  14. ' =' (0.000000)
  15. '":' (0.000000)
  16. ' hjemmeside' (0.000000)
  17. ':",' (0.000000)
  18. ' resposta' (0.000000)
  19. ' japonais' (0.000000)
  20. ' chêne' (0.000000)

Layer  8 (after transformer block 7) (entropy: 0.003 bits):
   1. ':' (0.999832)
   2. ' answer' (0.000142)
   3. 'answer' (0.000011)
   4. ' Antwort' (0.000004)
   5. ' answers' (0.000003)
   6. 'Answer' (0.000002)
   7. ' resposta' (0.000002)
   8. ' Batalla' (0.000002)
   9. ' :' (0.000001)
  10. ' =' (0.000000)
  11. '：' (0.000000)
  12. ' Answer' (0.000000)
  13. ' menjawab' (0.000000)
  14. '2' (0.000000)
  15. 'resposta' (0.000000)
  16. '

' (0.000000)
  17. '  ' (0.000000)
  18. ' Respuesta' (0.000000)
  19. '":' (0.000000)
  20. ' Münch' (0.000000)

Layer  9 (after transformer block 8) (entropy: 0.079 bits):
   1. ':' (0.992037)
   2. ' answer' (0.005542)
   3. 'Answer' (0.001427)
   4. 'answer' (0.000410)
   5. '  ' (0.000224)
   6. '

' (0.000105)
   7. '1' (0.000080)
   8. '
' (0.000053)
   9. ' answers' (0.000046)
  10. '2' (0.000026)
  11. 's' (0.000016)
  12. ' :' (0.000008)
  13. ' =' (0.000006)
  14. ' Antwort' (0.000005)
  15. ' ' (0.000005)
  16. '<strong>' (0.000004)
  17. 'A' (0.000003)
  18. '' (0.000002)
  19. '3' (0.000002)
  20. '#' (0.000001)

Layer 10 (after transformer block 9) (entropy: 0.738 bits):
   1. ' answer' (0.848502)
   2. 'Answer' (0.130095)
   3. ':' (0.012951)
   4. 'answer' (0.004006)
   5. ' answers' (0.003277)
   6. ' Antwort' (0.000810)
   7. '
' (0.000162)
   8. 'A' (0.000062)
   9. '<strong>' (0.000050)
  10. ' Answer' (0.000029)
  11. '1' (0.000016)
  12. '

' (0.000013)
  13. 's' (0.000009)
  14. '  ' (0.000004)
  15. 'Yes' (0.000004)
  16. '2' (0.000003)
  17. 'a' (0.000002)
  18. '//' (0.000002)
  19. '#' (0.000002)
  20. ' =' (0.000001)

Layer 11 (after transformer block 10) (entropy: 0.326 bits):
   1. ':' (0.952850)
   2. ' answer' (0.039913)
   3. 'Answer' (0.002517)
   4. 'answer' (0.002112)
   5. ' answers' (0.000743)
   6. ' Answer' (0.000488)
   7. '
' (0.000295)
   8. ' =' (0.000232)
   9. 's' (0.000164)
  10. ' Antwort' (0.000118)
  11. 'Solution' (0.000116)
  12. 'a' (0.000103)
  13. 'e' (0.000089)
  14. ' ' (0.000071)
  15. ' :' (0.000067)
  16. 't' (0.000039)
  17. ' Hidup' (0.000026)
  18. 'A' (0.000024)
  19. 'B' (0.000019)
  20. 'i' (0.000015)

Layer 12 (after transformer block 11) (entropy: 0.279 bits):
   1. ':' (0.969413)
   2. 'A' (0.009639)
   3. 'Answer' (0.006272)
   4. 'a' (0.006081)
   5. '
' (0.004792)
   6. '1' (0.001591)
   7. 's' (0.000765)
   8. '2' (0.000420)
   9. 'E' (0.000229)
  10. 'B' (0.000165)
  11. ' ' (0.000116)
  12. 'i' (0.000108)
  13. '

' (0.000103)
  14. 'e' (0.000096)
  15. ' answer' (0.000076)
  16. '3' (0.000040)
  17. ' =' (0.000032)
  18. '  ' (0.000030)
  19. '' (0.000016)
  20. 'I' (0.000015)

Layer 13 (after transformer block 12) (entropy: 0.968 bits):
   1. ':' (0.826259)
   2. '
' (0.115519)
   3. 'a' (0.032041)
   4. 'A' (0.008207)
   5. '

' (0.005709)
   6. 's' (0.004144)
   7. '2' (0.001738)
   8. ' ' (0.001737)
   9. '1' (0.001439)
  10. 'Answer' (0.000628)
  11. 'e' (0.000509)
  12. '3' (0.000438)
  13. ' =' (0.000429)
  14. 'd' (0.000261)
  15. 'i' (0.000203)
  16. '  ' (0.000194)
  17. 'I' (0.000161)
  18. 'B' (0.000145)
  19. '//' (0.000134)
  20. '' (0.000105)

Layer 14 (after transformer block 13) (entropy: 1.081 bits):
   1. 'a' (0.840068)
   2. ':' (0.050166)
   3. 'A' (0.038807)
   4. '
' (0.028069)
   5. '

' (0.009117)
   6. '2' (0.008967)
   7. 's' (0.008440)
   8. '1' (0.007098)
   9. 'i' (0.005610)
  10. ' ' (0.001395)
  11. '3' (0.000512)
  12. 'The' (0.000423)
  13. 'B' (0.000276)
  14. 'I' (0.000241)
  15. 't' (0.000195)
  16. 'd' (0.000143)
  17. 'in' (0.000137)
  18. 'e' (0.000133)
  19. 'C' (0.000115)
  20. ' =' (0.000089)

Layer 15 (after transformer block 14) (entropy: 2.239 bits):
   1. 'a' (0.541419)
   2. 's' (0.189375)
   3. ':' (0.102674)
   4. 'A' (0.071129)
   5. '
' (0.025637)
   6. '

' (0.019138)
   7. '1' (0.012632)
   8. 'i' (0.012053)
   9. '2' (0.008931)
  10. 't' (0.003168)
  11. ' =' (0.002752)
  12. 'The' (0.002534)
  13. ' ' (0.002403)
  14. 'B' (0.001304)
  15. ' a' (0.000954)
  16. 'd' (0.000894)
  17. 'M' (0.000860)
  18. '3' (0.000792)
  19. 'an' (0.000685)
  20. 'x' (0.000666)

Layer 16 (after transformer block 15) (entropy: 1.679 bits):
   1. 'a' (0.676192)
   2. 'A' (0.203796)
   3. ':' (0.031615)
   4. 's' (0.027831)
   5. 'The' (0.017185)
   6. '1' (0.009616)
   7. 'M' (0.005477)
   8. ' ' (0.004996)
   9. ' a' (0.004795)
  10. '
' (0.003713)
  11. '2' (0.003136)
  12. '3' (0.002222)
  13. ' =' (0.001488)
  14. 't' (0.001367)
  15. 'i' (0.001364)
  16. 'B' (0.001266)
  17. 'C' (0.001261)
  18. 'x' (0.001151)
  19. 'L' (0.000888)
  20. 'd' (0.000640)

Layer 17 (after transformer block 16) (entropy: 2.203 bits):
   1. 's' (0.463498)
   2. 'a' (0.228624)
   3. 'A' (0.182252)
   4. ' ' (0.057561)
   5. '1' (0.019716)
   6. ':' (0.018058)
   7. ' a' (0.013361)
   8. ' the' (0.002829)
   9. 'The' (0.002137)
  10. '2' (0.002057)
  11. 'y' (0.001515)
  12. 'i' (0.001340)
  13. 'C' (0.001242)
  14. '3' (0.001207)
  15. 'M' (0.001072)
  16. '
' (0.001003)
  17. 't' (0.000873)
  18. 'B' (0.000679)
  19. ' to' (0.000504)
  20. ''' (0.000472)

Layer 18 (after transformer block 17) (entropy: 2.402 bits):
   1. ' ' (0.324423)
   2. 'a' (0.279006)
   3. 's' (0.249141)
   4. ' a' (0.055713)
   5. ' the' (0.016853)
   6. '1' (0.016849)
   7. ':' (0.016533)
   8. 'The' (0.014877)
   9. 'A' (0.008594)
  10. '’' (0.005983)
  11. ''' (0.005254)
  12. '2' (0.002856)
  13. 'do' (0.001018)
  14. ' to' (0.000844)
  15. '3' (0.000524)
  16. ' =' (0.000478)
  17. 'i' (0.000391)
  18. 'M' (0.000227)
  19. ' The' (0.000226)
  20. 'is' (0.000208)

Layer 19 (after transformer block 18) (entropy: 2.453 bits):
   1. ' ' (0.323201)
   2. ' the' (0.260402)
   3. ' a' (0.185787)
   4. 's' (0.129389)
   5. 'The' (0.052636)
   6. 'a' (0.015429)
   7. ':' (0.014104)
   8. ' to' (0.008845)
   9. ' =' (0.002785)
  10. ''' (0.002208)
  11. '’' (0.001689)
  12. 'A' (0.000917)
  13. ' The' (0.000496)
  14. ' is' (0.000462)
  15. '1' (0.000460)
  16. ' of' (0.000339)
  17. ' answer' (0.000265)
  18. 'i' (0.000256)
  19. '2' (0.000174)
  20. 'do' (0.000158)

Layer 20 (after transformer block 19) (entropy: 1.056 bits):
   1. ' the' (0.784928)
   2. ' a' (0.127910)
   3. ' ' (0.073972)
   4. 'The' (0.003943)
   5. 's' (0.003594)
   6. 'a' (0.002306)
   7. ':' (0.001142)
   8. ' to' (0.000673)
   9. ' is' (0.000310)
  10. '’' (0.000292)
  11. 'i' (0.000230)
  12. ' Gonçalves' (0.000138)
  13. ''' (0.000117)
  14. ' =' (0.000110)
  15. ' of' (0.000101)
  16. ' not' (0.000077)
  17. ' answer' (0.000049)
  18. ' The' (0.000041)
  19. 'A' (0.000037)
  20. 'as' (0.000032)

Layer 21 (after transformer block 20) (entropy: 0.399 bits):
   1. ' the' (0.933892)
   2. ' ' (0.058077)
   3. ' a' (0.005435)
   4. 'The' (0.001934)
   5. 's' (0.000161)
   6. ' The' (0.000112)
   7. ' to' (0.000084)
   8. ':' (0.000067)
   9. ''' (0.000048)
  10. ' is' (0.000039)
  11. 'a' (0.000029)
  12. ' of' (0.000023)
  13. ' It' (0.000023)
  14. ' "' (0.000017)
  15. ' Gonçalves' (0.000014)
  16. '’' (0.000011)
  17. 'i' (0.000009)
  18. '1' (0.000009)
  19. ' “' (0.000009)
  20. ' you' (0.000008)

Layer 22 (after transformer block 21) (entropy: 1.048 bits):
   1. ' the' (0.769772)
   2. ' ' (0.154299)
   3. 'The' (0.064842)
   4. ' a' (0.009820)
   5. ' The' (0.000582)
   6. ':' (0.000190)
   7. ' is' (0.000163)
   8. 's' (0.000066)
   9. 'a' (0.000047)
  10. ' to' (0.000040)
  11. ' Gonçalves' (0.000035)
  12. 'No' (0.000023)
  13. ',' (0.000023)
  14. 'It' (0.000017)
  15. ' you' (0.000015)
  16. '-' (0.000014)
  17. ''' (0.000014)
  18. 'One' (0.000013)
  19. ' of' (0.000012)
  20. 'A' (0.000012)

Layer 23 (after transformer block 22) (entropy: 0.954 bits):
   1. ' the' (0.811817)
   2. ' ' (0.102738)
   3. 'The' (0.065497)
   4. ' a' (0.019716)
   5. ' The' (0.000109)
   6. ':' (0.000044)
   7. ' is' (0.000018)
   8. ',' (0.000010)
   9. '-' (0.000010)
  10. 'a' (0.000009)
  11. ' “' (0.000007)
  12. ' to' (0.000005)
  13. ' in' (0.000005)
  14. 's' (0.000003)
  15. '.' (0.000003)
  16. ' "' (0.000002)
  17. '’' (0.000002)
  18. ' of' (0.000001)
  19. '1' (0.000001)
  20. ' It' (0.000001)

Layer 24 (after transformer block 23) (entropy: 1.014 bits):
   1. ' the' (0.788103)
   2. 'The' (0.126550)
   3. ' ' (0.071902)
   4. ' a' (0.011553)
   5. ' The' (0.001783)
   6. ' is' (0.000032)
   7. ',' (0.000014)
   8. '-' (0.000012)
   9. ' to' (0.000011)
  10. '.' (0.000009)
  11. ' Gonçalves' (0.000007)
  12. ' “' (0.000005)
  13. ' in' (0.000005)
  14. '“' (0.000003)
  15. ' It' (0.000002)
  16. 's' (0.000002)
  17. '1' (0.000002)
  18. 'It' (0.000002)
  19. 'a' (0.000002)
  20. ' ‘' (0.000002)

Layer 25 (after transformer block 24) (entropy: 1.539 bits):
   1. 'The' (0.450382)
   2. ' the' (0.427769)
   3. ' ' (0.087151)
   4. ' a' (0.030290)
   5. ' The' (0.004184)
   6. ',' (0.000090)
   7. ' is' (0.000064)
   8. '.' (0.000019)
   9. 'It' (0.000012)
  10. 'the' (0.000010)
  11. ' in' (0.000007)
  12. ' "' (0.000005)
  13. '-' (0.000004)
  14. 'You' (0.000003)
  15. 's' (0.000003)
  16. ' It' (0.000003)
  17. 'M' (0.000002)
  18. ':' (0.000002)
  19. ' “' (0.000001)
  20. '1' (0.000001)

Layer 26 (after transformer block 25) (entropy: 1.155 bits):
   1. 'The' (0.746047)
   2. ' the' (0.178837)
   3. ' ' (0.033034)
   4. ' The' (0.026609)
   5. ' a' (0.015410)
   6. ',' (0.000018)
   7. 'the' (0.000014)
   8. 'It' (0.000008)
   9. 's' (0.000006)
  10. 'a' (0.000003)
  11. '.' (0.000003)
  12. ' in' (0.000003)
  13. ' it' (0.000003)
  14. ' is' (0.000002)
  15. ' It' (0.000001)
  16. 'What' (0.000001)
  17. 'You' (0.000001)
  18. '-' (0.000001)
  19. 'A' (0.000000)
  20. '1' (0.000000)

Layer 27 (after transformer block 26) (entropy: 0.492 bits):
   1. 'The' (0.922515)
   2. ' the' (0.051450)
   3. ' The' (0.016445)
   4. ' ' (0.009184)
   5. ' a' (0.000354)
   6. 'It' (0.000021)
   7. 'the' (0.000005)
   8. ' It' (0.000004)
   9. '.' (0.000004)
  10. 'What' (0.000003)
  11. ' it' (0.000003)
  12. ',' (0.000002)
  13. 'A' (0.000002)
  14. 'You' (0.000002)
  15. ' is' (0.000001)
  16. '1' (0.000001)
  17. ' "' (0.000001)
  18. 'M' (0.000001)
  19. 's' (0.000001)
  20. '-' (0.000001)

Layer 28 (after transformer block 27) (entropy: 1.037 bits):
   1. 'The' (0.766396)
   2. ' The' (0.172169)
   3. ' the' (0.041292)
   4. ' ' (0.019896)
   5. ' a' (0.000233)
   6. ' it' (0.000003)
   7. 'It' (0.000003)
   8. ' It' (0.000003)
   9. ' is' (0.000001)
  10. 'What' (0.000001)
  11. ',' (0.000001)
  12. 'the' (0.000001)
  13. ' "' (0.000000)
  14. ' in' (0.000000)
  15. 'Great' (0.000000)
  16. 'You' (0.000000)
  17. ' I' (0.000000)
  18. 'A' (0.000000)
  19. 'Don' (0.000000)
  20. 'We' (0.000000)

Layer 29 (after transformer block 28) (entropy: 0.773 bits):
   1. 'The' (0.862892)
   2. ' The' (0.071621)
   3. ' the' (0.044939)
   4. ' ' (0.020471)
   5. ' a' (0.000070)
   6. 'It' (0.000002)
   7. ' "' (0.000001)
   8. 'We' (0.000001)
   9. 'You' (0.000001)
  10. ' it' (0.000001)
  11. 'A' (0.000001)
  12. ' in' (0.000000)
  13. ' It' (0.000000)
  14. ' “' (0.000000)
  15. 's' (0.000000)
  16. 'the' (0.000000)
  17. 'What' (0.000000)
  18. ' is' (0.000000)
  19. '

' (0.000000)
  20. ',' (0.000000)

Layer 30 (after transformer block 29) (entropy: 1.353 bits):
   1. ' The' (0.669707)
   2. 'The' (0.207030)
   3. ' ' (0.068828)
   4. ' the' (0.054377)
   5. ' a' (0.000053)
   6. ' in' (0.000001)
   7. 'We' (0.000001)
   8. 'the' (0.000001)
   9. ' it' (0.000000)
  10. ' It' (0.000000)
  11. 'What' (0.000000)
  12. ' I' (0.000000)
  13. 'It' (0.000000)
  14. ',' (0.000000)
  15. ' (' (0.000000)
  16. ' city' (0.000000)
  17. ' is' (0.000000)
  18. ' "' (0.000000)
  19. ' “' (0.000000)
  20. ' City' (0.000000)

Layer 31 (after transformer block 30) (entropy: 1.277 bits):
   1. ' The' (0.617391)
   2. 'The' (0.311895)
   3. ' the' (0.056517)
   4. ' ' (0.014075)
   5. ' a' (0.000056)
   6. ' Hauptstadt' (0.000026)
   7. ' capital' (0.000012)
   8. ' in' (0.000008)
   9. ' London' (0.000004)
  10. 'We' (0.000003)
  11. ' I' (0.000002)
  12. ' city' (0.000002)
  13. 'It' (0.000001)
  14. ' it' (0.000001)
  15. ' Capital' (0.000001)
  16. ' It' (0.000001)
  17. ' Berlin' (0.000001)
  18. 'the' (0.000001)
  19. ' (' (0.000001)
  20. ' Washington' (0.000001)

Layer 32 (after transformer block 31) (entropy: 0.786 bits):
   1. ' The' (0.860339)
   2. 'The' (0.076938)
   3. ' ' (0.035146)
   4. ' the' (0.027432)
   5. ' a' (0.000063)
   6. ' Berlin' (0.000037)
   7. ' I' (0.000009)
   8. ' It' (0.000008)
   9. ' Germany' (0.000004)
  10. 'We' (0.000004)
  11. ' We' (0.000003)
  12. ' (' (0.000003)
  13. ' in' (0.000003)
  14. ' capital' (0.000003)
  15. ' it' (0.000002)
  16. ',' (0.000001)
  17. ' Washington' (0.000001)
  18. 'Berlin' (0.000001)
  19. 'It' (0.000001)
  20. ' London' (0.000001)

Layer 33 (after transformer block 32) (entropy: 1.136 bits):
   1. ' The' (0.755194)
   2. ' ' (0.169407)
   3. 'The' (0.047612)
   4. ' the' (0.016204)
   5. ' Berlin' (0.006657)
   6. ' capital' (0.004341)
   7. ' Capital' (0.000195)
   8. ' I' (0.000117)
   9. 'Berlin' (0.000072)
  10. ' a' (0.000060)
  11. ' Hauptstadt' (0.000035)
  12. 'Capital' (0.000032)
  13. ' (' (0.000020)
  14. '  ' (0.000010)
  15. ' It' (0.000009)
  16. '1' (0.000008)
  17. ' Washington' (0.000008)
  18. 'We' (0.000007)
  19. ',' (0.000007)
  20. ' in' (0.000006)

Layer 34 (after transformer block 33) (entropy: 1.150 bits):
   1. ' Berlin' (0.687240)
   2. ' The' (0.271646)
   3. ' ' (0.018773)
   4. 'The' (0.012311)
   5. 'Berlin' (0.005731)
   6. ' the' (0.003429)
   7. ' Germany' (0.000328)
   8. ' capital' (0.000322)
   9. ' We' (0.000062)
  10. ' London' (0.000047)
  11. ' It' (0.000023)
  12. ' I' (0.000018)
  13. 'We' (0.000018)
  14. ' Washington' (0.000017)
  15. ' a' (0.000011)
  16. ' Hauptstadt' (0.000008)
  17. ' Capital' (0.000007)
  18. 'It' (0.000005)
  19. ' Paris' (0.000003)
  20. 'Capital' (0.000002)

Layer 35 (after transformer block 34) (entropy: 0.235 bits):
   1. ' Berlin' (0.970546)
   2. 'Berlin' (0.014778)
   3. ' The' (0.011874)
   4. ' ' (0.001549)
   5. 'The' (0.001088)
   6. ' Washington' (0.000054)
   7. ' the' (0.000034)
   8. ' Hauptstadt' (0.000026)
   9. ' Germany' (0.000021)
  10. ' London' (0.000013)
  11. ' capital' (0.000012)
  12. 'Washington' (0.000002)
  13. 'We' (0.000001)
  14. ' I' (0.000001)
  15. 'Capital' (0.000001)
  16. '1' (0.000001)
  17. ' Paris' (0.000001)
  18. ' a' (0.000000)
  19. '  ' (0.000000)
  20. 'This' (0.000000)

Layer 36 (after transformer block 35) (entropy: 0.079 bits):
   1. ' Berlin' (0.991194)
   2. 'Berlin' (0.007466)
   3. ' The' (0.001146)
   4. ' ' (0.000132)
   5. 'The' (0.000057)
   6. ' the' (0.000001)
   7. ' Hauptstadt' (0.000001)
   8. ' London' (0.000001)
   9. ' capital' (0.000001)
  10. ' Germany' (0.000000)
  11. ' Washington' (0.000000)
  12. ' Berlín' (0.000000)
  13. '1' (0.000000)
  14. 'We' (0.000000)
  15. ' I' (0.000000)
  16. 'It' (0.000000)
  17. '

' (0.000000)
  18. 'London' (0.000000)
  19. 'This' (0.000000)
  20. '  ' (0.000000)

Layer 37 (after transformer block 36) (entropy: 0.166 bits):
   1. ' Berlin' (0.979762)
   2. ' The' (0.015262)
   3. 'Berlin' (0.003297)
   4. ' ' (0.000974)
   5. 'The' (0.000675)
   6. ' Hauptstadt' (0.000012)
   7. ' the' (0.000008)
   8. '

' (0.000003)
   9. ' capital' (0.000002)
  10. ' London' (0.000001)
  11. ' Berlín' (0.000001)
  12. ' I' (0.000001)
  13. '1' (0.000000)
  14. ' Washington' (0.000000)
  15. ' (' (0.000000)
  16. 'We' (0.000000)
  17. '  ' (0.000000)
  18. 'Ber' (0.000000)
  19. 'This' (0.000000)
  20. 'New' (0.000000)

Layer 38 (after transformer block 37) (entropy: 0.009 bits):
   1. ' Berlin' (0.999282)
   2. 'Berlin' (0.000718)
   3. ' The' (0.000000)
   4. ' Berlín' (0.000000)
   5. 'Ber' (0.000000)
   6. ' ' (0.000000)
   7. ' Hauptstadt' (0.000000)
   8. 'The' (0.000000)
   9. ' Germany' (0.000000)
  10. ' capital' (0.000000)
  11. ' Ber' (0.000000)
  12. ' BERLIN' (0.000000)
  13. ' Paris' (0.000000)
  14. ' Washington' (0.000000)
  15. ' London' (0.000000)
  16. 'berlin' (0.000000)
  17. ' the' (0.000000)
  18. 'Capital' (0.000000)
  19. ' Capital' (0.000000)
  20. '

' (0.000000)

Layer 39 (after transformer block 38) (entropy: 0.008 bits):
   1. ' Berlin' (0.999319)
   2. 'Berlin' (0.000680)
   3. ' The' (0.000001)
   4. ' Berlín' (0.000000)
   5. 'Ber' (0.000000)
   6. ' Ber' (0.000000)
   7. 'The' (0.000000)
   8. ' ' (0.000000)
   9. ' It' (0.000000)
  10. ' Hauptstadt' (0.000000)
  11. ' This' (0.000000)
  12. ' I' (0.000000)
  13. '

' (0.000000)
  14. ' the' (0.000000)
  15. ' BERLIN' (0.000000)
  16. ' We' (0.000000)
  17. ' capital' (0.000000)
  18. ' There' (0.000000)
  19. 'This' (0.000000)
  20. 'BER' (0.000000)

Layer 40 (after transformer block 39) (entropy: 0.004 bits):
   1. ' Berlin' (0.999738)
   2. ' The' (0.000176)
   3. 'Berlin' (0.000043)
   4. ' ' (0.000026)
   5. '

' (0.000011)
   6. ' It' (0.000003)
   7. ' This' (0.000001)
   8. '  ' (0.000000)
   9. 'The' (0.000000)
  10. ' Ber' (0.000000)
  11. ' “' (0.000000)
  12. ' I' (0.000000)
  13. 'Ber' (0.000000)
  14. ' A' (0.000000)
  15. ' There' (0.000000)
  16. ' What' (0.000000)
  17. ' (' (0.000000)
  18. ' "' (0.000000)
  19. '
' (0.000000)
  20. ' We' (0.000000)

Layer 41 (after transformer block 40) (entropy: 0.000 bits):
   1. ' Berlin' (0.999995)
   2. 'Berlin' (0.000004)
   3. ' The' (0.000001)
   4. ' Germany' (0.000000)
   5. ' It' (0.000000)
   6. ' BERLIN' (0.000000)
   7. ' ' (0.000000)
   8. ' I' (0.000000)
   9. '

' (0.000000)
  10. ' Bonn' (0.000000)
  11. ' This' (0.000000)
  12. ' What' (0.000000)
  13. ' German' (0.000000)
  14. ' We' (0.000000)
  15. '  ' (0.000000)
  16. ' A' (0.000000)
  17. ' “' (0.000000)
  18. ' You' (0.000000)
  19. ' There' (0.000000)
  20. ' In' (0.000000)

Layer 42 (after transformer block 41) (entropy: 0.000 bits):
   1. ' Berlin' (0.999999)
   2. ' The' (0.000000)
   3. ' It' (0.000000)
   4. ' Bonn' (0.000000)
   5. ' Germany' (0.000000)
   6. ' BERLIN' (0.000000)
   7. '

' (0.000000)
   8. ' I' (0.000000)
   9. ' ' (0.000000)
  10. 'Berlin' (0.000000)
  11. ' What' (0.000000)
  12. ' This' (0.000000)
  13. ' “' (0.000000)
  14. ' Hamburg' (0.000000)
  15. ' A' (0.000000)
  16. '  ' (0.000000)
  17. ' Munich' (0.000000)
  18. ' "' (0.000000)
  19. ' There' (0.000000)
  20. ' You' (0.000000)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 2.003 bits):
   1. ' Berlin' (0.790751)
   2. ' The' (0.020495)
   3. ' It' (0.016068)
   4. ' Bonn' (0.013445)
   5. ' Germany' (0.011033)
   6. ' BERLIN' (0.009927)
   7. '

' (0.009079)
   8. ' I' (0.007989)
   9. ' ' (0.007667)
  10. 'Berlin' (0.006253)
  11. ' What' (0.006138)
  12. ' This' (0.004951)
  13. ' “' (0.004634)
  14. ' Hamburg' (0.003492)
  15. ' A' (0.003308)
  16. '  ' (0.003273)
  17. ' Munich' (0.003112)
  18. ' "' (0.002852)
  19. ' There' (0.002710)
  20. ' You' (0.002629)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 5.326 bits)
   1. ' a' (0.212154)
   2. ' one' (0.088894)
   3. ' the' (0.075590)
   4. ' home' (0.058343)
   5. ' known' (0.044897)
   6. ' not' (0.040992)
   7. ' an' (0.031461)
   8. ' full' (0.024788)
   9. ' famous' (0.022445)
  10. ' set' (0.018748)

Prompt: 'Berlin is the capital of'
  (entropy: 0.962 bits)
   1. ' Germany' (0.876591)
   2. ' the' (0.069935)
   3. ' modern' (0.007702)
   4. ' a' (0.005295)
   5. ' ' (0.003393)
   6. ' Europe' (0.003080)
   7. ' German' (0.002839)
   8. ' both' (0.002428)
   9. ' Berlin' (0.001870)
  10. ' germany' (0.001524)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 5.318 bits)
   1. '

' (0.158684)
   2. ' If' (0.111439)
   3. ' Berlin' (0.104639)
   4. ' Of' (0.045569)
   5. ' The' (0.045383)
   6. ' You' (0.033567)
   7. '
' (0.030118)
   8. ' That' (0.026873)
   9. ' Yes' (0.022590)
  10. ' And' (0.020589)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000 bits)
   1. ' Berlin' (1.000000)
   2. ' The' (0.000000)
   3. ' It' (0.000000)
   4. ' Bonn' (0.000000)
   5. ' Germany' (0.000000)
   6. ' BERLIN' (0.000000)
   7. '

' (0.000000)
   8. ' I' (0.000000)
   9. ' ' (0.000000)
  10. 'Berlin' (0.000000)
  11. ' What' (0.000000)
  12. ' This' (0.000000)
  13. ' “' (0.000000)
  14. ' Hamburg' (0.000000)
  15. ' A' (0.000000)

Temperature 2.0:
  (entropy: 11.011 bits)
   1. ' Berlin' (0.085554)
   2. ' The' (0.013773)
   3. ' It' (0.012195)
   4. ' Bonn' (0.011156)
   5. ' Germany' (0.010106)
   6. ' BERLIN' (0.009586)
   7. '

' (0.009167)
   8. ' I' (0.008599)
   9. ' ' (0.008425)
  10. 'Berlin' (0.007608)
  11. ' What' (0.007537)
  12. ' This' (0.006770)
  13. ' “' (0.006549)
  14. ' Hamburg' (0.005685)
  15. ' A' (0.005533)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 42
Model dimension: 3584
Number of heads: 16
Vocab size: 256000
Context length: 8192
=== END OF MODEL STATS ========

