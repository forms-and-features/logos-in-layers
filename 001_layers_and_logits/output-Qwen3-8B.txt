
============================================================
EVALUATING MODEL: Qwen/Qwen3-8B
============================================================
Loading model: Qwen/Qwen3-8B...
Loaded pretrained model Qwen/Qwen3-8B into HookedTransformer

=== NORMALIZATION ANALYSIS ========
Block normalization type: RMSNormPre
⚠️  RMSNorm detected but no weight/scale parameter - norm-lens will be skipped
Final normalization type: RMSNormPre
=== END NORMALIZATION ANALYSIS ====


=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['Question', ':', ' What', ' is', ' the', ' capital', ' of', ' Germany', '?', ' Answer', ':']
Computing layer-wise predictions (memory-efficient targeted caching)...

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using RAW residual stream (unsupported normalization, skipping to avoid distortion)
Note: Shown probabilities are from full softmax (calibrated and comparable)
------------------------------------------------------------
Layer  0 (embeddings):
  (entropy: 17.207 bits):
   1. 'いらっ' (0.000010)
   2. ' binaries' (0.000010)
   3. 'おすす' (0.000010)
   4. '家喻户' (0.000010)
   5. 'インターネ' (0.000010)
   6. '(DBG' (0.000010)
   7. ' binary' (0.000010)
   8. 'FindObject' (0.000010)
   9. '烟花爆' (0.000010)
  10. 'お互' (0.000010)
  11. 'アメリ' (0.000010)
  12. '特色社会' (0.000010)
  13. 'InputBorder' (0.000009)
  14. 'ご�' (0.000009)
  15. '/import' (0.000009)
  16. '続きを読' (0.000009)
  17. '電話及' (0.000009)
  18. '.CG' (0.000009)
  19. ' DIR' (0.000009)
  20. '(CONT' (0.000009)

Layer  1 (after transformer block 0) (entropy: 17.034 bits):
   1. ' ListViewItem' (0.000049)
   2. '临港' (0.000048)
   3. '三大职业' (0.000048)
   4. ' Lauderdale' (0.000045)
   5. ' Buccane' (0.000045)
   6. '负面影响' (0.000044)
   7. '生产和' (0.000040)
   8. ' Phonetic' (0.000040)
   9. ' Seeder' (0.000040)
  10. '总队' (0.000040)
  11. ' phé' (0.000039)
  12. 'ichtet' (0.000039)
  13. '单职业' (0.000038)
  14. '前述' (0.000038)
  15. 'uppy' (0.000038)
  16. '/gpl' (0.000038)
  17. '密切相关' (0.000037)
  18. 'endencies' (0.000037)
  19. '中关村' (0.000037)
  20. '嘉年华' (0.000036)

Layer  2 (after transformer block 1) (entropy: 16.868 bits):
   1. ' Buccane' (0.000122)
   2. ' Phonetic' (0.000095)
   3. 'öffentlich' (0.000095)
   4. ' coquine' (0.000093)
   5. ' Seeder' (0.000087)
   6. '凝聚力' (0.000081)
   7. ' Cougar' (0.000080)
   8. '三大职业' (0.000080)
   9. ' datings' (0.000080)
  10. '下半场' (0.000077)
  11. '批复' (0.000077)
  12. '直接影响' (0.000075)
  13. ' catcher' (0.000075)
  14. ' Sesso' (0.000073)
  15. 'rijk' (0.000073)
  16. ' ListViewItem' (0.000073)
  17. '嘉年华' (0.000071)
  18. '负面影响' (0.000070)
  19. ' squirt' (0.000069)
  20. '太快' (0.000068)

Layer  3 (after transformer block 2) (entropy: 16.743 bits):
   1. ' Lauderdale' (0.000182)
   2. ' Buccane' (0.000160)
   3. ' Phonetic' (0.000141)
   4. '批复' (0.000136)
   5. ' ValidationResult' (0.000133)
   6. 'nehmer' (0.000120)
   7. 'mittel' (0.000117)
   8. 'arrison' (0.000114)
   9. 'öffentlich' (0.000112)
  10. '三大职业' (0.000112)
  11. 'başı' (0.000109)
  12. 'ismet' (0.000108)
  13. '并不意味' (0.000107)
  14. '直接影响' (0.000107)
  15. '第二批' (0.000103)
  16. ' coquine' (0.000102)
  17. '延误' (0.000101)
  18. 'anzeigen' (0.000100)
  19. ' Seeder' (0.000099)
  20. 'ledon' (0.000098)

Layer  4 (after transformer block 3) (entropy: 16.615 bits):
   1. ' Buccane' (0.000185)
   2. 'caf' (0.000178)
   3. ' Lauderdale' (0.000166)
   4. 'arrison' (0.000163)
   5. '直接影响' (0.000161)
   6. 'führ' (0.000160)
   7. '批复' (0.000154)
   8. '凝聚力' (0.000149)
   9. '第二批' (0.000147)
  10. 'öffentlich' (0.000141)
  11. '昴' (0.000135)
  12. 'nehmer' (0.000130)
  13. '付け' (0.000129)
  14. 'aggregate' (0.000127)
  15. '延误' (0.000126)
  16. 'vlc' (0.000126)
  17. 'mittel' (0.000124)
  18. ' ListViewItem' (0.000123)
  19. 'licken' (0.000122)
  20. 'portion' (0.000122)

Layer  5 (after transformer block 4) (entropy: 16.530 bits):
   1. '直接影响' (0.000367)
   2. 'portion' (0.000269)
   3. 'arrison' (0.000262)
   4. 'razier' (0.000227)
   5. 'führ' (0.000222)
   6. 'ход' (0.000220)
   7. '单职业' (0.000202)
   8. 'caf' (0.000200)
   9. '(typeof' (0.000198)
  10. '在此之前' (0.000198)
  11. '我省' (0.000184)
  12. 'nehmer' (0.000184)
  13. ' тоже' (0.000183)
  14. '并不代表' (0.000179)
  15. 'directories' (0.000173)
  16. '付け' (0.000172)
  17. 'ым' (0.000170)
  18. '.SpringApplication' (0.000169)
  19. 'affe' (0.000164)
  20. ' ebenfalls' (0.000154)

Layer  6 (after transformer block 5) (entropy: 16.289 bits):
   1. '我省' (0.000672)
   2. 'abella' (0.000534)
   3. '(typeof' (0.000480)
   4. '不利于' (0.000469)
   5. 'portion' (0.000415)
   6. ' lâu' (0.000384)
   7. '�인' (0.000329)
   8. 'alter' (0.000319)
   9. 'ständ' (0.000319)
  10. 'führ' (0.000316)
  11. '直接影响' (0.000303)
  12. 'ingle' (0.000294)
  13. 'ivers' (0.000293)
  14. '略' (0.000280)
  15. 'steller' (0.000273)
  16. 'ив' (0.000265)
  17. ' 부분' (0.000261)
  18. ' информации' (0.000256)
  19. 'oster' (0.000254)
  20. 'bbie' (0.000244)

Layer  7 (after transformer block 6) (entropy: 15.728 bits):
   1. 'portion' (0.001427)
   2. 'steller' (0.001166)
   3. '<Entry' (0.000967)
   4. 'führ' (0.000945)
   5. ' Located' (0.000941)
   6. ' lẽ' (0.000798)
   7. '�인' (0.000770)
   8. '样的' (0.000699)
   9. 'alter' (0.000625)
  10. ' Seconds' (0.000614)
  11. 'abella' (0.000612)
  12. 'ив' (0.000578)
  13. ' ArrayAdapter' (0.000565)
  14. ' Rap' (0.000565)
  15. 'ivers' (0.000556)
  16. 'asio' (0.000544)
  17. 'ать' (0.000531)
  18. 'owe' (0.000528)
  19. ' Scientists' (0.000520)
  20. 'Locator' (0.000499)

Layer  8 (after transformer block 7) (entropy: 15.034 bits):
   1. 'steller' (0.005625)
   2. ' عنه' (0.003174)
   3. '付け' (0.003005)
   4. ' accumulator' (0.001866)
   5. '.answer' (0.001529)
   6. ' Wel' (0.001507)
   7. '答' (0.001473)
   8. 'mittel' (0.001330)
   9. ' Ergebn' (0.001194)
  10. 'portion' (0.001051)
  11. 'не' (0.001049)
  12. 'нес' (0.001023)
  13. '在床上' (0.001007)
  14. ' Instead' (0.000929)
  15. 'owe' (0.000926)
  16. ' Locations' (0.000918)
  17. ' San' (0.000866)
  18. '꧁' (0.000851)
  19. 'ivers' (0.000847)
  20. 'abella' (0.000824)

Layer  9 (after transformer block 8) (entropy: 13.759 bits):
   1. ' Mus' (0.014542)
   2. 'ihar' (0.004974)
   3. ' Enumerator' (0.004694)
   4. '答' (0.004395)
   5. ' Eleven' (0.003911)
   6. 'ывать' (0.003327)
   7. ' Iter' (0.003300)
   8. ' Answer' (0.003144)
   9. 'immer' (0.003105)
  10. ' unpopular' (0.002757)
  11. ' Parliamentary' (0.002700)
  12. 'ما' (0.002684)
  13. '付け' (0.002516)
  14. ' nd' (0.002346)
  15. ' Visit' (0.002248)
  16. ' Element' (0.002196)
  17. 'ndern' (0.002053)
  18. ' Sessions' (0.002012)
  19. ' Moral' (0.001934)
  20. ' Tiger' (0.001918)

Layer 10 (after transformer block 9) (entropy: 13.436 bits):
   1. '在游戏中' (0.016583)
   2. 'owe' (0.010738)
   3. '�' (0.009976)
   4. ' Mus' (0.003774)
   5. '的职业' (0.003704)
   6. 'steller' (0.003504)
   7. ' unpopular' (0.003309)
   8. 'abella' (0.003250)
   9. ' Parliamentary' (0.003177)
  10. 'beit' (0.002819)
  11. 'hei' (0.002805)
  12. ' Binary' (0.002463)
  13. '样的' (0.002341)
  14. '瑟' (0.002317)
  15. ' Illegal' (0.002062)
  16. ' *__' (0.002055)
  17. '_PT' (0.001977)
  18. ' Southern' (0.001935)
  19. ' unre' (0.001888)
  20. ' Frames' (0.001851)

Layer 11 (after transformer block 10) (entropy: 13.661 bits):
   1. '在游戏中' (0.012998)
   2. ' Gem' (0.012127)
   3. ' Incorrect' (0.009470)
   4. ' Illegal' (0.007345)
   5. 'áp' (0.005521)
   6. ' Entre' (0.004709)
   7. 'abella' (0.004510)
   8. ' vậy' (0.003733)
   9. ' Boy' (0.003699)
  10. ' Adult' (0.002735)
  11. ' Desk' (0.002678)
  12. ' Participation' (0.002668)
  13. 'owe' (0.002667)
  14. 'gebn' (0.002605)
  15. ' Binary' (0.002483)
  16. ' Mus' (0.002383)
  17. 'anker' (0.002378)
  18. '史上最' (0.002235)
  19. ' Parliamentary' (0.002222)
  20. ' Occupation' (0.002167)

Layer 12 (after transformer block 11) (entropy: 12.825 bits):
   1. ' Answer' (0.022223)
   2. ' Incorrect' (0.020698)
   3. ' Electoral' (0.012626)
   4. ' Desk' (0.009153)
   5. ' Illegal' (0.008195)
   6. ' Boy' (0.007641)
   7. ' Develop' (0.007226)
   8. 'áp' (0.006962)
   9. '不服' (0.006876)
  10. ' Entities' (0.005520)
  11. ' Gem' (0.004499)
  12. ' Cities' (0.004446)
  13. '答え' (0.004306)
  14. ' Prompt' (0.003642)
  15. 'あれ' (0.003369)
  16. ' VE' (0.003166)
  17. '在游戏中' (0.003055)
  18. ' Parliamentary' (0.003052)
  19. ' Begin' (0.003037)
  20. 'abella' (0.002880)

Layer 13 (after transformer block 12) (entropy: 12.194 bits):
   1. ' Answer' (0.054416)
   2. 'abella' (0.019162)
   3. ' Incorrect' (0.014760)
   4. '在游戏中' (0.009441)
   5. '在全球' (0.008553)
   6. ' Binary' (0.007002)
   7. ' VE' (0.006863)
   8. '答え' (0.006186)
   9. ' Frag' (0.005429)
  10. ' thirteen' (0.005267)
  11. ' Prompt' (0.004958)
  12. ' Knowledge' (0.004942)
  13. '_FM' (0.004738)
  14. 'oday' (0.004436)
  15. ' tsl' (0.004011)
  16. ' Intelligent' (0.003635)
  17. ' Boy' (0.003591)
  18. ' Entities' (0.003339)
  19. ' Minecraft' (0.003068)
  20. '_primitive' (0.003068)

Layer 14 (after transformer block 13) (entropy: 11.894 bits):
   1. ' Binary' (0.078967)
   2. ' Answer' (0.029712)
   3. ' Incorrect' (0.009559)
   4. '在游戏中' (0.008670)
   5. ' Prompt' (0.006925)
   6. ' The' (0.006541)
   7. ' Intelligent' (0.006408)
   8. '在全球' (0.005801)
   9. ' Illegal' (0.005586)
  10. ' Boy' (0.005411)
  11. '_FM' (0.005283)
  12. ' Organ' (0.005226)
  13. ' none' (0.005170)
  14. ' Minecraft' (0.005051)
  15. ' Donald' (0.004581)
  16. 'abella' (0.004028)
  17. ' binary' (0.003795)
  18. ' English' (0.003238)
  19. ' Digital' (0.003086)
  20. ' thirteen' (0.002946)

Layer 15 (after transformer block 14) (entropy: 11.325 bits):
   1. ' Answer' (0.039197)
   2. ' Incorrect' (0.026246)
   3. ' Binary' (0.017202)
   4. ' Knowledge' (0.017027)
   5. '在全球' (0.014440)
   6. ' Correct' (0.013483)
   7. '正确的' (0.013379)
   8. '_FM' (0.010610)
   9. ' FM' (0.007812)
  10. ' Statements' (0.007700)
  11. ' Prompt' (0.005904)
  12. ' prefixed' (0.005897)
  13. ' hierarchical' (0.005849)
  14. ' $__' (0.005806)
  15. 'abella' (0.005532)
  16. '不服' (0.005336)
  17. ' The' (0.005189)
  18. ' First' (0.004969)
  19. ' Minecraft' (0.004479)
  20. '太阳城' (0.004423)

Layer 16 (after transformer block 15) (entropy: 11.092 bits):
   1. ' Answer' (0.061884)
   2. ' Classes' (0.041033)
   3. '正确的' (0.023263)
   4. ' Incorrect' (0.014795)
   5. ' Knowledge' (0.012938)
   6. ' Context' (0.012000)
   7. '在全球' (0.008448)
   8. ' política' (0.007412)
   9. ' Earth' (0.006777)
  10. ' The' (0.006523)
  11. ' First' (0.006171)
  12. '在游戏中' (0.006145)
  13. ' ____' (0.005822)
  14. ' Define' (0.005411)
  15. 'ilik' (0.005095)
  16. ' Minecraft' (0.005086)
  17. 'GraphNode' (0.004274)
  18. ' Statements' (0.004265)
  19. ' answer' (0.004242)
  20. ' __' (0.003931)

Layer 17 (after transformer block 16) (entropy: 10.462 bits):
   1. ' Answer' (0.122253)
   2. ' Incorrect' (0.027122)
   3. '编码' (0.016927)
   4. ' __' (0.011549)
   5. ' Classes' (0.010748)
   6. '_encoding' (0.009856)
   7. ' Architect' (0.009627)
   8. '_chunks' (0.007997)
   9. '在全球' (0.006361)
  10. 'ilik' (0.005936)
  11. '_topology' (0.005828)
  12. '_assert' (0.005775)
  13. ' política' (0.005696)
  14. ' answer' (0.005397)
  15. '第一条' (0.005230)
  16. ' First' (0.005165)
  17. 'ichel' (0.005081)
  18. ' trivia' (0.005059)
  19. 'aal' (0.004544)
  20. ' Define' (0.004465)

Layer 18 (after transformer block 17) (entropy: 5.097 bits):
   1. ' Answer' (0.545126)
   2. '回答' (0.047931)
   3. ' None' (0.028727)
   4. '_ANS' (0.022571)
   5. ' Classes' (0.016544)
   6. '在全球' (0.015602)
   7. '.answer' (0.011577)
   8. '编码' (0.008576)
   9. ' answer' (0.004738)
  10. ' 数' (0.004478)
  11. 'GraphNode' (0.003985)
  12. ' Incorrect' (0.003211)
  13. ' none' (0.003109)
  14. '第一条' (0.002899)
  15. ' AssertionError' (0.002670)
  16. ' answered' (0.002538)
  17. '在游戏中' (0.002530)
  18. '_answer' (0.002353)
  19. ' The' (0.002325)
  20. '我知道' (0.002288)

Layer 19 (after transformer block 18) (entropy: 1.775 bits):
   1. ' Answer' (0.824900)
   2. '回答' (0.057038)
   3. '_ANS' (0.010487)
   4. '.answer' (0.007837)
   5. '的回答' (0.005346)
   6. 'คำตอบ' (0.003836)
   7. ' ______' (0.003520)
   8. '我知道' (0.002957)
   9. '_answer' (0.002852)
  10. ' _____' (0.002529)
  11. ' ___' (0.002442)
  12. '嘘' (0.002064)
  13. ' None' (0.002033)
  14. '世界第一' (0.001431)
  15. ' Prompt' (0.001382)
  16. ' Located' (0.001283)
  17. ' ____' (0.001248)
  18. 'ichel' (0.001208)
  19. ' answered' (0.001186)
  20. '的答案' (0.001144)

Layer 20 (after transformer block 19) (entropy: 0.207 bits):
   1. ' Answer' (0.981501)
   2. '_ANS' (0.006643)
   3. '的回答' (0.002637)
   4. '的答案' (0.001960)
   5. ' None' (0.001097)
   6. ' none' (0.001054)
   7. '_answer' (0.000543)
   8. '回答' (0.000459)
   9. ' ______' (0.000427)
  10. ' Yes' (0.000351)
  11. ' ____' (0.000337)
  12. ' answer' (0.000302)
  13. '世界第一' (0.000153)
  14. ' ___' (0.000124)
  15. '.answer' (0.000122)
  16. ' yes' (0.000105)
  17. ' Ans' (0.000097)
  18. 'คำตอบ' (0.000090)
  19. ' unequiv' (0.000088)
  20. ' $__' (0.000072)

Layer 21 (after transformer block 20) (entropy: 0.050 bits):
   1. ' Answer' (0.995931)
   2. '_ANS' (0.002493)
   3. ' ____' (0.000291)
   4. ' none' (0.000181)
   5. '回答' (0.000172)
   6. ' answer' (0.000088)
   7. '我知道' (0.000080)
   8. ' ______' (0.000079)
   9. ' Yes' (0.000076)
  10. ' ___' (0.000066)
  11. '_answer' (0.000059)
  12. ' None' (0.000050)
  13. '的答案' (0.000046)
  14. ' yes' (0.000037)
  15. ' $__' (0.000029)
  16. ' _____' (0.000024)
  17. '这个问题' (0.000020)
  18. 'คำตอบ' (0.000019)
  19. ' __' (0.000018)
  20. '.answer' (0.000017)

Layer 22 (after transformer block 21) (entropy: 0.064 bits):
   1. ' Answer' (0.994987)
   2. ' ____' (0.001374)
   3. '_ANS' (0.001328)
   4. ' ______' (0.000441)
   5. '我知道' (0.000311)
   6. '的答案' (0.000218)
   7. ' Ans' (0.000171)
   8. ' __' (0.000143)
   9. ' ANSW' (0.000120)
  10. '回答' (0.000074)
  11. ' answered' (0.000062)
  12. ' ...

' (0.000061)
  13. ' ___' (0.000053)
  14. '_answer' (0.000043)
  15. ' answer' (0.000042)
  16. '当然是' (0.000042)
  17. 'zell' (0.000040)
  18. '____' (0.000032)
  19. '答' (0.000024)
  20. ' None' (0.000023)

Layer 23 (after transformer block 22) (entropy: 0.319 bits):
   1. ' Answer' (0.956760)
   2. ' ____' (0.031920)
   3. ' ______' (0.005995)
   4. ' The' (0.001877)
   5. ' Ans' (0.001297)
   6. ' __' (0.000743)
   7. '的答案' (0.000395)
   8. ' __________________' (0.000246)
   9. '____' (0.000200)
  10. ' _____' (0.000170)
  11. ' ...
' (0.000168)
  12. ' ...

' (0.000038)
  13. ' answer' (0.000025)
  14. ' answered' (0.000025)
  15. ' ___' (0.000017)
  16. ' ANSW' (0.000017)
  17. ' Please' (0.000016)
  18. '答え' (0.000013)
  19. ' There' (0.000010)
  20. ' Answers' (0.000010)

Layer 24 (after transformer block 23) (entropy: 0.701 bits):
   1. ' ______' (0.880689)
   2. ' ____' (0.086026)
   3. ' The' (0.012818)
   4. ' __' (0.008618)
   5. ' __________________' (0.007202)
   6. ' _____' (0.002103)
   7. '____' (0.001968)
   8. ' ___' (0.000216)
   9. ' Answer' (0.000175)
  10. ' Located' (0.000079)
  11. ' There' (0.000049)
  12. ' ...
' (0.000022)
  13. ' Yes' (0.000015)
  14. ' ...

' (0.000007)
  15. ' None' (0.000005)
  16. '__
' (0.000001)
  17. ' A' (0.000001)
  18. ' Ans' (0.000001)
  19. ' This' (0.000001)
  20. '---------
' (0.000000)

Layer 25 (after transformer block 24) (entropy: 0.001 bits):
   1. ' Germany' (0.999925)
   2. ' ____' (0.000056)
   3. ' ______' (0.000010)
   4. ' __________________' (0.000004)
   5. ' _____' (0.000002)
   6. ' __' (0.000002)
   7. ' ___' (0.000000)
   8. '____' (0.000000)
   9. ' Berlin' (0.000000)
  10. ' German' (0.000000)
  11. ' Deutschland' (0.000000)
  12. ' Europe' (0.000000)
  13. ' The' (0.000000)
  14. ' Munich' (0.000000)
  15. ' Germans' (0.000000)
  16. '__
' (0.000000)
  17. 'Germany' (0.000000)
  18. '____________' (0.000000)
  19. ' _______,' (0.000000)
  20. '_____' (0.000000)

Layer 26 (after transformer block 25) (entropy: 0.947 bits):
   1. ' ____' (0.815687)
   2. ' ______' (0.120622)
   3. '____' (0.038330)
   4. ' __' (0.019201)
   5. ' __________________' (0.005488)
   6. ' _____' (0.000373)
   7. ' Germany' (0.000248)
   8. ' ___' (0.000052)
   9. '________' (0.000000)
  10. '____________' (0.000000)
  11. '__
' (0.000000)
  12. '_____' (0.000000)
  13. '________________' (0.000000)
  14. ' The' (0.000000)
  15. '__[' (0.000000)
  16. ' Berlin' (0.000000)
  17. ' _______,' (0.000000)
  18. '___' (0.000000)
  19. '[__' (0.000000)
  20. '__

' (0.000000)

Layer 27 (after transformer block 26) (entropy: 0.077 bits):
   1. ' Germany' (0.992544)
   2. ' ____' (0.002756)
   3. ' ______' (0.002511)
   4. ' Berlin' (0.001675)
   5. ' __________________' (0.000309)
   6. ' __' (0.000195)
   7. ' ___' (0.000006)
   8. ' _____' (0.000004)
   9. '____' (0.000001)
  10. ' The' (0.000000)
  11. '__
' (0.000000)
  12. ' _______,' (0.000000)
  13. '_____' (0.000000)
  14. ' German' (0.000000)
  15. '__

' (0.000000)
  16. '________' (0.000000)
  17. '____________' (0.000000)
  18. '__[' (0.000000)
  19. '________________' (0.000000)
  20. ' __________________________________' (0.000000)

Layer 28 (after transformer block 27) (entropy: 0.000 bits):
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. ' Paris' (0.000000)
   4. ' Munich' (0.000000)
   5. ' Frankfurt' (0.000000)
   6. ' Rome' (0.000000)
   7. ' The' (0.000000)
   8. ' __________________' (0.000000)
   9. ' London' (0.000000)
  10. ' Madrid' (0.000000)
  11. ' ______' (0.000000)
  12. 'Berlin' (0.000000)
  13. ' __' (0.000000)
  14. ' cities' (0.000000)
  15. ' ____' (0.000000)
  16. ' _____' (0.000000)
  17. ' Vienna' (0.000000)
  18. ' Москва' (0.000000)
  19. ' Beijing' (0.000000)
  20. ' ___' (0.000000)

Layer 29 (after transformer block 28) (entropy: 0.000 bits):
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. ' Munich' (0.000000)
   4. ' Frankfurt' (0.000000)
   5. 'Berlin' (0.000000)
   6. 'Germany' (0.000000)
   7. ' Stuttgart' (0.000000)
   8. ' Hamburg' (0.000000)
   9. ' German' (0.000000)
  10. ' Vienna' (0.000000)
  11. ' München' (0.000000)
  12. ' Paris' (0.000000)
  13. '柏林' (0.000000)
  14. ' __________________' (0.000000)
  15. '德国' (0.000000)
  16. ' berlin' (0.000000)
  17. '武汉' (0.000000)
  18. ' Prague' (0.000000)
  19. ' Rome' (0.000000)
  20. ' Germans' (0.000000)

Layer 30 (after transformer block 29) (entropy: 0.009 bits):
   1. ' Berlin' (0.999210)
   2. ' Germany' (0.000790)
   3. ' Frankfurt' (0.000000)
   4. 'Berlin' (0.000000)
   5. 'Germany' (0.000000)
   6. ' Munich' (0.000000)
   7. ' German' (0.000000)
   8. '德国' (0.000000)
   9. ' germany' (0.000000)
  10. '柏林' (0.000000)
  11. ' Stuttgart' (0.000000)
  12. ' Hamburg' (0.000000)
  13. ' Paris' (0.000000)
  14. ' Germans' (0.000000)
  15. 'ドイツ' (0.000000)
  16. ' Deutschland' (0.000000)
  17. ' frankfurt' (0.000000)
  18. ' München' (0.000000)
  19. ' __________________' (0.000000)
  20. ' berlin' (0.000000)

Layer 31 (after transformer block 30) (entropy: 0.000 bits):
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. 'Berlin' (0.000000)
   4. ' Frankfurt' (0.000000)
   5. ' Munich' (0.000000)
   6. '柏林' (0.000000)
   7. ' Hamburg' (0.000000)
   8. ' Paris' (0.000000)
   9. ' Stuttgart' (0.000000)
  10. 'Germany' (0.000000)
  11. ' berlin' (0.000000)
  12. ' Vienna' (0.000000)
  13. ' München' (0.000000)
  14. ' frankfurt' (0.000000)
  15. ' London' (0.000000)
  16. ' Dresden' (0.000000)
  17. ' Brussels' (0.000000)
  18. '武汉' (0.000000)
  19. ' Cologne' (0.000000)
  20. ' Amsterdam' (0.000000)

Layer 32 (after transformer block 31) (entropy: 0.000 bits):
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. ' German' (0.000000)
   4. 'Berlin' (0.000000)
   5. '柏林' (0.000000)
   6. '德国' (0.000000)
   7. 'Germany' (0.000000)
   8. ' Germans' (0.000000)
   9. ' Frankfurt' (0.000000)
  10. ' Hamburg' (0.000000)
  11. ' Munich' (0.000000)
  12. ' germany' (0.000000)
  13. 'ドイツ' (0.000000)
  14. 'German' (0.000000)
  15. ' berlin' (0.000000)
  16. ' Stuttgart' (0.000000)
  17. ' Deutschland' (0.000000)
  18. '武汉' (0.000000)
  19. ' Dresden' (0.000000)
  20. ' german' (0.000000)

Layer 33 (after transformer block 32) (entropy: 0.000 bits):
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. '柏林' (0.000000)
   4. 'Berlin' (0.000000)
   5. ' German' (0.000000)
   6. ' Frankfurt' (0.000000)
   7. ' Munich' (0.000000)
   8. '德国' (0.000000)
   9. 'Germany' (0.000000)
  10. ' berlin' (0.000000)
  11. ' Hamburg' (0.000000)
  12. ' Germans' (0.000000)
  13. ' Stuttgart' (0.000000)
  14. ' germany' (0.000000)
  15. 'ドイツ' (0.000000)
  16. 'German' (0.000000)
  17. ' Dresden' (0.000000)
  18. ' Deutschland' (0.000000)
  19. ' München' (0.000000)
  20. '法兰' (0.000000)

Layer 34 (after transformer block 33) (entropy: -0.000 bits):
   1. ' Berlin' (1.000000)
   2. '柏林' (0.000000)
   3. 'Berlin' (0.000000)
   4. ' Frankfurt' (0.000000)
   5. ' Germany' (0.000000)
   6. ' berlin' (0.000000)
   7. ' Hamburg' (0.000000)
   8. ' Munich' (0.000000)
   9. ' BER' (0.000000)
  10. ' Ber' (0.000000)
  11. ' Bon' (0.000000)
  12. '_ber' (0.000000)
  13. ' Stuttgart' (0.000000)
  14. '法兰' (0.000000)
  15. ' Paris' (0.000000)
  16. ' The' (0.000000)
  17. ' German' (0.000000)
  18. ' Bern' (0.000000)
  19. ' London' (0.000000)
  20. ' Brussels' (0.000000)

Layer 35 (after transformer block 34) (entropy: -0.000 bits):
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. 'Berlin' (0.000000)
   4. ' The' (0.000000)
   5. ' __' (0.000000)
   6. '柏林' (0.000000)
   7. ' Frankfurt' (0.000000)
   8. ' ______' (0.000000)
   9. ' Hamburg' (0.000000)
  10. ' Bon' (0.000000)
  11. ' Munich' (0.000000)
  12. ' German' (0.000000)
  13. ' __________________' (0.000000)
  14. ' A' (0.000000)
  15. ' Ber' (0.000000)
  16. ' ____' (0.000000)
  17. ' ___' (0.000000)
  18. ' berlin' (0.000000)
  19. ' _____' (0.000000)
  20. ' London' (0.000000)

Layer 36 (after transformer block 35) (entropy: 0.000 bits):
   1. ' Berlin' (1.000000)
   2. ' The' (0.000000)
   3. ' Germany' (0.000000)
   4. ' ' (0.000000)
   5. ' __' (0.000000)
   6. ' 

' (0.000000)
   7. ' ______' (0.000000)
   8. ' A' (0.000000)
   9. ' (' (0.000000)
  10. ' What' (0.000000)
  11. ' Ber' (0.000000)
  12. ' This' (0.000000)
  13. ' Bon' (0.000000)
  14. ' ?

' (0.000000)
  15. ' __________________' (0.000000)
  16. ' Frankfurt' (0.000000)
  17. ' [' (0.000000)
  18. ' ?' (0.000000)
  19. ' I' (0.000000)
  20. ' ___' (0.000000)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 2.020 bits):
   1. ' Berlin' (0.719303)
   2. ' The' (0.086098)
   3. ' Germany' (0.034850)
   4. ' ' (0.034655)
   5. ' __' (0.034201)
   6. ' 

' (0.010644)
   7. ' ______' (0.009246)
   8. ' A' (0.007960)
   9. ' (' (0.003471)
  10. ' What' (0.002803)
  11. ' Ber' (0.002647)
  12. ' This' (0.002610)
  13. ' Bon' (0.002474)
  14. ' ?

' (0.002392)
  15. ' __________________' (0.002202)
  16. ' Frankfurt' (0.002138)
  17. ' [' (0.002138)
  18. ' ?' (0.001835)
  19. ' I' (0.001672)
  20. ' ___' (0.001398)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 2.989 bits)
   1. ' Berlin' (0.617024)
   2. ' not' (0.070311)
   3. ' a' (0.067692)
   4. ' in' (0.041279)
   5. ' located' (0.021944)
   6. ' the' (0.016110)
   7. ' __' (0.015827)
   8. ' called' (0.011862)
   9. ' ______' (0.008227)
  10. ' known' (0.007708)

Prompt: 'Berlin is the capital of'
  (entropy: 1.205 bits)
   1. ' Germany' (0.728606)
   2. ' which' (0.220746)
   3. ' the' (0.023739)
   4. ' what' (0.011356)
   5. ' __' (0.002298)
   6. ' ______' (0.001446)
   7. ' a' (0.001124)
   8. ' German' (0.000802)
   9. ' Berlin' (0.000459)
  10. '
' (0.000439)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 1.633 bits)
   1. ' Berlin' (0.721244)
   2. ' The' (0.174039)
   3. ' Also' (0.015090)
   4. '
' (0.012804)
   5. ' What' (0.009650)
   6. ' 

' (0.008670)
   7. ' ' (0.008418)
   8. ' Ber' (0.007864)
   9. ' Additionally' (0.004229)
  10. ' In' (0.003077)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000 bits)
   1. ' Berlin' (1.000000)
   2. ' The' (0.000000)
   3. ' Germany' (0.000000)
   4. ' ' (0.000000)
   5. ' __' (0.000000)
   6. ' 

' (0.000000)
   7. ' ______' (0.000000)
   8. ' A' (0.000000)
   9. ' (' (0.000000)
  10. ' What' (0.000000)
  11. ' Ber' (0.000000)
  12. ' This' (0.000000)
  13. ' Bon' (0.000000)
  14. ' ?

' (0.000000)
  15. ' __________________' (0.000000)

Temperature 2.0:
  (entropy: 13.141 bits)
   1. ' Berlin' (0.066316)
   2. ' The' (0.022944)
   3. ' Germany' (0.014597)
   4. ' ' (0.014556)
   5. ' __' (0.014461)
   6. ' 

' (0.008067)
   7. ' ______' (0.007519)
   8. ' A' (0.006976)
   9. ' (' (0.004607)
  10. ' What' (0.004140)
  11. ' Ber' (0.004023)
  12. ' This' (0.003995)
  13. ' Bon' (0.003889)
  14. ' ?

' (0.003824)
  15. ' __________________' (0.003669)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 36
Model dimension: 4096
Number of heads: 32
Vocab size: 151936
Context length: 2048
=== END OF MODEL STATS ========

