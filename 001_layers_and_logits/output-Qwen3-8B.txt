
============================================================
EVALUATING MODEL: Qwen/Qwen3-8B
============================================================
Loading model: Qwen/Qwen3-8B...
Loaded pretrained model Qwen/Qwen3-8B into HookedTransformer

=== NORMALIZATION ANALYSIS ========
Block LayerNorm type: RMSNormPre
⚠️  Non-vanilla norm detected (RMSNormPre) - norm-lens will be skipped to avoid distortion
Final LayerNorm type: RMSNormPre
=== END NORMALIZATION ANALYSIS ====


=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['Question', ':', ' What', ' is', ' the', ' capital', ' of', ' Germany', '?', ' Answer', ':']
Computing layer-wise predictions (memory-efficient targeted caching)...

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using RAW residual stream (non-vanilla norms detected, skipping normalization to avoid distortion)
Note: Shown probabilities are softmax over top-k only (don't sum to 1)
------------------------------------------------------------
Layer  0 (embeddings):
  (entropy: 11.927):
   1. 'いらっ' (0.053121)
   2. ' binaries' (0.052814)
   3. 'おすす' (0.052055)
   4. '家喻户' (0.051544)
   5. 'インターネ' (0.051461)
   6. '(DBG' (0.050464)
   7. ' binary' (0.050393)
   8. 'FindObject' (0.050004)
   9. '烟花爆' (0.049941)
  10. 'お互' (0.049909)
  11. 'アメリ' (0.049669)
  12. '特色社会' (0.049469)
  13. 'InputBorder' (0.049256)
  14. 'ご�' (0.049047)
  15. '/import' (0.048831)
  16. '続きを読' (0.048553)
  17. '電話及' (0.048532)
  18. '.CG' (0.048320)
  19. ' DIR' (0.048317)
  20. '(CONT' (0.048303)

Layer  1 (after block 0) (entropy: 11.807):
   1. ' ListViewItem' (0.060224)
   2. '临港' (0.058607)
   3. '三大职业' (0.058542)
   4. ' Lauderdale' (0.055322)
   5. ' Buccane' (0.055266)
   6. '负面影响' (0.054402)
   7. '生产和' (0.049510)
   8. ' Phonetic' (0.049053)
   9. ' Seeder' (0.048951)
  10. '总队' (0.048677)
  11. ' phé' (0.048352)
  12. 'ichtet' (0.047677)
  13. '单职业' (0.046627)
  14. '前述' (0.046387)
  15. 'uppy' (0.046207)
  16. '/gpl' (0.046162)
  17. '密切相关' (0.045705)
  18. 'endencies' (0.045383)
  19. '中关村' (0.045252)
  20. '嘉年华' (0.043695)

Layer  2 (after block 1) (entropy: 11.692):
   1. ' Buccane' (0.075304)
   2. ' Phonetic' (0.058873)
   3. 'öffentlich' (0.058544)
   4. ' coquine' (0.057769)
   5. ' Seeder' (0.054113)
   6. '凝聚力' (0.049905)
   7. ' Cougar' (0.049687)
   8. '三大职业' (0.049483)
   9. ' datings' (0.049474)
  10. '下半场' (0.047859)
  11. '批复' (0.047421)
  12. '直接影响' (0.046709)
  13. ' catcher' (0.046374)
  14. ' Sesso' (0.045501)
  15. 'rijk' (0.045311)
  16. ' ListViewItem' (0.045180)
  17. '嘉年华' (0.044219)
  18. '负面影响' (0.043386)
  19. ' squirt' (0.042569)
  20. '太快' (0.042317)

Layer  3 (after block 2) (entropy: 11.606):
   1. ' Lauderdale' (0.077074)
   2. ' Buccane' (0.067627)
   3. ' Phonetic' (0.059766)
   4. '批复' (0.057550)
   5. ' ValidationResult' (0.056175)
   6. 'nehmer' (0.050984)
   7. 'mittel' (0.049375)
   8. 'arrison' (0.048408)
   9. 'öffentlich' (0.047374)
  10. '三大职业' (0.047267)
  11. 'başı' (0.046150)
  12. 'ismet' (0.045736)
  13. '并不意味' (0.045381)
  14. '直接影响' (0.045332)
  15. '第二批' (0.043706)
  16. ' coquine' (0.043227)
  17. '延误' (0.042766)
  18. 'anzeigen' (0.042474)
  19. ' Seeder' (0.042066)
  20. 'ledon' (0.041564)

Layer  4 (after block 3) (entropy: 11.517):
   1. ' Buccane' (0.064591)
   2. 'caf' (0.061992)
   3. ' Lauderdale' (0.057779)
   4. 'arrison' (0.056719)
   5. '直接影响' (0.056340)
   6. 'führ' (0.055696)
   7. '批复' (0.053844)
   8. '凝聚力' (0.052111)
   9. '第二批' (0.051123)
  10. 'öffentlich' (0.049173)
  11. '昴' (0.047175)
  12. 'nehmer' (0.045309)
  13. '付け' (0.044940)
  14. 'aggregate' (0.044270)
  15. '延误' (0.043868)
  16. 'vlc' (0.043808)
  17. 'mittel' (0.043407)
  18. ' ListViewItem' (0.042848)
  19. 'licken' (0.042573)
  20. 'portion' (0.042436)

Layer  5 (after block 4) (entropy: 11.458):
   1. '直接影响' (0.089672)
   2. 'portion' (0.065548)
   3. 'arrison' (0.064059)
   4. 'razier' (0.055444)
   5. 'führ' (0.054122)
   6. 'ход' (0.053757)
   7. '单职业' (0.049404)
   8. 'caf' (0.048935)
   9. '(typeof' (0.048418)
  10. '在此之前' (0.048235)
  11. '我省' (0.044948)
  12. 'nehmer' (0.044823)
  13. ' тоже' (0.044558)
  14. '并不代表' (0.043698)
  15. 'directories' (0.042183)
  16. '付け' (0.042027)
  17. 'ым' (0.041466)
  18. '.SpringApplication' (0.041166)
  19. 'affe' (0.040052)
  20. ' ebenfalls' (0.037486)

Layer  6 (after block 5) (entropy: 11.291):
   1. '我省' (0.096529)
   2. 'abella' (0.076739)
   3. '(typeof' (0.068951)
   4. '不利于' (0.067363)
   5. 'portion' (0.059647)
   6. ' lâu' (0.055120)
   7. '�인' (0.047328)
   8. 'alter' (0.045822)
   9. 'ständ' (0.045795)
  10. 'führ' (0.045421)
  11. '直接影响' (0.043579)
  12. 'ingle' (0.042308)
  13. 'ivers' (0.042043)
  14. '略' (0.040187)
  15. 'steller' (0.039247)
  16. 'ив' (0.038081)
  17. ' 부분' (0.037455)
  18. ' информации' (0.036789)
  19. 'oster' (0.036545)
  20. 'bbie' (0.035051)

Layer  7 (after block 6) (entropy: 10.902):
   1. 'portion' (0.098750)
   2. 'steller' (0.080674)
   3. '<Entry' (0.066907)
   4. 'führ' (0.065438)
   5. ' Located' (0.065099)
   6. ' lẽ' (0.055203)
   7. '�인' (0.053277)
   8. '样的' (0.048386)
   9. 'alter' (0.043252)
  10. ' Seconds' (0.042531)
  11. 'abella' (0.042362)
  12. 'ив' (0.040006)
  13. ' ArrayAdapter' (0.039082)
  14. ' Rap' (0.039078)
  15. 'ivers' (0.038475)
  16. 'asio' (0.037663)
  17. 'ать' (0.036765)
  18. 'owe' (0.036511)
  19. ' Scientists' (0.035991)
  20. 'Locator' (0.034549)

Layer  8 (after block 7) (entropy: 10.421):
   1. 'steller' (0.181480)
   2. ' عنه' (0.102404)
   3. '付け' (0.096954)
   4. ' accumulator' (0.060210)
   5. '.answer' (0.049345)
   6. ' Wel' (0.048635)
   7. '答' (0.047513)
   8. 'mittel' (0.042914)
   9. ' Ergebn' (0.038520)
  10. 'portion' (0.033905)
  11. 'не' (0.033831)
  12. 'нес' (0.033006)
  13. '在床上' (0.032473)
  14. ' Instead' (0.029986)
  15. 'owe' (0.029888)
  16. ' Locations' (0.029602)
  17. ' San' (0.027954)
  18. '꧁' (0.027469)
  19. 'ivers' (0.027315)
  20. 'abella' (0.026596)

Layer  9 (after block 8) (entropy: 9.537):
   1. ' Mus' (0.205530)
   2. 'ihar' (0.070296)
   3. ' Enumerator' (0.066351)
   4. '答' (0.062114)
   5. ' Eleven' (0.055273)
   6. 'ывать' (0.047021)
   7. ' Iter' (0.046639)
   8. ' Answer' (0.044430)
   9. 'immer' (0.043889)
  10. ' unpopular' (0.038961)
  11. ' Parliamentary' (0.038163)
  12. 'ما' (0.037934)
  13. '付け' (0.035557)
  14. ' nd' (0.033158)
  15. ' Visit' (0.031767)
  16. ' Element' (0.031034)
  17. 'ndern' (0.029012)
  18. ' Sessions' (0.028431)
  19. ' Moral' (0.027338)
  20. ' Tiger' (0.027103)

Layer 10 (after block 9) (entropy: 9.313):
   1. '在游戏中' (0.200935)
   2. 'owe' (0.130115)
   3. '�' (0.120885)
   4. ' Mus' (0.045734)
   5. '的职业' (0.044876)
   6. 'steller' (0.042452)
   7. ' unpopular' (0.040090)
   8. 'abella' (0.039379)
   9. ' Parliamentary' (0.038500)
  10. 'beit' (0.034157)
  11. 'hei' (0.033990)
  12. ' Binary' (0.029849)
  13. '样的' (0.028364)
  14. '瑟' (0.028080)
  15. ' Illegal' (0.024985)
  16. ' *__' (0.024904)
  17. '_PT' (0.023954)
  18. ' Southern' (0.023444)
  19. ' unre' (0.022873)
  20. ' Frames' (0.022435)

Layer 11 (after block 10) (entropy: 9.469):
   1. '在游戏中' (0.142316)
   2. ' Gem' (0.132779)
   3. ' Incorrect' (0.103684)
   4. ' Illegal' (0.080419)
   5. 'áp' (0.060453)
   6. ' Entre' (0.051560)
   7. 'abella' (0.049379)
   8. ' vậy' (0.040877)
   9. ' Boy' (0.040499)
  10. ' Adult' (0.029947)
  11. ' Desk' (0.029318)
  12. ' Participation' (0.029211)
  13. 'owe' (0.029203)
  14. 'gebn' (0.028523)
  15. ' Binary' (0.027183)
  16. ' Mus' (0.026091)
  17. 'anker' (0.026031)
  18. '史上最' (0.024468)
  19. ' Parliamentary' (0.024332)
  20. ' Occupation' (0.023728)

Layer 12 (after block 11) (entropy: 8.889):
   1. ' Answer' (0.155874)
   2. ' Incorrect' (0.145179)
   3. ' Electoral' (0.088558)
   4. ' Desk' (0.064199)
   5. ' Illegal' (0.057478)
   6. ' Boy' (0.053592)
   7. ' Develop' (0.050682)
   8. 'áp' (0.048829)
   9. '不服' (0.048232)
  10. ' Entities' (0.038716)
  11. ' Gem' (0.031555)
  12. ' Cities' (0.031186)
  13. '答え' (0.030200)
  14. ' Prompt' (0.025543)
  15. 'あれ' (0.023628)
  16. ' VE' (0.022206)
  17. '在游戏中' (0.021430)
  18. ' Parliamentary' (0.021406)
  19. ' Begin' (0.021305)
  20. 'abella' (0.020202)

Layer 13 (after block 12) (entropy: 8.452):
   1. ' Answer' (0.307667)
   2. 'abella' (0.108341)
   3. ' Incorrect' (0.083455)
   4. '在游戏中' (0.053379)
   5. '在全球' (0.048361)
   6. ' Binary' (0.039590)
   7. ' VE' (0.038801)
   8. '答え' (0.034976)
   9. ' Frag' (0.030696)
  10. ' thirteen' (0.029779)
  11. ' Prompt' (0.028035)
  12. ' Knowledge' (0.027943)
  13. '_FM' (0.026789)
  14. 'oday' (0.025079)
  15. ' tsl' (0.022680)
  16. ' Intelligent' (0.020554)
  17. ' Boy' (0.020303)
  18. ' Entities' (0.018876)
  19. ' Minecraft' (0.017349)
  20. '_primitive' (0.017345)

Layer 14 (after block 13) (entropy: 8.245):
   1. ' Binary' (0.383362)
   2. ' Answer' (0.144241)
   3. ' Incorrect' (0.046404)
   4. '在游戏中' (0.042090)
   5. ' Prompt' (0.033621)
   6. ' The' (0.031757)
   7. ' Intelligent' (0.031111)
   8. '在全球' (0.028163)
   9. ' Illegal' (0.027121)
  10. ' Boy' (0.026270)
  11. '_FM' (0.025645)
  12. ' Organ' (0.025373)
  13. ' none' (0.025098)
  14. ' Minecraft' (0.024519)
  15. ' Donald' (0.022241)
  16. 'abella' (0.019557)
  17. ' binary' (0.018422)
  18. ' English' (0.015721)
  19. ' Digital' (0.014980)
  20. ' thirteen' (0.014304)

Layer 15 (after block 14) (entropy: 7.850):
   1. ' Answer' (0.177782)
   2. ' Incorrect' (0.119043)
   3. ' Binary' (0.078020)
   4. ' Knowledge' (0.077225)
   5. '在全球' (0.065494)
   6. ' Correct' (0.061153)
   7. '正确的' (0.060682)
   8. '_FM' (0.048122)
   9. ' FM' (0.035431)
  10. ' Statements' (0.034922)
  11. ' Prompt' (0.026776)
  12. ' prefixed' (0.026747)
  13. ' hierarchical' (0.026528)
  14. ' $__' (0.026333)
  15. 'abella' (0.025091)
  16. '不服' (0.024200)
  17. ' The' (0.023536)
  18. ' First' (0.022536)
  19. ' Minecraft' (0.020316)
  20. '太阳城' (0.020063)

Layer 16 (after block 15) (entropy: 7.688):
   1. ' Answer' (0.252057)
   2. ' Classes' (0.167128)
   3. '正确的' (0.094753)
   4. ' Incorrect' (0.060262)
   5. ' Knowledge' (0.052698)
   6. ' Context' (0.048876)
   7. '在全球' (0.034409)
   8. ' política' (0.030191)
   9. ' Earth' (0.027605)
  10. ' The' (0.026568)
  11. ' First' (0.025136)
  12. '在游戏中' (0.025027)
  13. ' ____' (0.023715)
  14. ' Define' (0.022041)
  15. 'ilik' (0.020751)
  16. ' Minecraft' (0.020714)
  17. 'GraphNode' (0.017408)
  18. ' Statements' (0.017371)
  19. ' answer' (0.017277)
  20. ' __' (0.016012)

Layer 17 (after block 16) (entropy: 7.252):
   1. ' Answer' (0.435659)
   2. ' Incorrect' (0.096652)
   3. '编码' (0.060321)
   4. ' __' (0.041155)
   5. ' Classes' (0.038301)
   6. '_encoding' (0.035124)
   7. ' Architect' (0.034305)
   8. '_chunks' (0.028499)
   9. '在全球' (0.022668)
  10. 'ilik' (0.021153)
  11. '_topology' (0.020768)
  12. '_assert' (0.020579)
  13. ' política' (0.020299)
  14. ' answer' (0.019232)
  15. '第一条' (0.018639)
  16. ' First' (0.018407)
  17. 'ichel' (0.018106)
  18. ' trivia' (0.018027)
  19. 'aal' (0.016194)
  20. ' Define' (0.015911)

Layer 18 (after block 17) (entropy: 3.533):
   1. ' Answer' (0.742905)
   2. '回答' (0.065321)
   3. ' None' (0.039149)
   4. '_ANS' (0.030760)
   5. ' Classes' (0.022546)
   6. '在全球' (0.021262)
   7. '.answer' (0.015777)
   8. '编码' (0.011688)
   9. ' answer' (0.006457)
  10. ' 数' (0.006103)
  11. 'GraphNode' (0.005430)
  12. ' Incorrect' (0.004376)
  13. ' none' (0.004237)
  14. '第一条' (0.003951)
  15. ' AssertionError' (0.003638)
  16. ' answered' (0.003459)
  17. '在游戏中' (0.003448)
  18. '_answer' (0.003206)
  19. ' The' (0.003169)
  20. '我知道' (0.003117)

Layer 19 (after block 18) (entropy: 1.230):
   1. ' Answer' (0.880622)
   2. '回答' (0.060891)
   3. '_ANS' (0.011195)
   4. '.answer' (0.008366)
   5. '的回答' (0.005707)
   6. 'คำตอบ' (0.004095)
   7. ' ______' (0.003758)
   8. '我知道' (0.003157)
   9. '_answer' (0.003044)
  10. ' _____' (0.002700)
  11. ' ___' (0.002607)
  12. '嘘' (0.002204)
  13. ' None' (0.002171)
  14. '世界第一' (0.001527)
  15. ' Prompt' (0.001475)
  16. ' Located' (0.001370)
  17. ' ____' (0.001333)
  18. 'ichel' (0.001290)
  19. ' answered' (0.001267)
  20. '的答案' (0.001221)

Layer 20 (after block 19) (entropy: 0.143):
   1. ' Answer' (0.983307)
   2. '_ANS' (0.006656)
   3. '的回答' (0.002642)
   4. '的答案' (0.001963)
   5. ' None' (0.001099)
   6. ' none' (0.001056)
   7. '_answer' (0.000544)
   8. '回答' (0.000460)
   9. ' ______' (0.000428)
  10. ' Yes' (0.000352)
  11. ' ____' (0.000338)
  12. ' answer' (0.000302)
  13. '世界第一' (0.000153)
  14. ' ___' (0.000125)
  15. '.answer' (0.000122)
  16. ' yes' (0.000105)
  17. ' Ans' (0.000097)
  18. 'คำตอบ' (0.000090)
  19. ' unequiv' (0.000088)
  20. ' $__' (0.000072)

Layer 21 (after block 20) (entropy: 0.034):
   1. ' Answer' (0.996154)
   2. '_ANS' (0.002493)
   3. ' ____' (0.000292)
   4. ' none' (0.000181)
   5. '回答' (0.000172)
   6. ' answer' (0.000088)
   7. '我知道' (0.000080)
   8. ' ______' (0.000079)
   9. ' Yes' (0.000077)
  10. ' ___' (0.000066)
  11. '_answer' (0.000059)
  12. ' None' (0.000050)
  13. '的答案' (0.000046)
  14. ' yes' (0.000037)
  15. ' $__' (0.000029)
  16. ' _____' (0.000024)
  17. '这个问题' (0.000020)
  18. 'คำตอบ' (0.000019)
  19. ' __' (0.000018)
  20. '.answer' (0.000017)

Layer 22 (after block 21) (entropy: 0.045):
   1. ' Answer' (0.995394)
   2. ' ____' (0.001375)
   3. '_ANS' (0.001329)
   4. ' ______' (0.000441)
   5. '我知道' (0.000311)
   6. '的答案' (0.000218)
   7. ' Ans' (0.000171)
   8. ' __' (0.000144)
   9. ' ANSW' (0.000120)
  10. '回答' (0.000075)
  11. ' answered' (0.000062)
  12. ' ...

' (0.000061)
  13. ' ___' (0.000053)
  14. '_answer' (0.000043)
  15. ' answer' (0.000042)
  16. '当然是' (0.000042)
  17. 'zell' (0.000040)
  18. '____' (0.000032)
  19. '答' (0.000024)
  20. ' None' (0.000023)

Layer 23 (after block 22) (entropy: 0.221):
   1. ' Answer' (0.956814)
   2. ' ____' (0.031921)
   3. ' ______' (0.005996)
   4. ' The' (0.001877)
   5. ' Ans' (0.001297)
   6. ' __' (0.000744)
   7. '的答案' (0.000395)
   8. ' __________________' (0.000246)
   9. '____' (0.000201)
  10. ' _____' (0.000170)
  11. ' ...
' (0.000168)
  12. ' ...

' (0.000038)
  13. ' answer' (0.000025)
  14. ' answered' (0.000025)
  15. ' ___' (0.000017)
  16. ' ANSW' (0.000017)
  17. ' Please' (0.000016)
  18. '答え' (0.000013)
  19. ' There' (0.000010)
  20. ' Answers' (0.000010)

Layer 24 (after block 23) (entropy: 0.486):
   1. ' ______' (0.880692)
   2. ' ____' (0.086026)
   3. ' The' (0.012818)
   4. ' __' (0.008618)
   5. ' __________________' (0.007202)
   6. ' _____' (0.002103)
   7. '____' (0.001968)
   8. ' ___' (0.000216)
   9. ' Answer' (0.000175)
  10. ' Located' (0.000079)
  11. ' There' (0.000049)
  12. ' ...
' (0.000022)
  13. ' Yes' (0.000015)
  14. ' ...

' (0.000007)
  15. ' None' (0.000005)
  16. '__
' (0.000001)
  17. ' A' (0.000001)
  18. ' Ans' (0.000001)
  19. ' This' (0.000001)
  20. '---------
' (0.000000)

Layer 25 (after block 24) (entropy: 0.001):
   1. ' Germany' (0.999926)
   2. ' ____' (0.000056)
   3. ' ______' (0.000010)
   4. ' __________________' (0.000004)
   5. ' _____' (0.000002)
   6. ' __' (0.000002)
   7. ' ___' (0.000000)
   8. '____' (0.000000)
   9. ' Berlin' (0.000000)
  10. ' German' (0.000000)
  11. ' Deutschland' (0.000000)
  12. ' Europe' (0.000000)
  13. ' The' (0.000000)
  14. ' Munich' (0.000000)
  15. ' Germans' (0.000000)
  16. '__
' (0.000000)
  17. 'Germany' (0.000000)
  18. '____________' (0.000000)
  19. ' _______,' (0.000000)
  20. '_____' (0.000000)

Layer 26 (after block 25) (entropy: 0.656):
   1. ' ____' (0.815687)
   2. ' ______' (0.120622)
   3. '____' (0.038330)
   4. ' __' (0.019201)
   5. ' __________________' (0.005488)
   6. ' _____' (0.000373)
   7. ' Germany' (0.000248)
   8. ' ___' (0.000052)
   9. '________' (0.000000)
  10. '____________' (0.000000)
  11. '__
' (0.000000)
  12. '_____' (0.000000)
  13. '________________' (0.000000)
  14. ' The' (0.000000)
  15. '__[' (0.000000)
  16. ' Berlin' (0.000000)
  17. ' _______,' (0.000000)
  18. '___' (0.000000)
  19. '[__' (0.000000)
  20. '__

' (0.000000)

Layer 27 (after block 26) (entropy: 0.054):
   1. ' Germany' (0.992544)
   2. ' ____' (0.002756)
   3. ' ______' (0.002511)
   4. ' Berlin' (0.001675)
   5. ' __________________' (0.000309)
   6. ' __' (0.000195)
   7. ' ___' (0.000006)
   8. ' _____' (0.000004)
   9. '____' (0.000001)
  10. ' The' (0.000000)
  11. '__
' (0.000000)
  12. ' _______,' (0.000000)
  13. '_____' (0.000000)
  14. ' German' (0.000000)
  15. '__

' (0.000000)
  16. '________' (0.000000)
  17. '____________' (0.000000)
  18. '__[' (0.000000)
  19. '________________' (0.000000)
  20. ' __________________________________' (0.000000)

Layer 28 (after block 27) (entropy: 0.000):
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. ' Paris' (0.000000)
   4. ' Munich' (0.000000)
   5. ' Frankfurt' (0.000000)
   6. ' Rome' (0.000000)
   7. ' The' (0.000000)
   8. ' __________________' (0.000000)
   9. ' London' (0.000000)
  10. ' Madrid' (0.000000)
  11. ' ______' (0.000000)
  12. 'Berlin' (0.000000)
  13. ' __' (0.000000)
  14. ' cities' (0.000000)
  15. ' ____' (0.000000)
  16. ' _____' (0.000000)
  17. ' Vienna' (0.000000)
  18. ' Москва' (0.000000)
  19. ' Beijing' (0.000000)
  20. ' ___' (0.000000)

Layer 29 (after block 28) (entropy: 0.000):
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. ' Munich' (0.000000)
   4. ' Frankfurt' (0.000000)
   5. 'Berlin' (0.000000)
   6. 'Germany' (0.000000)
   7. ' Stuttgart' (0.000000)
   8. ' Hamburg' (0.000000)
   9. ' German' (0.000000)
  10. ' Vienna' (0.000000)
  11. ' München' (0.000000)
  12. ' Paris' (0.000000)
  13. '柏林' (0.000000)
  14. ' __________________' (0.000000)
  15. '德国' (0.000000)
  16. ' berlin' (0.000000)
  17. '武汉' (0.000000)
  18. ' Prague' (0.000000)
  19. ' Rome' (0.000000)
  20. ' Germans' (0.000000)

Layer 30 (after block 29) (entropy: 0.006):
   1. ' Berlin' (0.999210)
   2. ' Germany' (0.000790)
   3. ' Frankfurt' (0.000000)
   4. 'Berlin' (0.000000)
   5. 'Germany' (0.000000)
   6. ' Munich' (0.000000)
   7. ' German' (0.000000)
   8. '德国' (0.000000)
   9. ' germany' (0.000000)
  10. '柏林' (0.000000)
  11. ' Stuttgart' (0.000000)
  12. ' Hamburg' (0.000000)
  13. ' Paris' (0.000000)
  14. ' Germans' (0.000000)
  15. 'ドイツ' (0.000000)
  16. ' Deutschland' (0.000000)
  17. ' frankfurt' (0.000000)
  18. ' München' (0.000000)
  19. ' __________________' (0.000000)
  20. ' berlin' (0.000000)

Layer 31 (after block 30) (entropy: 0.000):
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. 'Berlin' (0.000000)
   4. ' Frankfurt' (0.000000)
   5. ' Munich' (0.000000)
   6. '柏林' (0.000000)
   7. ' Hamburg' (0.000000)
   8. ' Paris' (0.000000)
   9. ' Stuttgart' (0.000000)
  10. 'Germany' (0.000000)
  11. ' berlin' (0.000000)
  12. ' Vienna' (0.000000)
  13. ' München' (0.000000)
  14. ' frankfurt' (0.000000)
  15. ' London' (0.000000)
  16. ' Dresden' (0.000000)
  17. ' Brussels' (0.000000)
  18. '武汉' (0.000000)
  19. ' Cologne' (0.000000)
  20. ' Amsterdam' (0.000000)

Layer 32 (after block 31) (entropy: 0.000):
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. ' German' (0.000000)
   4. 'Berlin' (0.000000)
   5. '柏林' (0.000000)
   6. '德国' (0.000000)
   7. 'Germany' (0.000000)
   8. ' Germans' (0.000000)
   9. ' Frankfurt' (0.000000)
  10. ' Hamburg' (0.000000)
  11. ' Munich' (0.000000)
  12. ' germany' (0.000000)
  13. 'ドイツ' (0.000000)
  14. 'German' (0.000000)
  15. ' berlin' (0.000000)
  16. ' Stuttgart' (0.000000)
  17. ' Deutschland' (0.000000)
  18. '武汉' (0.000000)
  19. ' Dresden' (0.000000)
  20. ' german' (0.000000)

Layer 33 (after block 32) (entropy: 0.000):
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. '柏林' (0.000000)
   4. 'Berlin' (0.000000)
   5. ' German' (0.000000)
   6. ' Frankfurt' (0.000000)
   7. ' Munich' (0.000000)
   8. '德国' (0.000000)
   9. 'Germany' (0.000000)
  10. ' berlin' (0.000000)
  11. ' Hamburg' (0.000000)
  12. ' Germans' (0.000000)
  13. ' Stuttgart' (0.000000)
  14. ' germany' (0.000000)
  15. 'ドイツ' (0.000000)
  16. 'German' (0.000000)
  17. ' Dresden' (0.000000)
  18. ' Deutschland' (0.000000)
  19. ' München' (0.000000)
  20. '法兰' (0.000000)

Layer 34 (after block 33) (entropy: -0.000):
   1. ' Berlin' (1.000000)
   2. '柏林' (0.000000)
   3. 'Berlin' (0.000000)
   4. ' Frankfurt' (0.000000)
   5. ' Germany' (0.000000)
   6. ' berlin' (0.000000)
   7. ' Hamburg' (0.000000)
   8. ' Munich' (0.000000)
   9. ' BER' (0.000000)
  10. ' Ber' (0.000000)
  11. ' Bon' (0.000000)
  12. '_ber' (0.000000)
  13. ' Stuttgart' (0.000000)
  14. '法兰' (0.000000)
  15. ' Paris' (0.000000)
  16. ' The' (0.000000)
  17. ' German' (0.000000)
  18. ' Bern' (0.000000)
  19. ' London' (0.000000)
  20. ' Brussels' (0.000000)

Layer 35 (after block 34) (entropy: -0.000):
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. 'Berlin' (0.000000)
   4. ' The' (0.000000)
   5. ' __' (0.000000)
   6. '柏林' (0.000000)
   7. ' Frankfurt' (0.000000)
   8. ' ______' (0.000000)
   9. ' Hamburg' (0.000000)
  10. ' Bon' (0.000000)
  11. ' Munich' (0.000000)
  12. ' German' (0.000000)
  13. ' __________________' (0.000000)
  14. ' A' (0.000000)
  15. ' Ber' (0.000000)
  16. ' ____' (0.000000)
  17. ' ___' (0.000000)
  18. ' berlin' (0.000000)
  19. ' _____' (0.000000)
  20. ' London' (0.000000)

Layer 36 (after block 35) (entropy: 0.000):
   1. ' Berlin' (1.000000)
   2. ' The' (0.000000)
   3. ' Germany' (0.000000)
   4. ' ' (0.000000)
   5. ' __' (0.000000)
   6. ' 

' (0.000000)
   7. ' ______' (0.000000)
   8. ' A' (0.000000)
   9. ' (' (0.000000)
  10. ' What' (0.000000)
  11. ' Ber' (0.000000)
  12. ' This' (0.000000)
  13. ' Bon' (0.000000)
  14. ' ?

' (0.000000)
  15. ' __________________' (0.000000)
  16. ' Frankfurt' (0.000000)
  17. ' [' (0.000000)
  18. ' ?' (0.000000)
  19. ' I' (0.000000)
  20. ' ___' (0.000000)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 1.400):
   1. ' Berlin' (0.745596)
   2. ' The' (0.089245)
   3. ' Germany' (0.036124)
   4. ' ' (0.035922)
   5. ' __' (0.035451)
   6. ' 

' (0.011033)
   7. ' ______' (0.009584)
   8. ' A' (0.008251)
   9. ' (' (0.003598)
  10. ' What' (0.002906)
  11. ' Ber' (0.002744)
  12. ' This' (0.002706)
  13. ' Bon' (0.002565)
  14. ' ?

' (0.002480)
  15. ' __________________' (0.002283)
  16. ' Frankfurt' (0.002216)
  17. ' [' (0.002216)
  18. ' ?' (0.001902)
  19. ' I' (0.001733)
  20. ' ___' (0.001449)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 2.072)
   1. ' Berlin' (0.702774)
   2. ' not' (0.080083)
   3. ' a' (0.077099)
   4. ' in' (0.047015)
   5. ' located' (0.024993)
   6. ' the' (0.018349)
   7. ' __' (0.018027)
   8. ' called' (0.013510)
   9. ' ______' (0.009370)
  10. ' known' (0.008779)

Prompt: 'Berlin is the capital of'
  (entropy: 0.835)
   1. ' Germany' (0.735213)
   2. ' which' (0.222747)
   3. ' the' (0.023954)
   4. ' what' (0.011459)
   5. ' __' (0.002318)
   6. ' ______' (0.001459)
   7. ' a' (0.001134)
   8. ' German' (0.000809)
   9. ' Berlin' (0.000463)
  10. '
' (0.000443)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 1.132)
   1. ' Berlin' (0.747337)
   2. ' The' (0.180335)
   3. ' Also' (0.015636)
   4. '
' (0.013268)
   5. ' What' (0.009999)
   6. ' 

' (0.008984)
   7. ' ' (0.008722)
   8. ' Ber' (0.008149)
   9. ' Additionally' (0.004382)
  10. ' In' (0.003189)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000)
   1. ' Berlin' (1.000000)
   2. ' The' (0.000000)
   3. ' Germany' (0.000000)
   4. ' ' (0.000000)
   5. ' __' (0.000000)
   6. ' 

' (0.000000)
   7. ' ______' (0.000000)
   8. ' A' (0.000000)
   9. ' (' (0.000000)
  10. ' What' (0.000000)
  11. ' Ber' (0.000000)
  12. ' This' (0.000000)
  13. ' Bon' (0.000000)
  14. ' ?

' (0.000000)
  15. ' __________________' (0.000000)

Temperature 2.0:
  (entropy: 9.109)
   1. ' Berlin' (0.361233)
   2. ' The' (0.124976)
   3. ' Germany' (0.079512)
   4. ' ' (0.079289)
   5. ' __' (0.078768)
   6. ' 

' (0.043942)
   7. ' ______' (0.040954)
   8. ' A' (0.038001)
   9. ' (' (0.025093)
  10. ' What' (0.022552)
  11. ' Ber' (0.021913)
  12. ' This' (0.021760)
  13. ' Bon' (0.021186)
  14. ' ?

' (0.020832)
  15. ' __________________' (0.019988)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 36
Model dimension: 4096
Number of heads: 32
Vocab size: 151936
Context length: 2048
=== END OF MODEL STATS ========

