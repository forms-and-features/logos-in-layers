
============================================================
EVALUATING MODEL: Qwen/Qwen3-8B
============================================================
Loading model: Qwen/Qwen3-8B...
Loaded pretrained model Qwen/Qwen3-8B into HookedTransformer

=== NORMALIZATION ANALYSIS ========
Block normalization type: RMSNormPre
Using NORMALISED residual stream (RMS, no learnable scale)
Final normalization type: RMSNormPre
=== END NORMALIZATION ANALYSIS ====


=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['Question', ':', ' What', ' is', ' the', ' capital', ' of', ' Germany', '?', ' Answer', ':']
Computing layer-wise predictions (memory-efficient targeted caching)...

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using NORMALIZED residual stream (RMS, no learnable scale)
Note: Shown probabilities are from full softmax (calibrated and comparable)
------------------------------------------------------------
Layer  0 (embeddings):
[diagnostic] No separate positional embedding hook; using only token embeddings for layer 0 residual.
  (entropy: 5.671 bits):
   1. 'いらっ' (0.277301)
   2. ' binaries' (0.218554)
   3. 'おすす' (0.120685)
   4. '家喻户' (0.080565)
   5. 'インターネ' (0.075383)
   6. '(DBG' (0.033797)
   7. ' binary' (0.031892)
   8. 'FindObject' (0.023212)
   9. '烟花爆' (0.022041)
  10. 'お互' (0.021471)
  11. 'アメリ' (0.017619)
  12. '特色社会' (0.014934)
  13. 'InputBorder' (0.012510)
  14. 'ご�' (0.010509)
  15. '/import' (0.008771)
  16. '続きを読' (0.006940)
  17. '電話及' (0.006818)
  18. '.CG' (0.005699)
  19. ' DIR' (0.005683)
  20. '(CONT' (0.005617)

Layer  1 (after transformer block 0) (entropy: 10.974 bits):
   1. ' ListViewItem' (0.132090)
   2. '临港' (0.111955)
   3. '三大职业' (0.111198)
   4. ' Lauderdale' (0.078836)
   5. ' Buccane' (0.078353)
   6. '负面影响' (0.071198)
   7. '生产和' (0.040152)
   8. ' Phonetic' (0.037952)
   9. ' Seeder' (0.037472)
  10. '总队' (0.036216)
  11. ' phé' (0.034768)
  12. 'ichtet' (0.031921)
  13. '单职业' (0.027881)
  14. '前述' (0.027018)
  15. 'uppy' (0.026386)
  16. '/gpl' (0.026231)
  17. '密切相关' (0.024692)
  18. 'endencies' (0.023654)
  19. '中关村' (0.023242)
  20. '嘉年华' (0.018786)

Layer  2 (after transformer block 1) (entropy: 10.000 bits):
   1. ' Buccane' (0.266377)
   2. ' Phonetic' (0.084958)
   3. 'öffentlich' (0.082778)
   4. ' coquine' (0.077809)
   5. ' Seeder' (0.057447)
   6. '凝聚力' (0.039447)
   7. ' Cougar' (0.038653)
   8. '三大职业' (0.037923)
   9. ' datings' (0.037892)
  10. '下半场' (0.032481)
  11. '批复' (0.031126)
  12. '直接影响' (0.029012)
  13. ' catcher' (0.028060)
  14. ' Sesso' (0.025691)
  15. 'rijk' (0.025194)
  16. ' ListViewItem' (0.024859)
  17. '嘉年华' (0.022497)
  18. '负面影响' (0.020596)
  19. ' squirt' (0.018858)
  20. '太快' (0.018345)

Layer  3 (after transformer block 2) (entropy: 9.901 bits):
   1. ' Lauderdale' (0.227186)
   2. ' Buccane' (0.134659)
   3. ' Phonetic' (0.082134)
   4. '批复' (0.070612)
   5. ' ValidationResult' (0.064103)
   6. 'nehmer' (0.043493)
   7. 'mittel' (0.038258)
   8. 'arrison' (0.035348)
   9. 'öffentlich' (0.032422)
  10. '三大职业' (0.032131)
  11. 'başı' (0.029199)
  12. 'ismet' (0.028165)
  13. '并不意味' (0.027300)
  14. '直接影响' (0.027184)
  15. '第二批' (0.023488)
  16. ' coquine' (0.022475)
  17. '延误' (0.021531)
  18. 'anzeigen' (0.020948)
  19. ' Seeder' (0.020156)
  20. 'ledon' (0.019210)

Layer  4 (after transformer block 3) (entropy: 11.345 bits):
   1. ' Buccane' (0.108950)
   2. 'caf' (0.095059)
   3. ' Lauderdale' (0.075248)
   4. 'arrison' (0.070759)
   5. '直接影响' (0.069202)
   6. 'führ' (0.066611)
   7. '批复' (0.059528)
   8. '凝聚力' (0.053404)
   9. '第二批' (0.050112)
  10. 'öffentlich' (0.044041)
  11. '昴' (0.038372)
  12. 'nehmer' (0.033560)
  13. '付け' (0.032659)
  14. 'aggregate' (0.031071)
  15. '延误' (0.030145)
  16. 'vlc' (0.030008)
  17. 'mittel' (0.029105)
  18. ' ListViewItem' (0.027880)
  19. 'licken' (0.027288)
  20. 'portion' (0.026998)

Layer  5 (after transformer block 4) (entropy: 12.906 bits):
   1. '直接影响' (0.190186)
   2. 'portion' (0.088071)
   3. 'arrison' (0.083235)
   4. 'razier' (0.058375)
   5. 'führ' (0.055014)
   6. 'ход' (0.054108)
   7. '单职业' (0.043971)
   8. 'caf' (0.042953)
   9. '(typeof' (0.041848)
  10. '在此之前' (0.041461)
  11. '我省' (0.034860)
  12. 'nehmer' (0.034623)
  13. ' тоже' (0.034122)
  14. '并不代表' (0.032528)
  15. 'directories' (0.029825)
  16. '付け' (0.029556)
  17. 'ым' (0.028596)
  18. '.SpringApplication' (0.028091)
  19. 'affe' (0.026260)
  20. ' ebenfalls' (0.022318)

Layer  6 (after transformer block 5) (entropy: 14.145 bits):
   1. '我省' (0.149908)
   2. 'abella' (0.099982)
   3. '(typeof' (0.082773)
   4. '不利于' (0.079436)
   5. 'portion' (0.064084)
   6. ' lâu' (0.055747)
   7. '�인' (0.042599)
   8. 'alter' (0.040235)
   9. 'ständ' (0.040193)
  10. 'führ' (0.039616)
  11. '直接影响' (0.036823)
  12. 'ingle' (0.034948)
  13. 'ivers' (0.034562)
  14. '略' (0.031915)
  15. 'steller' (0.030609)
  16. 'ив' (0.029021)
  17. ' 부분' (0.028185)
  18. ' информации' (0.027306)
  19. 'oster' (0.026988)
  20. 'bbie' (0.025071)

Layer  7 (after transformer block 6) (entropy: 13.430 bits):
   1. 'portion' (0.138526)
   2. 'steller' (0.100934)
   3. '<Entry' (0.075298)
   4. 'führ' (0.072726)
   5. ' Located' (0.072136)
   6. ' lẽ' (0.055722)
   7. '�인' (0.052708)
   8. '样的' (0.045329)
   9. 'alter' (0.038027)
  10. ' Seconds' (0.037040)
  11. 'abella' (0.036811)
  12. 'ив' (0.033655)
  13. ' ArrayAdapter' (0.032447)
  14. ' Rap' (0.032441)
  15. 'ivers' (0.031661)
  16. 'asio' (0.030621)
  17. 'ать' (0.029484)
  18. 'owe' (0.029166)
  19. ' Scientists' (0.028519)
  20. 'Locator' (0.026749)

Layer  8 (after transformer block 7) (entropy: 12.072 bits):
   1. 'steller' (0.294668)
   2. ' عنه' (0.125214)
   3. '付け' (0.115381)
   4. ' accumulator' (0.056584)
   5. '.answer' (0.042017)
   6. ' Wel' (0.041116)
   7. '答' (0.039707)
   8. 'mittel' (0.034099)
   9. ' Ergebn' (0.029011)
  10. 'portion' (0.023971)
  11. 'не' (0.023892)
  12. 'нес' (0.023027)
  13. '在床上' (0.022473)
  14. ' Instead' (0.019948)
  15. 'owe' (0.019850)
  16. ' Locations' (0.019567)
  17. ' San' (0.017960)
  18. '꧁' (0.017497)
  19. 'ivers' (0.017350)
  20. 'abella' (0.016671)

Layer  9 (after transformer block 8) (entropy: 11.299 bits):
   1. ' Mus' (0.295152)
   2. 'ihar' (0.072047)
   3. ' Enumerator' (0.066784)
   4. '答' (0.061234)
   5. ' Eleven' (0.052528)
   6. 'ывать' (0.042473)
   7. ' Iter' (0.042019)
   8. ' Answer' (0.039423)
   9. 'immer' (0.038793)
  10. ' unpopular' (0.033173)
  11. ' Parliamentary' (0.032281)
  12. 'ما' (0.032027)
  13. '付け' (0.029416)
  14. ' nd' (0.026835)
  15. ' Visit' (0.025366)
  16. ' Element' (0.024600)
  17. 'ndern' (0.022515)
  18. ' Sessions' (0.021924)
  19. ' Moral' (0.020823)
  20. ' Tiger' (0.020588)

Layer 10 (after transformer block 9) (entropy: 11.532 bits):
   1. '在游戏中' (0.255036)
   2. 'owe' (0.149331)
   3. '�' (0.136394)
   4. ' Mus' (0.041201)
   5. '的职业' (0.040250)
   6. 'steller' (0.037589)
   7. ' unpopular' (0.035029)
   8. 'abella' (0.034267)
   9. ' Parliamentary' (0.033326)
  10. 'beit' (0.028758)
  11. 'hei' (0.028585)
  12. ' Binary' (0.024359)
  13. '样的' (0.022875)
  14. '瑟' (0.022593)
  15. ' Illegal' (0.019566)
  16. ' *__' (0.019488)
  17. '_PT' (0.018577)
  18. ' Southern' (0.018091)
  19. ' unre' (0.017549)
  20. ' Frames' (0.017137)

Layer 11 (after transformer block 10) (entropy: 12.148 bits):
   1. '在游戏中' (0.164523)
   2. ' Gem' (0.151584)
   3. ' Incorrect' (0.113182)
   4. ' Illegal' (0.083837)
   5. 'áp' (0.059850)
   6. ' Entre' (0.049595)
   7. 'abella' (0.047128)
   8. ' vậy' (0.037700)
   9. ' Boy' (0.037289)
  10. ' Adult' (0.026106)
  11. ' Desk' (0.025459)
  12. ' Participation' (0.025349)
  13. 'owe' (0.025342)
  14. 'gebn' (0.024645)
  15. ' Binary' (0.023285)
  16. ' Mus' (0.022183)
  17. 'anker' (0.022123)
  18. '史上最' (0.020563)
  19. ' Parliamentary' (0.020428)
  20. ' Occupation' (0.019831)

Layer 12 (after transformer block 11) (entropy: 12.156 bits):
   1. ' Answer' (0.165589)
   2. ' Incorrect' (0.153490)
   3. ' Electoral' (0.090545)
   4. ' Desk' (0.064224)
   5. ' Illegal' (0.057071)
   6. ' Boy' (0.052962)
   7. ' Develop' (0.049896)
   8. 'áp' (0.047950)
   9. '不服' (0.047326)
  10. ' Entities' (0.037428)
  11. ' Gem' (0.030085)
  12. ' Cities' (0.029709)
  13. '答え' (0.028708)
  14. ' Prompt' (0.024007)
  15. 'あれ' (0.022089)
  16. ' VE' (0.020674)
  17. '在游戏中' (0.019903)
  18. ' Parliamentary' (0.019879)
  19. ' Begin' (0.019779)
  20. 'abella' (0.018688)

Layer 13 (after transformer block 12) (entropy: 11.685 bits):
   1. ' Answer' (0.326783)
   2. 'abella' (0.109808)
   3. ' Incorrect' (0.083602)
   4. '在游戏中' (0.052410)
   5. '在全球' (0.047274)
   6. ' Binary' (0.038355)
   7. ' VE' (0.037556)
   8. '答え' (0.033696)
   9. ' Frag' (0.029400)
  10. ' thirteen' (0.028484)
  11. ' Prompt' (0.026743)
  12. ' Knowledge' (0.026652)
  13. '_FM' (0.025503)
  14. 'oday' (0.023802)
  15. ' tsl' (0.021429)
  16. ' Intelligent' (0.019335)
  17. ' Boy' (0.019089)
  18. ' Entities' (0.017690)
  19. ' Minecraft' (0.016196)
  20. '_primitive' (0.016193)

Layer 14 (after transformer block 13) (entropy: 10.953 bits):
   1. ' Binary' (0.421902)
   2. ' Answer' (0.147943)
   3. ' Incorrect' (0.043858)
   4. '在游戏中' (0.039501)
   5. ' Prompt' (0.031046)
   6. ' The' (0.029204)
   7. ' Intelligent' (0.028568)
   8. '在全球' (0.025676)
   9. ' Illegal' (0.024659)
  10. ' Boy' (0.023831)
  11. '_FM' (0.023224)
  12. ' Organ' (0.022959)
  13. ' none' (0.022693)
  14. ' Minecraft' (0.022132)
  15. ' Donald' (0.019935)
  16. 'abella' (0.017367)
  17. ' binary' (0.016288)
  18. ' English' (0.013743)
  19. ' Digital' (0.013050)
  20. ' thirteen' (0.012419)

Layer 15 (after transformer block 14) (entropy: 11.604 bits):
   1. ' Answer' (0.173176)
   2. ' Incorrect' (0.117139)
   3. ' Binary' (0.077597)
   4. ' Knowledge' (0.076829)
   5. '在全球' (0.065430)
   6. ' Correct' (0.061200)
   7. '正确的' (0.060741)
   8. '_FM' (0.048452)
   9. ' FM' (0.035953)
  10. ' Statements' (0.035449)
  11. ' Prompt' (0.027364)
  12. ' prefixed' (0.027334)
  13. ' hierarchical' (0.027116)
  14. ' $__' (0.026921)
  15. 'abella' (0.025684)
  16. '不服' (0.024795)
  17. ' The' (0.024131)
  18. ' First' (0.023131)
  19. ' Minecraft' (0.020906)
  20. '太阳城' (0.020653)

Layer 16 (after transformer block 15) (entropy: 11.712 bits):
   1. ' Answer' (0.236697)
   2. ' Classes' (0.160357)
   3. '正确的' (0.093656)
   4. ' Incorrect' (0.060990)
   5. ' Knowledge' (0.053713)
   6. ' Context' (0.050012)
   7. '在全球' (0.035862)
   8. ' política' (0.031683)
   9. ' Earth' (0.029105)
  10. ' The' (0.028068)
  11. ' First' (0.026633)
  12. '在游戏中' (0.026523)
  13. ' ____' (0.025203)
  14. ' Define' (0.023515)
  15. 'ilik' (0.022209)
  16. ' Minecraft' (0.022171)
  17. 'GraphNode' (0.018802)
  18. ' Statements' (0.018764)
  19. ' answer' (0.018668)
  20. ' __' (0.017370)

Layer 17 (after transformer block 16) (entropy: 12.629 bits):
   1. ' Answer' (0.331811)
   2. ' Incorrect' (0.095452)
   3. '编码' (0.064622)
   4. ' __' (0.047097)
   5. ' Classes' (0.044377)
   6. '_encoding' (0.041310)
   7. ' Architect' (0.040510)
   8. '_chunks' (0.034749)
   9. '在全球' (0.028753)
  10. 'ilik' (0.027154)
  11. '_topology' (0.026743)
  12. '_assert' (0.026542)
  13. ' política' (0.026243)
  14. ' answer' (0.025096)
  15. '第一条' (0.024454)
  16. ' First' (0.024202)
  17. 'ichel' (0.023875)
  18. ' trivia' (0.023788)
  19. 'aal' (0.021769)
  20. ' Define' (0.021453)

Layer 18 (after transformer block 17) (entropy: 10.841 bits):
   1. ' Answer' (0.559292)
   2. '回答' (0.085803)
   3. ' None' (0.057818)
   4. '_ANS' (0.048007)
   5. ' Classes' (0.037782)
   6. '在全球' (0.036110)
   7. '.answer' (0.028690)
   8. '编码' (0.022765)
   9. ' answer' (0.014407)
  10. ' 数' (0.013794)
  11. 'GraphNode' (0.012606)
  12. ' Incorrect' (0.010672)
  13. ' none' (0.010411)
  14. '第一条' (0.009865)
  15. ' AssertionError' (0.009257)
  16. ' answered' (0.008903)
  17. '在游戏中' (0.008881)
  18. '_answer' (0.008398)
  19. ' The' (0.008321)
  20. '我知道' (0.008217)

Layer 19 (after transformer block 18) (entropy: 8.556 bits):
   1. ' Answer' (0.677663)
   2. '回答' (0.102400)
   3. '_ANS' (0.030903)
   4. '.answer' (0.025149)
   5. '的回答' (0.019186)
   6. 'คำตอบ' (0.015172)
   7. ' ______' (0.014277)
   8. '我知道' (0.012621)
   9. '_answer' (0.012301)
  10. ' _____' (0.011301)
  11. ' ___' (0.011023)
  12. '嘘' (0.009788)
  13. ' None' (0.009684)
  14. '世界第一' (0.007551)
  15. ' Prompt' (0.007367)
  16. ' Located' (0.006993)
  17. ' ____' (0.006857)
  18. 'ichel' (0.006701)
  19. ' answered' (0.006615)
  20. '的答案' (0.006447)

Layer 20 (after transformer block 19) (entropy: 3.708 bits):
   1. ' Answer' (0.853419)
   2. '_ANS' (0.034938)
   3. '的回答' (0.019345)
   4. '的答案' (0.015999)
   5. ' None' (0.011042)
   6. ' none' (0.010762)
   7. '_answer' (0.007044)
   8. '回答' (0.006324)
   9. ' ______' (0.006038)
  10. ' Yes' (0.005324)
  11. ' ____' (0.005190)
  12. ' answer' (0.004833)
  13. '世界第一' (0.003133)
  14. ' ___' (0.002742)
  15. '.answer' (0.002709)
  16. ' yes' (0.002455)
  17. ' Ans' (0.002342)
  18. 'คำตอบ' (0.002225)
  19. ' unequiv' (0.002197)
  20. ' $__' (0.001938)

Layer 21 (after transformer block 20) (entropy: 2.600 bits):
   1. ' Answer' (0.908194)
   2. '_ANS' (0.028740)
   3. ' ____' (0.008340)
   4. ' none' (0.006331)
   5. '回答' (0.006149)
   6. ' answer' (0.004173)
   7. '我知道' (0.003972)
   8. ' ______' (0.003927)
   9. ' Yes' (0.003858)
  10. ' ___' (0.003536)
  11. '_answer' (0.003331)
  12. ' None' (0.003022)
  13. '的答案' (0.002875)
  14. ' yes' (0.002547)
  15. ' $__' (0.002193)
  16. ' _____' (0.001992)
  17. '这个问题' (0.001796)
  18. 'คำตอบ' (0.001736)
  19. ' __' (0.001670)
  20. '.answer' (0.001617)

Layer 22 (after transformer block 21) (entropy: 3.370 bits):
   1. ' Answer' (0.875080)
   2. ' ____' (0.021749)
   3. '_ANS' (0.021337)
   4. ' ______' (0.011494)
   5. '我知道' (0.009442)
   6. '的答案' (0.007741)
   7. ' Ans' (0.006757)
   8. ' __' (0.006122)
   9. ' ANSW' (0.005535)
  10. '回答' (0.004238)
  11. ' answered' (0.003839)
  12. ' ...

' (0.003781)
  13. ' ___' (0.003489)
  14. '_answer' (0.003128)
  15. ' answer' (0.003082)
  16. '当然是' (0.003081)
  17. 'zell' (0.003003)
  18. '____' (0.002626)
  19. '答' (0.002268)
  20. ' None' (0.002207)

Layer 23 (after transformer block 22) (entropy: 3.550 bits):
   1. ' Answer' (0.621863)
   2. ' ____' (0.129096)
   3. ' ______' (0.059583)
   4. ' The' (0.034826)
   5. ' Ans' (0.029360)
   6. ' __' (0.022696)
   7. '的答案' (0.016937)
   8. ' __________________' (0.013606)
   9. '____' (0.012382)
  10. ' _____' (0.011477)
  11. ' ...
' (0.011395)
  12. ' ...

' (0.005769)
  13. ' answer' (0.004762)
  14. ' answered' (0.004701)
  15. ' ___' (0.003996)
  16. ' ANSW' (0.003960)
  17. ' Please' (0.003887)
  18. '答え' (0.003534)
  19. ' There' (0.003093)
  20. ' Answers' (0.003076)

Layer 24 (after transformer block 23) (entropy: 4.101 bits):
   1. ' ______' (0.384784)
   2. ' ____' (0.171179)
   3. ' The' (0.088215)
   4. ' __' (0.076825)
   5. ' __________________' (0.072168)
   6. ' _____' (0.047012)
   7. '____' (0.045939)
   8. ' ___' (0.021269)
   9. ' Answer' (0.019770)
  10. ' Located' (0.015005)
  11. ' There' (0.012657)
  12. ' ...
' (0.009661)
  13. ' Yes' (0.008458)
  14. ' ...

' (0.006540)
  15. ' None' (0.005884)
  16. '__
' (0.003531)
  17. ' A' (0.002898)
  18. ' Ans' (0.002855)
  19. ' This' (0.002825)
  20. '---------
' (0.002527)

Layer 25 (after transformer block 24) (entropy: 1.411 bits):
   1. ' Germany' (0.802652)
   2. ' ____' (0.055620)
   3. ' ______' (0.034732)
   4. ' __________________' (0.027687)
   5. ' _____' (0.021489)
   6. ' __' (0.020720)
   7. ' ___' (0.010201)
   8. '____' (0.009618)
   9. ' Berlin' (0.006135)
  10. ' German' (0.004157)
  11. ' Deutschland' (0.001137)
  12. ' Europe' (0.000945)
  13. ' The' (0.000857)
  14. ' Munich' (0.000800)
  15. ' Germans' (0.000700)
  16. '__
' (0.000612)
  17. 'Germany' (0.000610)
  18. '____________' (0.000456)
  19. ' _______,' (0.000443)
  20. '_____' (0.000427)

Layer 26 (after transformer block 25) (entropy: 2.857 bits):
   1. ' ____' (0.300361)
   2. ' ______' (0.191624)
   3. '____' (0.146348)
   4. ' __' (0.124381)
   5. ' __________________' (0.092656)
   6. ' _____' (0.049235)
   7. ' Germany' (0.044736)
   8. ' ___' (0.030944)
   9. '________' (0.004070)
  10. '____________' (0.003152)
  11. '__
' (0.002853)
  12. '_____' (0.002474)
  13. '________________' (0.001848)
  14. ' The' (0.001209)
  15. '__[' (0.001166)
  16. ' Berlin' (0.000855)
  17. ' _______,' (0.000732)
  18. '___' (0.000525)
  19. '[__' (0.000524)
  20. '__

' (0.000305)

Layer 27 (after transformer block 26) (entropy: 2.917 bits):
   1. ' Germany' (0.393471)
   2. ' ____' (0.119439)
   3. ' ______' (0.117212)
   4. ' Berlin' (0.107978)
   5. ' __________________' (0.076687)
   6. ' __' (0.069844)
   7. ' ___' (0.034362)
   8. ' _____' (0.032001)
   9. '____' (0.022365)
  10. ' The' (0.009213)
  11. '__
' (0.006292)
  12. ' _______,' (0.002105)
  13. '_____' (0.002066)
  14. ' German' (0.001350)
  15. '__

' (0.001293)
  16. '________' (0.001200)
  17. '____________' (0.001038)
  18. '__[' (0.000763)
  19. '________________' (0.000707)
  20. ' __________________________________' (0.000616)

Layer 28 (after transformer block 27) (entropy: 0.430 bits):
   1. ' Berlin' (0.927426)
   2. ' Germany' (0.067941)
   3. ' Paris' (0.001708)
   4. ' Munich' (0.000447)
   5. ' Frankfurt' (0.000277)
   6. ' Rome' (0.000249)
   7. ' The' (0.000248)
   8. ' __________________' (0.000225)
   9. ' London' (0.000191)
  10. ' Madrid' (0.000171)
  11. ' ______' (0.000162)
  12. 'Berlin' (0.000149)
  13. ' __' (0.000133)
  14. ' cities' (0.000132)
  15. ' ____' (0.000129)
  16. ' _____' (0.000092)
  17. ' Vienna' (0.000090)
  18. ' Москва' (0.000089)
  19. ' Beijing' (0.000079)
  20. ' ___' (0.000062)

Layer 29 (after transformer block 28) (entropy: 0.194 bits):
   1. ' Berlin' (0.971296)
   2. ' Germany' (0.028003)
   3. ' Munich' (0.000328)
   4. ' Frankfurt' (0.000154)
   5. 'Berlin' (0.000130)
   6. 'Germany' (0.000023)
   7. ' Stuttgart' (0.000011)
   8. ' Hamburg' (0.000009)
   9. ' German' (0.000009)
  10. ' Vienna' (0.000008)
  11. ' München' (0.000006)
  12. ' Paris' (0.000005)
  13. '柏林' (0.000005)
  14. ' __________________' (0.000003)
  15. '德国' (0.000002)
  16. ' berlin' (0.000001)
  17. '武汉' (0.000001)
  18. ' Prague' (0.000001)
  19. ' Rome' (0.000001)
  20. ' Germans' (0.000001)

Layer 30 (after transformer block 29) (entropy: 0.871 bits):
   1. ' Berlin' (0.718371)
   2. ' Germany' (0.280505)
   3. ' Frankfurt' (0.000378)
   4. 'Berlin' (0.000280)
   5. 'Germany' (0.000170)
   6. ' Munich' (0.000094)
   7. ' German' (0.000090)
   8. '德国' (0.000029)
   9. ' germany' (0.000022)
  10. '柏林' (0.000016)
  11. ' Stuttgart' (0.000015)
  12. ' Hamburg' (0.000011)
  13. ' Paris' (0.000005)
  14. ' Germans' (0.000004)
  15. 'ドイツ' (0.000004)
  16. ' Deutschland' (0.000003)
  17. ' frankfurt' (0.000001)
  18. ' München' (0.000001)
  19. ' __________________' (0.000001)
  20. ' berlin' (0.000001)

Layer 31 (after transformer block 30) (entropy: 0.011 bits):
   1. ' Berlin' (0.999213)
   2. ' Germany' (0.000408)
   3. 'Berlin' (0.000171)
   4. ' Frankfurt' (0.000153)
   5. ' Munich' (0.000020)
   6. '柏林' (0.000016)
   7. ' Hamburg' (0.000012)
   8. ' Paris' (0.000004)
   9. ' Stuttgart' (0.000001)
  10. 'Germany' (0.000001)
  11. ' berlin' (0.000000)
  12. ' Vienna' (0.000000)
  13. ' München' (0.000000)
  14. ' frankfurt' (0.000000)
  15. ' London' (0.000000)
  16. ' Dresden' (0.000000)
  17. ' Brussels' (0.000000)
  18. '武汉' (0.000000)
  19. ' Cologne' (0.000000)
  20. ' Amsterdam' (0.000000)

Layer 32 (after transformer block 31) (entropy: 0.076 bits):
   1. ' Berlin' (0.991409)
   2. ' Germany' (0.007853)
   3. ' German' (0.000431)
   4. 'Berlin' (0.000142)
   5. '柏林' (0.000100)
   6. '德国' (0.000050)
   7. 'Germany' (0.000005)
   8. ' Germans' (0.000004)
   9. ' Frankfurt' (0.000004)
  10. ' Hamburg' (0.000000)
  11. ' Munich' (0.000000)
  12. ' germany' (0.000000)
  13. 'ドイツ' (0.000000)
  14. 'German' (0.000000)
  15. ' berlin' (0.000000)
  16. ' Stuttgart' (0.000000)
  17. ' Deutschland' (0.000000)
  18. '武汉' (0.000000)
  19. ' Dresden' (0.000000)
  20. ' german' (0.000000)

Layer 33 (after transformer block 32) (entropy: 0.011 bits):
   1. ' Berlin' (0.999185)
   2. ' Germany' (0.000345)
   3. '柏林' (0.000264)
   4. 'Berlin' (0.000174)
   5. ' German' (0.000021)
   6. ' Frankfurt' (0.000007)
   7. ' Munich' (0.000001)
   8. '德国' (0.000000)
   9. 'Germany' (0.000000)
  10. ' berlin' (0.000000)
  11. ' Hamburg' (0.000000)
  12. ' Germans' (0.000000)
  13. ' Stuttgart' (0.000000)
  14. ' germany' (0.000000)
  15. 'ドイツ' (0.000000)
  16. 'German' (0.000000)
  17. ' Dresden' (0.000000)
  18. ' Deutschland' (0.000000)
  19. ' München' (0.000000)
  20. '法兰' (0.000000)

Layer 34 (after transformer block 33) (entropy: 0.012 bits):
   1. ' Berlin' (0.999078)
   2. '柏林' (0.000500)
   3. 'Berlin' (0.000352)
   4. ' Frankfurt' (0.000038)
   5. ' Germany' (0.000012)
   6. ' berlin' (0.000005)
   7. ' Hamburg' (0.000004)
   8. ' Munich' (0.000004)
   9. ' BER' (0.000002)
  10. ' Ber' (0.000001)
  11. ' Bon' (0.000001)
  12. '_ber' (0.000001)
  13. ' Stuttgart' (0.000000)
  14. '法兰' (0.000000)
  15. ' Paris' (0.000000)
  16. ' The' (0.000000)
  17. ' German' (0.000000)
  18. ' Bern' (0.000000)
  19. ' London' (0.000000)
  20. ' Brussels' (0.000000)

Layer 35 (after transformer block 34) (entropy: 0.130 bits):
   1. ' Berlin' (0.989823)
   2. ' Germany' (0.003138)
   3. 'Berlin' (0.001611)
   4. ' The' (0.001328)
   5. ' __' (0.000849)
   6. '柏林' (0.000724)
   7. ' Frankfurt' (0.000679)
   8. ' ______' (0.000559)
   9. ' Hamburg' (0.000232)
  10. ' Bon' (0.000231)
  11. ' Munich' (0.000171)
  12. ' German' (0.000126)
  13. ' __________________' (0.000082)
  14. ' A' (0.000080)
  15. ' Ber' (0.000072)
  16. ' ____' (0.000070)
  17. ' ___' (0.000065)
  18. ' berlin' (0.000055)
  19. ' _____' (0.000055)
  20. ' London' (0.000049)

Layer 36 (after transformer block 35) (entropy: 2.020 bits):
   1. ' Berlin' (0.745596)
   2. ' The' (0.089245)
   3. ' Germany' (0.036124)
   4. ' ' (0.035922)
   5. ' __' (0.035451)
   6. ' 

' (0.011033)
   7. ' ______' (0.009584)
   8. ' A' (0.008251)
   9. ' (' (0.003598)
  10. ' What' (0.002906)
  11. ' Ber' (0.002744)
  12. ' This' (0.002706)
  13. ' Bon' (0.002565)
  14. ' ?

' (0.002480)
  15. ' __________________' (0.002283)
  16. ' Frankfurt' (0.002216)
  17. ' [' (0.002216)
  18. ' ?' (0.001902)
  19. ' I' (0.001733)
  20. ' ___' (0.001449)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 2.020 bits):
   1. ' Berlin' (0.719303)
   2. ' The' (0.086098)
   3. ' Germany' (0.034850)
   4. ' ' (0.034655)
   5. ' __' (0.034201)
   6. ' 

' (0.010644)
   7. ' ______' (0.009246)
   8. ' A' (0.007960)
   9. ' (' (0.003471)
  10. ' What' (0.002803)
  11. ' Ber' (0.002647)
  12. ' This' (0.002610)
  13. ' Bon' (0.002474)
  14. ' ?

' (0.002392)
  15. ' __________________' (0.002202)
  16. ' Frankfurt' (0.002138)
  17. ' [' (0.002138)
  18. ' ?' (0.001835)
  19. ' I' (0.001672)
  20. ' ___' (0.001398)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 2.989 bits)
   1. ' Berlin' (0.617024)
   2. ' not' (0.070311)
   3. ' a' (0.067692)
   4. ' in' (0.041279)
   5. ' located' (0.021944)
   6. ' the' (0.016110)
   7. ' __' (0.015827)
   8. ' called' (0.011862)
   9. ' ______' (0.008227)
  10. ' known' (0.007708)

Prompt: 'Berlin is the capital of'
  (entropy: 1.205 bits)
   1. ' Germany' (0.728606)
   2. ' which' (0.220746)
   3. ' the' (0.023739)
   4. ' what' (0.011356)
   5. ' __' (0.002298)
   6. ' ______' (0.001446)
   7. ' a' (0.001124)
   8. ' German' (0.000802)
   9. ' Berlin' (0.000459)
  10. '
' (0.000439)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 1.633 bits)
   1. ' Berlin' (0.721244)
   2. ' The' (0.174039)
   3. ' Also' (0.015090)
   4. '
' (0.012804)
   5. ' What' (0.009650)
   6. ' 

' (0.008670)
   7. ' ' (0.008418)
   8. ' Ber' (0.007864)
   9. ' Additionally' (0.004229)
  10. ' In' (0.003077)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000 bits)
   1. ' Berlin' (1.000000)
   2. ' The' (0.000000)
   3. ' Germany' (0.000000)
   4. ' ' (0.000000)
   5. ' __' (0.000000)
   6. ' 

' (0.000000)
   7. ' ______' (0.000000)
   8. ' A' (0.000000)
   9. ' (' (0.000000)
  10. ' What' (0.000000)
  11. ' Ber' (0.000000)
  12. ' This' (0.000000)
  13. ' Bon' (0.000000)
  14. ' ?

' (0.000000)
  15. ' __________________' (0.000000)

Temperature 2.0:
  (entropy: 13.141 bits)
   1. ' Berlin' (0.066316)
   2. ' The' (0.022944)
   3. ' Germany' (0.014597)
   4. ' ' (0.014556)
   5. ' __' (0.014461)
   6. ' 

' (0.008067)
   7. ' ______' (0.007519)
   8. ' A' (0.006976)
   9. ' (' (0.004607)
  10. ' What' (0.004140)
  11. ' Ber' (0.004023)
  12. ' This' (0.003995)
  13. ' Bon' (0.003889)
  14. ' ?

' (0.003824)
  15. ' __________________' (0.003669)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 36
Model dimension: 4096
Number of heads: 32
Vocab size: 151936
Context length: 2048
=== END OF MODEL STATS ========

