
============================================================
EVALUATING MODEL: mistralai/Mistral-7B-v0.1
============================================================
Loading model: mistralai/Mistral-7B-v0.1...
Loaded pretrained model mistralai/Mistral-7B-v0.1 into HookedTransformer

=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['<s>', 'Question', ':', 'What', 'is', 'the', 'capital', 'of', 'Germany', '?', 'Answer', ':']

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using NORMALIZED residual stream (LayerNorm applied - more accurate)
------------------------------------------------------------
Layer  0 (entropy: 10.205):
   1. 'laug' (0.000325)
   2. 'avax' (0.000324)
   3. 'auf' (0.000310)
   4. 'erre' (0.000297)
   5. 'voir' (0.000281)
   6. 'istribute' (0.000273)
   7. 'innen' (0.000261)
   8. 'ueto' (0.000250)
   9. 'aliment' (0.000239)
  10. 'onces' (0.000234)
  11. 'usqu' (0.000233)
  12. 'ettes' (0.000232)
  13. 'abi' (0.000229)
  14. 'ffen' (0.000228)
  15. 'holm' (0.000218)
  16. 'Sink' (0.000217)
  17. 'lette' (0.000210)
  18. 'EV' (0.000208)
  19. 'thous' (0.000206)
  20. 'serts' (0.000205)

Layer  1 (entropy: 9.738):
   1. 'ts' (0.004327)
   2. 'richt' (0.002631)
   3. 'reck' (0.002464)
   4. 'thoroughly' (0.002088)
   5. 'ffen' (0.001829)
   6. 'rig' (0.001711)
   7. 'rike' (0.001520)
   8. 'tered' (0.001179)
   9. 'unk' (0.001098)
  10. 'aja' (0.001020)
  11. 'bal' (0.000985)
  12. 'oya' (0.000859)
  13. 'ts' (0.000788)
  14. 'quet' (0.000755)
  15. 'battery' (0.000734)
  16. 'soci' (0.000706)
  17. 'reck' (0.000652)
  18. 'disagree' (0.000623)
  19. 'abs' (0.000612)
  20. 'Ts' (0.000610)

Layer  2 (entropy: 9.614):
   1. 'richt' (0.007685)
   2. 'aiser' (0.004172)
   3. 'rig' (0.003212)
   4. 'ts' (0.002713)
   5. 'Â' (0.002202)
   6. 'reck' (0.001975)
   7. 'reck' (0.001862)
   8. 'lo' (0.001834)
   9. 'thoroughly' (0.001827)
  10. 'ffen' (0.001759)
  11. 'amber' (0.001591)
  12. 'pel' (0.001378)
  13. 'nevertheless' (0.001180)
  14. 'reich' (0.001063)
  15. 'serts' (0.001044)
  16. 'ENDOR' (0.000936)
  17. '�' (0.000901)
  18. 'ilen' (0.000900)
  19. 'Griff' (0.000895)
  20. 'anne' (0.000891)

Layer  3 (entropy: 9.580):
   1. 'amber' (0.005354)
   2. 'aiser' (0.003269)
   3. 'thoroughly' (0.002342)
   4. 'situ' (0.001980)
   5. 'ople' (0.001955)
   6. 'ves' (0.001923)
   7. 'lo' (0.001863)
   8. 'reck' (0.001845)
   9. 'rig' (0.001770)
  10. 'Â' (0.001695)
  11. 'reck' (0.001449)
  12. 'bes' (0.001354)
  13. 'pronounced' (0.001310)
  14. 'illes' (0.001129)
  15. 'anne' (0.001102)
  16. 'ilen' (0.001092)
  17. 'richt' (0.001090)
  18. 'ner' (0.001086)
  19. 'ENDOR' (0.001018)
  20. 'Coord' (0.001002)

Layer  4 (entropy: 9.523):
   1. 'aiser' (0.004481)
   2. 'stadt' (0.003934)
   3. 'amber' (0.002301)
   4. 'arta' (0.001945)
   5. 'ople' (0.001850)
   6. 'rien' (0.001746)
   7. 'ady' (0.001736)
   8. 'ffen' (0.001660)
   9. 'ieden' (0.001556)
  10. 'ais' (0.001554)
  11. 'stract' (0.001547)
  12. 'illes' (0.001302)
  13. 'haus' (0.001258)
  14. 'cert' (0.001242)
  15. 'tober' (0.001226)
  16. 'smith' (0.001205)
  17. 'iani' (0.001204)
  18. 'avia' (0.001168)
  19. 'located' (0.001126)
  20. 'odes' (0.001126)

Layer  5 (entropy: 9.518):
   1. 'amber' (0.002847)
   2. 'adt' (0.002725)
   3. 'tober' (0.002666)
   4. 'stadt' (0.002126)
   5. 'lagen' (0.001952)
   6. 'Answer' (0.001928)
   7. 'aises' (0.001682)
   8. 'precisely' (0.001497)
   9. 'rais' (0.001421)
  10. 'answered' (0.001409)
  11. 'ais' (0.001390)
  12. 'Â' (0.001373)
  13. 'AIN' (0.001331)
  14. 'nes' (0.001319)
  15. 'rezent' (0.001244)
  16. 'swer' (0.001192)
  17. 'answer' (0.001172)
  18. 'odes' (0.001154)
  19. 'attan' (0.001143)
  20. 'ши' (0.001047)

Layer  6 (entropy: 9.452):
   1. 'nab' (0.005532)
   2. 'stadt' (0.003954)
   3. 'ь' (0.003733)
   4. 'adt' (0.003500)
   5. 'swer' (0.002809)
   6. 'amber' (0.002496)
   7. 'Â' (0.002139)
   8. 'trag' (0.001925)
   9. 'unden' (0.001923)
  10. 'tober' (0.001783)
  11. 'Kot' (0.001764)
  12. 'ei' (0.001699)
  13. 'aises' (0.001518)
  14. 'stract' (0.001416)
  15. 'alike' (0.001414)
  16. 'civilian' (0.001375)
  17. 'nan' (0.001356)
  18. 'werken' (0.001334)
  19. 'ppe' (0.001194)
  20. 'nes' (0.001184)

Layer  7 (entropy: 9.438):
   1. 'amber' (0.005769)
   2. 'answer' (0.004052)
   3. 'anel' (0.002799)
   4. 'spy' (0.002239)
   5. 'alike' (0.002192)
   6. 'Answer' (0.002188)
   7. 'ner' (0.002011)
   8. 'stract' (0.001840)
   9. 'bekan' (0.001796)
  10. 'кта' (0.001703)
  11. 'adt' (0.001638)
  12. 'avia' (0.001610)
  13. 'ei' (0.001568)
  14. 'Gott' (0.001563)
  15. 'NER' (0.001443)
  16. 'nab' (0.001439)
  17. 'esis' (0.001385)
  18. 'stadt' (0.001290)
  19. 'ungen' (0.001185)
  20. 'ias' (0.001139)

Layer  8 (entropy: 9.414):
   1. 'answer' (0.009707)
   2. 'Answer' (0.005419)
   3. 'answered' (0.003310)
   4. 'unden' (0.003144)
   5. 'stadt' (0.002635)
   6. 'NER' (0.001780)
   7. 'owi' (0.001695)
   8. 'LE' (0.001639)
   9. 'aval' (0.001601)
  10. 'twice' (0.001533)
  11. 'seh' (0.001512)
  12. 'tour' (0.001421)
  13. 'igd' (0.001347)
  14. 'answering' (0.001343)
  15. 'yes' (0.001325)
  16. 'aris' (0.001247)
  17. 'amber' (0.001138)
  18. 'ilon' (0.001106)
  19. 'adrat' (0.001069)
  20. 'ema' (0.001054)

Layer  9 (entropy: 9.234):
   1. 'answer' (0.017901)
   2. 'Answer' (0.016484)
   3. 'unden' (0.006404)
   4. '/******/' (0.005641)
   5. 'answer' (0.003243)
   6. 'answered' (0.003096)
   7. 'stadt' (0.003079)
   8. 'ora' (0.002864)
   9. 'Arthur' (0.002248)
  10. 'answers' (0.002074)
  11. '答' (0.001851)
  12. 'icz' (0.001821)
  13. 'eph' (0.001550)
  14. 'iesz' (0.001538)
  15. 'iels' (0.001536)
  16. 'swer' (0.001492)
  17. 'än' (0.001425)
  18. 'ót' (0.001381)
  19. 'lio' (0.001363)
  20. 'ilia' (0.001339)

Layer 10 (entropy: 9.343):
   1. 'Answer' (0.010752)
   2. 'answer' (0.007833)
   3. 'stadt' (0.005306)
   4. 'rium' (0.004399)
   5. 'indow' (0.002370)
   6. 'edes' (0.002315)
   7. 'ír' (0.002126)
   8. 'answer' (0.001996)
   9. 'swer' (0.001889)
  10. 'answered' (0.001678)
  11. 'inder' (0.001662)
  12. 'astr' (0.001589)
  13. 'ora' (0.001524)
  14. '/******/' (0.001494)
  15. 'woh' (0.001465)
  16. 'amber' (0.001319)
  17. 'Zwe' (0.001303)
  18. 'iba' (0.001273)
  19. 'над' (0.001272)
  20. 'Arthur' (0.001268)

Layer 11 (entropy: 9.217):
   1. '/******/' (0.040952)
   2. 'шта' (0.003601)
   3. 'ilor' (0.003339)
   4. 'swer' (0.003227)
   5. 'amber' (0.002421)
   6. 'answer' (0.002412)
   7. 'zw' (0.002403)
   8. '带' (0.002187)
   9. 'Answer' (0.002107)
  10. 'stadt' (0.002073)
  11. 'än' (0.002011)
  12. 'cius' (0.001869)
  13. 'ześ' (0.001862)
  14. 'ír' (0.001737)
  15. 'spieler' (0.001625)
  16. 'een' (0.001594)
  17. 'comerc' (0.001335)
  18. 'imal' (0.001333)
  19. 'uilder' (0.001309)
  20. 'anel' (0.001214)

Layer 12 (entropy: 9.466):
   1. 'ír' (0.004529)
   2. 'Answer' (0.003147)
   3. 'шта' (0.002923)
   4. 'htt' (0.002777)
   5. 'swer' (0.002653)
   6. 'answer' (0.002410)
   7. 'arden' (0.002345)
   8. 'zw' (0.002083)
   9. 'iger' (0.001754)
  10. 'шти' (0.001649)
  11. 'än' (0.001493)
  12. 'град' (0.001450)
  13. 'Films' (0.001339)
  14. 'égal' (0.001290)
  15. 'stadt' (0.001252)
  16. 'answered' (0.001170)
  17. 'ischer' (0.001114)
  18. 'plural' (0.001107)
  19. 'unden' (0.001083)
  20. 'dru' (0.001036)

Layer 13 (entropy: 9.474):
   1. 'Answer' (0.005442)
   2. 'ieden' (0.004011)
   3. 'amber' (0.003941)
   4. 'ír' (0.003641)
   5. 'swer' (0.002903)
   6. '........' (0.002812)
   7. 'anel' (0.001881)
   8. 'officially' (0.001852)
   9. 'usch' (0.001819)
  10. 'htt' (0.001800)
  11. 'iger' (0.001779)
  12. '................' (0.001618)
  13. '...' (0.001611)
  14. 'answer' (0.001567)
  15. '…' (0.001394)
  16. 'ът' (0.001275)
  17. 'ниш' (0.001194)
  18. 'orent' (0.001191)
  19. 'yes' (0.001190)
  20. '花' (0.001171)

Layer 14 (entropy: 9.395):
   1. 'Answer' (0.016466)
   2. 'answer' (0.004988)
   3. 'usch' (0.004593)
   4. '........' (0.004243)
   5. '……' (0.002991)
   6. 'ammen' (0.002505)
   7. 'officially' (0.002428)
   8. 'swer' (0.002275)
   9. 'amber' (0.002239)
  10. 'kommun' (0.002176)
  11. '…' (0.002056)
  12. 'iger' (0.002014)
  13. 'orent' (0.001997)
  14. '…' (0.001925)
  15. '...' (0.001847)
  16. 'zie' (0.001746)
  17. '….' (0.001595)
  18. 'htt' (0.001438)
  19. '....' (0.001431)
  20. 'ниш' (0.001425)

Layer 15 (entropy: 9.313):
   1. 'Answer' (0.015232)
   2. 'answer' (0.013613)
   3. 'cities' (0.004392)
   4. 'amber' (0.004177)
   5. 'answered' (0.003333)
   6. '........' (0.003154)
   7. 'WC' (0.003013)
   8. 'answer' (0.002389)
   9. 'swer' (0.001958)
  10. '……' (0.001897)
  11. 'Computer' (0.001864)
  12. 'officially' (0.001814)
  13. 'citizen' (0.001420)
  14. '答' (0.001388)
  15. 'Neither' (0.001376)
  16. '…' (0.001370)
  17. 'none' (0.001345)
  18. 'zie' (0.001312)
  19. 'ieden' (0.001302)
  20. 'kommun' (0.001287)

Layer 16 (entropy: 8.956):
   1. 'Answer' (0.032301)
   2. 'answer' (0.026451)
   3. 'cities' (0.011544)
   4. 'swer' (0.008133)
   5. 'answered' (0.005757)
   6. 'answer' (0.005586)
   7. '答' (0.004393)
   8. 'answ' (0.003136)
   9. 'ieden' (0.003040)
  10. 'qpoint' (0.002351)
  11. 'opyright' (0.002170)
  12. 'unden' (0.002093)
  13. 'ště' (0.002091)
  14. 'iem' (0.002083)
  15. 'orent' (0.001979)
  16. 'ammen' (0.001756)
  17. 'answering' (0.001613)
  18. 'tober' (0.001599)
  19. '........' (0.001545)
  20. 'esen' (0.001528)

Layer 17 (entropy: 9.164):
   1. 'cities' (0.020541)
   2. 'Answer' (0.017188)
   3. 'answer' (0.014243)
   4. 'unden' (0.005726)
   5. 'headquarters' (0.005409)
   6. 'swer' (0.003822)
   7. 'answer' (0.003264)
   8. 'towns' (0.003097)
   9. 'Capital' (0.002982)
  10. '…' (0.002179)
  11. '答' (0.002126)
  12. '……' (0.002041)
  13. 'None' (0.001931)
  14. 'officially' (0.001835)
  15. 'capital' (0.001813)
  16. 'answered' (0.001458)
  17. 'esch' (0.001442)
  18. 'città' (0.001410)
  19. 'Probably' (0.001248)
  20. 'city' (0.001211)

Layer 18 (entropy: 8.828):
   1. 'cities' (0.035738)
   2. 'Germany' (0.017594)
   3. 'Answer' (0.016788)
   4. 'answer' (0.013888)
   5. 'headquarters' (0.007187)
   6. 'Berlin' (0.006300)
   7. 'swer' (0.006043)
   8. 'Frankfurt' (0.005608)
   9. 'answer' (0.003538)
  10. 'towns' (0.003528)
  11. 'opyright' (0.003271)
  12. 'esch' (0.003261)
  13. 'Deutschland' (0.003048)
  14. 'Capital' (0.003045)
  15. '…' (0.002801)
  16. '➤' (0.002661)
  17. 'capital' (0.002493)
  18. '……' (0.002097)
  19. 'answered' (0.002087)
  20. 'Neither' (0.001710)

Layer 19 (entropy: 7.922):
   1. 'cities' (0.077311)
   2. 'capital' (0.050748)
   3. 'Germany' (0.045359)
   4. 'Capital' (0.026676)
   5. 'Berlin' (0.014162)
   6. 'answer' (0.009181)
   7. 'capit' (0.006604)
   8. 'city' (0.004405)
   9. 'Answer' (0.004391)
  10. '/******/' (0.004109)
  11. 'Deutschland' (0.003884)
  12. 'Frankfurt' (0.003540)
  13. 'parliament' (0.003435)
  14. 'Parliament' (0.003336)
  15. 'headquarters' (0.003135)
  16. '…' (0.002928)
  17. 'towns' (0.002774)
  18. 'swer' (0.002751)
  19. 'opyright' (0.002470)
  20. 'Washington' (0.002296)

Layer 20 (entropy: 5.881):
   1. 'capital' (0.167747)
   2. 'Berlin' (0.102244)
   3. 'Capital' (0.102084)
   4. 'Germany' (0.074323)
   5. 'cities' (0.039926)
   6. 'capit' (0.017211)
   7. 'answer' (0.008212)
   8. 'Deutschland' (0.005275)
   9. 'Frankfurt' (0.004651)
  10. 'Parliament' (0.004441)
  11. 'Washington' (0.004028)
  12. 'parliament' (0.003937)
  13. 'Answer' (0.003506)
  14. 'Paris' (0.003026)
  15. 'city' (0.002494)
  16. 'Government' (0.002335)
  17. 'Tokyo' (0.002260)
  18. 'politicians' (0.002035)
  19. 'Leip' (0.002004)
  20. 'Probably' (0.001952)

Layer 21 (entropy: 4.550):
   1. 'Berlin' (0.368305)
   2. 'Germany' (0.168211)
   3. 'capital' (0.047416)
   4. 'Capital' (0.030147)
   5. 'Frankfurt' (0.012872)
   6. 'cities' (0.008864)
   7. 'capit' (0.007167)
   8. 'Paris' (0.005471)
   9. 'Washington' (0.005390)
  10. 'Deutschland' (0.004038)
  11. 'answer' (0.003757)
  12. 'parliament' (0.002344)
  13. 'Parliament' (0.002341)
  14. 'German' (0.002001)
  15. 'London' (0.001889)
  16. 'Tokyo' (0.001867)
  17. 'Answer' (0.001825)
  18. 'Leip' (0.001727)
  19. 'None' (0.001668)
  20. 'technically' (0.001509)

Layer 22 (entropy: 2.190):
   1. 'Washington' (0.466048)
   2. 'Berlin' (0.342019)
   3. 'capital' (0.028395)
   4. 'Germany' (0.022755)
   5. 'Capital' (0.019955)
   6. 'capit' (0.005717)
   7. 'Frankfurt' (0.004031)
   8. 'washing' (0.003260)
   9. 'Paris' (0.002931)
  10. 'Beijing' (0.002573)
  11. 'Rome' (0.001741)
  12. 'cities' (0.001704)
  13. 'London' (0.001701)
  14. 'Moscow' (0.001620)
  15. 'ashington' (0.001152)
  16. 'DC' (0.001011)
  17. 'parliament' (0.000986)
  18. 'Tokyo' (0.000971)
  19. 'DC' (0.000942)
  20. 'Parliament' (0.000868)

Layer 23 (entropy: 1.998):
   1. 'Berlin' (0.581382)
   2. 'Washington' (0.189352)
   3. 'Germany' (0.111897)
   4. 'capital' (0.010902)
   5. 'Frankfurt' (0.006472)
   6. 'Capital' (0.004489)
   7. 'washing' (0.002124)
   8. 'German' (0.002005)
   9. 'capit' (0.001953)
  10. 'Paris' (0.001923)
  11. 'Deutschland' (0.001547)
  12. 'Beijing' (0.001351)
  13. 'Germans' (0.001073)
  14. 'cities' (0.000992)
  15. 'Ber' (0.000968)
  16. 'Moscow' (0.000839)
  17. 'Rome' (0.000783)
  18. 'Tokyo' (0.000667)
  19. 'Parliament' (0.000642)
  20. 'Jerusalem' (0.000634)

Layer 24 (entropy: 0.387):
   1. 'Berlin' (0.917514)
   2. 'Germany' (0.066525)
   3. 'capital' (0.004115)
   4. 'Frankfurt' (0.001882)
   5. 'German' (0.001705)
   6. 'Washington' (0.001395)
   7. 'Capital' (0.001314)
   8. 'Deutschland' (0.000577)
   9. 'Germans' (0.000566)
  10. 'capit' (0.000428)
  11. 'Ber' (0.000235)
  12. 'Paris' (0.000164)
  13. 'germ' (0.000100)
  14. 'Tokyo' (0.000095)
  15. 'Leip' (0.000090)
  16. 'Germ' (0.000075)
  17. 'Rome' (0.000070)
  18. 'Hamburg' (0.000051)
  19. 'cities' (0.000051)
  20. 'Moscow' (0.000048)

Layer 25 (entropy: 0.195):
   1. 'Berlin' (0.967519)
   2. 'Germany' (0.024786)
   3. 'Frankfurt' (0.001439)
   4. 'capital' (0.001130)
   5. 'German' (0.000975)
   6. 'Washington' (0.000386)
   7. 'Germans' (0.000342)
   8. 'Capital' (0.000330)
   9. 'Ber' (0.000271)
  10. 'Deutschland' (0.000232)
  11. 'Paris' (0.000206)
  12. 'Tokyo' (0.000079)
  13. 'Leip' (0.000064)
  14. 'germ' (0.000052)
  15. 'capit' (0.000050)
  16. 'Hamburg' (0.000049)
  17. 'Beijing' (0.000039)
  18. 'Germ' (0.000029)
  19. 'cities' (0.000029)
  20. 'Moscow' (0.000022)

Layer 26 (entropy: 0.072):
   1. 'Berlin' (0.990398)
   2. 'Germany' (0.006885)
   3. 'Ber' (0.000657)
   4. 'Frankfurt' (0.000633)
   5. 'German' (0.000189)
   6. 'capital' (0.000094)
   7. 'Deutschland' (0.000084)
   8. 'Germans' (0.000055)
   9. 'Paris' (0.000035)
  10. 'Washington' (0.000033)
  11. 'Capital' (0.000026)
  12. 'ber' (0.000026)
  13. 'Ber' (0.000025)
  14. 'Tokyo' (0.000023)
  15. 'Hamburg' (0.000018)
  16. 'germ' (0.000016)
  17. 'Leip' (0.000015)
  18. 'Бер' (0.000011)
  19. 'BER' (0.000011)
  20. 'cities' (0.000011)

Layer 27 (entropy: 0.094):
   1. 'Berlin' (0.985632)
   2. 'Ber' (0.009986)
   3. 'Ber' (0.001597)
   4. 'ber' (0.001278)
   5. 'Germany' (0.000774)
   6. 'Бер' (0.000109)
   7. 'Frankfurt' (0.000102)
   8. 'BER' (0.000087)
   9. 'ber' (0.000030)
  10. 'German' (0.000028)
  11. 'capital' (0.000021)
  12. 'Deutschland' (0.000015)
  13. 'Germans' (0.000008)
  14. 'Capital' (0.000006)
  15. 'Washington' (0.000005)
  16. 'Bern' (0.000005)
  17. 'Tokyo' (0.000005)
  18. 'Paris' (0.000004)
  19. 'It' (0.000003)
  20. 'There' (0.000003)

Layer 28 (entropy: 0.085):
   1. 'Berlin' (0.988149)
   2. 'Ber' (0.007043)
   3. 'Ber' (0.001776)
   4. 'ber' (0.001030)
   5. 'Germany' (0.000900)
   6. 'Frankfurt' (0.000116)
   7. 'German' (0.000070)
   8. 'It' (0.000054)
   9. 'There' (0.000048)
  10. 'BER' (0.000045)
  11. 'Бер' (0.000043)
  12. 'Bon' (0.000036)
  13. 'ber' (0.000033)
  14. 'capital' (0.000031)
  15. 'The' (0.000027)
  16. 'it' (0.000012)
  17. 'well' (0.000010)
  18. 'there' (0.000010)
  19. 'Its' (0.000008)
  20. 'That' (0.000007)

Layer 29 (entropy: 0.244):
   1. 'Berlin' (0.972070)
   2. 'Germany' (0.007060)
   3. 'Ber' (0.004027)
   4. 'It' (0.001620)
   5. 'Ber' (0.001583)
   6. 'The' (0.001574)
   7. 'There' (0.001114)
   8. 'Frankfurt' (0.000463)
   9. 'German' (0.000447)
  10. 'ber' (0.000390)
  11. 'Bon' (0.000260)
  12. 'That' (0.000259)
  13. 'This' (0.000219)
  14. 'it' (0.000180)
  15. 'If' (0.000171)
  16. '(' (0.000164)
  17. 'What' (0.000151)
  18. 'well' (0.000146)
  19. 'there' (0.000137)
  20. '
' (0.000129)

Layer 30 (entropy: 0.548):
   1. 'Berlin' (0.931575)
   2. 'Germany' (0.017204)
   3. 'The' (0.007552)
   4. 'It' (0.005407)
   5. 'There' (0.004139)
   6. 'That' (0.001252)
   7. 'Bon' (0.001178)
   8. 'Ber' (0.001081)
   9. 'This' (0.000809)
  10. 'None' (0.000711)
  11. 'If' (0.000606)
  12. 'What' (0.000553)
  13. 'In' (0.000476)
  14. '' (0.000459)
  15. 'Frankfurt' (0.000456)
  16. 'No' (0.000439)
  17. 'According' (0.000437)
  18. 'German' (0.000415)
  19. 'Well' (0.000412)
  20. '(' (0.000396)

Layer 31 (entropy: 1.248):
   1. 'Berlin' (0.831504)
   2. 'The' (0.021823)
   3. 'Germany' (0.019521)
   4. 'It' (0.015328)
   5. 'There' (0.008855)
   6. 'That' (0.004082)
   7. 'Bon' (0.003260)
   8. '
' (0.002636)
   9. 'Frankfurt' (0.002592)
  10. 'I' (0.002567)
  11. '' (0.002318)
  12. 'Well' (0.002221)
  13. 'B' (0.002221)
  14. 'This' (0.002129)
  15. 'What' (0.002063)
  16. 'A' (0.001740)
  17. 'If' (0.001733)
  18. 'No' (0.001714)
  19. 'You' (0.001684)
  20. '(' (0.001621)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 1.248):
   1. 'Berlin' (0.831503)
   2. 'The' (0.021823)
   3. 'Germany' (0.019521)
   4. 'It' (0.015328)
   5. 'There' (0.008855)
   6. 'That' (0.004082)
   7. 'Bon' (0.003260)
   8. '
' (0.002636)
   9. 'Frankfurt' (0.002593)
  10. 'I' (0.002567)
  11. '' (0.002318)
  12. 'Well' (0.002221)
  13. 'B' (0.002221)
  14. 'This' (0.002129)
  15. 'What' (0.002063)
  16. 'A' (0.001740)
  17. 'If' (0.001733)
  18. 'No' (0.001714)
  19. 'You' (0.001684)
  20. '(' (0.001621)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 4.628)
   1. 'a' (0.187828)
   2. 'one' (0.089722)
   3. 'the' (0.051248)
   4. 'known' (0.042724)
   5. 'home' (0.039109)
   6. 'an' (0.028183)
   7. 'not' (0.021237)
   8. 'famous' (0.021131)
   9. 'full' (0.018002)
  10. 'set' (0.010509)

Prompt: 'Berlin is the capital of'
  (entropy: 0.659)
   1. 'Germany' (0.896600)
   2. 'the' (0.053936)
   3. 'both' (0.004360)
   4. 'a' (0.003802)
   5. 'Europe' (0.003114)
   6. 'Berlin' (0.002841)
   7. 'German' (0.002442)
   8. 'modern' (0.001617)
   9. 'and' (0.001473)
  10. 'one' (0.001109)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 3.162)
   1. '
' (0.453103)
   2. 'Berlin' (0.143815)
   3. 'If' (0.027227)
   4. 'The' (0.023025)
   5. 'You' (0.014681)
   6. '(' (0.014457)
   7. '' (0.014369)
   8. 'What' (0.011041)
   9. 'Bon' (0.010859)
  10. 'I' (0.009624)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000)
   1. 'Berlin' (1.000000)
   2. 'The' (0.000000)
   3. 'Germany' (0.000000)
   4. 'It' (0.000000)
   5. 'There' (0.000000)
   6. 'That' (0.000000)
   7. 'Bon' (0.000000)
   8. '
' (0.000000)
   9. 'Frankfurt' (0.000000)
  10. 'I' (0.000000)
  11. '' (0.000000)
  12. 'Well' (0.000000)
  13. 'B' (0.000000)
  14. 'This' (0.000000)
  15. 'What' (0.000000)

Temperature 2.0:
  (entropy: 8.422)
   1. 'Berlin' (0.058676)
   2. 'The' (0.009506)
   3. 'Germany' (0.008990)
   4. 'It' (0.007967)
   5. 'There' (0.006055)
   6. 'That' (0.004111)
   7. 'Bon' (0.003674)
   8. '
' (0.003304)
   9. 'Frankfurt' (0.003276)
  10. 'I' (0.003260)
  11. '' (0.003098)
  12. 'Well' (0.003033)
  13. 'B' (0.003032)
  14. 'This' (0.002969)
  15. 'What' (0.002922)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 32
Model dimension: 4096
Number of heads: 32
Vocab size: 32000
Context length: 2048
=== END OF MODEL STATS ========

