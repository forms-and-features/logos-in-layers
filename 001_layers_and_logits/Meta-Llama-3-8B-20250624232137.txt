
============================================================
EVALUATING MODEL: meta-llama/Meta-Llama-3-8B
============================================================
Loading model: meta-llama/Meta-Llama-3-8B...
Loaded pretrained model meta-llama/Meta-Llama-3-8B into HookedTransformer

=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['<|begin_of_text|>', 'Question', ':', ' What', ' is', ' the', ' capital', ' of', ' Germany', '?', ' Answer', ':']

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using NORMALIZED residual stream (LayerNorm applied - more accurate)
------------------------------------------------------------
Layer  0 (entropy: 10.065):
   1. 'oren' (0.007558)
   2. 'nton' (0.004973)
   3. '977' (0.003949)
   4. 'aland' (0.003312)
   5. '賀' (0.002306)
   6. 'ceso' (0.002216)
   7. 'adil' (0.001891)
   8. 'nesc' (0.001855)
   9. 'anson' (0.001835)
  10. 'anter' (0.001782)
  11. ' Pun' (0.001769)
  12. 'ění' (0.001606)
  13. 'Ｌ' (0.001475)
  14. ' arası' (0.001439)
  15. 'ophon' (0.001412)
  16. ' Pension' (0.001320)
  17. 'SetName' (0.001310)
  18. ' edin' (0.001262)
  19. 'writeln' (0.001248)
  20. 'undler' (0.001227)

Layer  1 (entropy: 9.438):
   1. ' ​​' (0.008946)
   2. 'aten' (0.008164)
   3. 'atten' (0.006811)
   4. '691' (0.004755)
   5. 'seys' (0.004283)
   6. 'eden' (0.003932)
   7. 'bai' (0.003852)
   8. 'dna' (0.003656)
   9. 'ルド' (0.003601)
  10. '851' (0.003328)
  11. 'bette' (0.003083)
  12. 'building' (0.002635)
  13. '-letter' (0.002457)
  14. ' klu' (0.002431)
  15. 'рів' (0.002393)
  16. 'ilt' (0.002285)
  17. 'aniem' (0.002264)
  18. 'dsl' (0.002135)
  19. 'ActiveSheet' (0.002115)
  20. ' Opr' (0.002046)

Layer  2 (entropy: 9.387):
   1. 'atten' (0.015662)
   2. ' Ven' (0.012849)
   3. 'เ' (0.011525)
   4. 'BA' (0.007169)
   5. '.Decode' (0.003926)
   6. 'orz' (0.003630)
   7. '536' (0.003463)
   8. '﻿#' (0.003368)
   9. 'imas' (0.003340)
  10. '施' (0.003190)
  11. 'ird' (0.002894)
  12. 'aska' (0.002704)
  13. '.WinForms' (0.002642)
  14. '.stride' (0.002446)
  15. 'ba' (0.002404)
  16. 'Ven' (0.002404)
  17. '-letter' (0.002295)
  18. 'rud' (0.002159)
  19. ' PIE' (0.002154)
  20. 'erah' (0.002083)

Layer  3 (entropy: 9.063):
   1. 'adal' (0.020163)
   2. 'chine' (0.010344)
   3. 'ADDE' (0.009571)
   4. 'Atoms' (0.007671)
   5. '�' (0.007367)
   6. '.Decode' (0.005969)
   7. 'heimer' (0.005940)
   8. '.Areas' (0.005869)
   9. 'ソン' (0.003952)
  10. 'bai' (0.003886)
  11. 'rana' (0.003773)
  12. ' ​​' (0.003720)
  13. ' Podesta' (0.003622)
  14. ' Coach' (0.003538)
  15. 'Capabilities' (0.003524)
  16. 'こんにちは' (0.003513)
  17. 'reator' (0.003339)
  18. '/Users' (0.003074)
  19. '/operator' (0.002895)
  20. '/Area' (0.002886)

Layer  4 (entropy: 9.235):
   1. 'chine' (0.021717)
   2. 'olg' (0.006496)
   3. 'XC' (0.005762)
   4. 'istrovství' (0.005636)
   5. 'IZER' (0.005179)
   6. ' Obr' (0.004474)
   7. 'เ' (0.003956)
   8. 'ekler' (0.003364)
   9. '�' (0.003167)
  10. 'RIPT' (0.003017)
  11. 'ADDE' (0.003014)
  12. 'cheng' (0.002909)
  13. 'adal' (0.002883)
  14. '.Generated' (0.002844)
  15. 'reator' (0.002324)
  16. ' Sov' (0.002307)
  17. 'STRU' (0.002298)
  18. ' bearer' (0.002285)
  19. 'OVE' (0.002277)
  20. 'еди' (0.002276)

Layer  5 (entropy: 9.207):
   1. '.Decode' (0.009012)
   2. 'aget' (0.006978)
   3. 'LANG' (0.005642)
   4. 'urname' (0.005485)
   5. 'IDO' (0.004980)
   6. '�' (0.004587)
   7. 'multipart' (0.004577)
   8. 'defgroup' (0.004493)
   9. ' Citizens' (0.004066)
  10. ''gc' (0.003867)
  11. 'остат' (0.003849)
  12. 'STREAM' (0.003569)
  13. '��' (0.003477)
  14. 'OnError' (0.003191)
  15. '_OBS' (0.003124)
  16. 'ulaire' (0.003033)
  17. 'reator' (0.002674)
  18. 'adal' (0.002595)
  19. ' multipart' (0.002592)
  20. ' nonatomic' (0.002577)

Layer  6 (entropy: 9.013):
   1. ' Cem' (0.009870)
   2. ' Citizens' (0.009068)
   3. 'итор' (0.006600)
   4. 'raž' (0.006183)
   5. 'adal' (0.005328)
   6. 'ToObject' (0.004800)
   7. 'ilip' (0.004670)
   8. 'MimeType' (0.004625)
   9. 'mpr' (0.004599)
  10. 'upd' (0.004154)
  11. 'strap' (0.004132)
  12. 'чики' (0.004090)
  13. 'urname' (0.004028)
  14. 'MPI' (0.003686)
  15. 'OnError' (0.003461)
  16. '/Area' (0.003452)
  17. '-Origin' (0.003438)
  18. ' Civ' (0.003233)
  19. 'มน' (0.003039)
  20. 'RuntimeObject' (0.002932)

Layer  7 (entropy: 9.101):
   1. 'AutoSize' (0.007241)
   2. ' RoundedRectangle' (0.006645)
   3. 'ilt' (0.005018)
   4. 'enville' (0.004562)
   5. '963' (0.004279)
   6. 'xFFF' (0.004087)
   7. 'strap' (0.004007)
   8. 'PCM' (0.003894)
   9. 'ANGED' (0.003623)
  10. 'setFlash' (0.003568)
  11. 'こんにちは' (0.003408)
  12. 'کان' (0.003340)
  13. 'LTR' (0.003336)
  14. '.GetById' (0.003305)
  15. 'ermo' (0.003299)
  16. '�' (0.003296)
  17. 'imeo' (0.003096)
  18. '/Area' (0.003054)
  19. 'wie' (0.003033)
  20. '。。

' (0.002993)

Layer  8 (entropy: 8.478):
   1. 'enville' (0.037452)
   2. '.onView' (0.016092)
   3. '.netbeans' (0.014814)
   4. 'こんにちは' (0.013229)
   5. 'iversit' (0.012472)
   6. '.xtext' (0.011406)
   7. 'PCM' (0.011342)
   8. 'شی' (0.006745)
   9. '.owl' (0.006665)
  10. 'bcm' (0.006528)
  11. 'ilir' (0.006352)
  12. 'ylland' (0.006252)
  13. 'tracer' (0.005870)
  14. '/Area' (0.005464)
  15. 'entar' (0.005284)
  16. 'VML' (0.004608)
  17. '감' (0.004287)
  18. 'ilan' (0.003378)
  19. 'stral' (0.003360)
  20. 'ینه' (0.003261)

Layer  9 (entropy: 8.720):
   1. 'PCM' (0.049532)
   2. 'iversit' (0.029671)
   3. '�' (0.007541)
   4. '.xtext' (0.006996)
   5. 'erialize' (0.006943)
   6. '孝' (0.006405)
   7. 'vanished' (0.006357)
   8. 'ilt' (0.005643)
   9. 'entes' (0.005085)
  10. 'ystore' (0.004799)
  11. 'olang' (0.004118)
  12. ' Civ' (0.003742)
  13. 'ilir' (0.003331)
  14. 'SPATH' (0.003319)
  15. 'rored' (0.003203)
  16. 'velt' (0.003157)
  17. 'цип' (0.003109)
  18. 'หาย' (0.002933)
  19. 'UILTIN' (0.002918)
  20. '_stdio' (0.002914)

Layer 10 (entropy: 9.072):
   1. 'ystack' (0.020013)
   2. 'PCM' (0.013980)
   3. 'Раз' (0.007219)
   4. ' Woody' (0.006797)
   5. 'iversit' (0.006082)
   6. 'ilir' (0.005491)
   7. 'หาย' (0.005046)
   8. '.xtext' (0.004756)
   9. 'SPATH' (0.004635)
  10. 'ustum' (0.004620)
  11. ' Multiply' (0.004400)
  12. 'рел' (0.004230)
  13. 'ystore' (0.004141)
  14. 'icies' (0.003930)
  15. ' Parenthood' (0.003663)
  16. 'oví' (0.003061)
  17. 'PCP' (0.002949)
  18. 'dik' (0.002865)
  19. '..



' (0.002783)
  20. 'LTR' (0.002683)

Layer 11 (entropy: 8.588):
   1. '.xtext' (0.058524)
   2. 'рел' (0.016929)
   3. 'ystack' (0.011004)
   4. 'PCM' (0.009923)
   5. 'FRING' (0.009507)
   6. 'urement' (0.007521)
   7. 'acific' (0.006216)
   8. '929' (0.006147)
   9. 'rete' (0.005903)
  10. '/Area' (0.004844)
  11. '곤' (0.004781)
  12. 'Advertisements' (0.004726)
  13. 'ormsg' (0.004401)
  14. '_mex' (0.004250)
  15. '539' (0.003864)
  16. ' PartialView' (0.003752)
  17. 'óst' (0.003743)
  18. 'ackbar' (0.003614)
  19. ' geschichten' (0.003461)
  20. '野' (0.003348)

Layer 12 (entropy: 9.261):
   1. 'краї' (0.012105)
   2. ' def' (0.009023)
   3. '_EDEFAULT' (0.007494)
   4. '.xtext' (0.006936)
   5. '@Id' (0.006927)
   6. 'PCM' (0.005681)
   7. '_mex' (0.005311)
   8. 'Архів' (0.004237)
   9. 'erializer' (0.003995)
  10. ' PartialView' (0.003882)
  11. ' Haupt' (0.003744)
  12. ' b' (0.003612)
  13. ' Greenwood' (0.003417)
  14. 'rophy' (0.003162)
  15. '_sid' (0.002943)
  16. '�' (0.002860)
  17. ' reins' (0.002699)
  18. 'celik' (0.002653)
  19. 'edar' (0.002539)
  20. 'remium' (0.002385)

Layer 13 (entropy: 9.254):
   1. '#ab' (0.010626)
   2. 'Архів' (0.009167)
   3. 'erece' (0.007331)
   4. '�回' (0.006847)
   5. '_mex' (0.006638)
   6. 'enderit' (0.006319)
   7. 'argin' (0.005876)
   8. 'ritel' (0.005539)
   9. 'zcze' (0.004773)
  10. 'hek' (0.003825)
  11. ' "}\' (0.003175)
  12. 'INLINE' (0.003160)
  13. 'šak' (0.002890)
  14. ' cố' (0.002742)
  15. ' Mes' (0.002604)
  16. '#ac' (0.002495)
  17. '_EDEFAULT' (0.002486)
  18. '.xtext' (0.002474)
  19. 'เฉ' (0.002460)
  20. 'umin' (0.002446)

Layer 14 (entropy: 8.881):
   1. '#af' (0.013361)
   2. ')frame' (0.012977)
   3. '#ab' (0.011292)
   4. 'enderit' (0.010497)
   5. '.datatables' (0.009352)
   6. ' dõi' (0.009284)
   7. '.utf' (0.009027)
   8. '#ad' (0.008273)
   9. 'ateg' (0.007968)
  10. 'undler' (0.007573)
  11. 'ediği' (0.005579)
  12. 'ritel' (0.005404)
  13. 'enderror' (0.004645)
  14. '#ac' (0.004463)
  15. '재' (0.004184)
  16. 'SPATH' (0.004156)
  17. 'üstü' (0.003924)
  18. 'Sdk' (0.003812)
  19. 'rowsable' (0.003637)
  20. '#index' (0.003538)

Layer 15 (entropy: 9.034):
   1. '#ab' (0.017635)
   2. 'undler' (0.011290)
   3. 'enderror' (0.009313)
   4. '_EDEFAULT' (0.007946)
   5. '.ecore' (0.007507)
   6. 'شف' (0.007249)
   7. 'umin' (0.007129)
   8. 'upo' (0.006585)
   9. '#ad' (0.006446)
  10. '.LoggerFactory' (0.006140)
  11. 'ほ' (0.005065)
  12. 'Cİ' (0.004518)
  13. 'ชอบ' (0.003812)
  14. 'erosis' (0.003747)
  15. '_mA' (0.003515)
  16. 'idal' (0.003486)
  17. '_mB' (0.003452)
  18. 'ootball' (0.003415)
  19. 'eldo' (0.003379)
  20. 'PREFIX' (0.003163)

Layer 16 (entropy: 8.284):
   1. '#ad' (0.052507)
   2. ')application' (0.032176)
   3. '#ab' (0.027821)
   4. 'oplayer' (0.024633)
   5. '/******/' (0.016677)
   6. 'ชอบ' (0.013288)
   7. 'ateg' (0.010305)
   8. '#af' (0.007657)
   9. 'ABCDEFGHIJKLMNOP' (0.007239)
  10. 'undler' (0.007040)
  11. ' thủ' (0.005236)
  12. 'ktop' (0.004901)
  13. 'Insets' (0.004842)
  14. 'imas' (0.004495)
  15. 'SendMessage' (0.004320)
  16. 'sched' (0.004053)
  17. 'cratch' (0.003740)
  18. 'ependency' (0.003687)
  19. 'etros' (0.003472)
  20. '#error' (0.003334)

Layer 17 (entropy: 8.651):
   1. '#ad' (0.045787)
   2. ')application' (0.024743)
   3. 'oplayer' (0.023197)
   4. 'ABCDEFGHIJKLMNOP' (0.016668)
   5. 'imas' (0.012502)
   6. '#ab' (0.008399)
   7. ' capital' (0.007596)
   8. 'hoot' (0.007420)
   9. 'wig' (0.006356)
  10. 'zd' (0.005488)
  11. ' Capitals' (0.004648)
  12. 'sched' (0.004439)
  13. 'ชอบ' (0.004255)
  14. 'NullException' (0.003738)
  15. 'undler' (0.003728)
  16. 'etros' (0.003336)
  17. 'undi' (0.003289)
  18. ' none' (0.003286)
  19. 'SendMessage' (0.003046)
  20. ' capitals' (0.003038)

Layer 18 (entropy: 8.145):
   1. 'ABCDEFGHIJKLMNOP' (0.095169)
   2. ')application' (0.067332)
   3. ' Capitals' (0.013836)
   4. 'oplayer' (0.011568)
   5. ' capital' (0.009620)
   6. 'ittings' (0.009420)
   7. ' Capital' (0.009260)
   8. ' none' (0.007409)
   9. ' (::' (0.006632)
  10. 'sched' (0.006305)
  11. 'imas' (0.005400)
  12. 'ZF' (0.004933)
  13. 'undi' (0.004353)
  14. 'hoot' (0.004131)
  15. 'AGED' (0.004041)
  16. '#ad' (0.003562)
  17. ' capitals' (0.002763)
  18. 'hed' (0.002676)
  19. 'urm' (0.002651)
  20. '.createSequentialGroup' (0.002621)

Layer 19 (entropy: 7.465):
   1. ' Capital' (0.076821)
   2. 'ABCDEFGHIJKLMNOP' (0.062594)
   3. 'urm' (0.055559)
   4. ' Capitals' (0.043475)
   5. ' capital' (0.029472)
   6. ')application' (0.025351)
   7. ' Washington' (0.020550)
   8. ' capitals' (0.015781)
   9. 'ashington' (0.007242)
  10. ' London' (0.006392)
  11. 'sched' (0.005346)
  12. '.openConnection' (0.005015)
  13. 'ittings' (0.004352)
  14. 'Capital' (0.003924)
  15. '-NLS' (0.003757)
  16. '#ad' (0.003648)
  17. 'Washington' (0.003486)
  18. ' Federal' (0.003455)
  19. ' Rome' (0.003393)
  20. ' federally' (0.002922)

Layer 20 (entropy: 6.437):
   1. ' capital' (0.169168)
   2. ' Capital' (0.101414)
   3. ' Capitals' (0.085030)
   4. ' capitals' (0.043631)
   5. 'urm' (0.014831)
   6. ')application' (0.011797)
   7. ' Washington' (0.010934)
   8. 'Capital' (0.009128)
   9. 'ABCDEFGHIJKLMNOP' (0.007733)
  10. ' CAPITAL' (0.007709)
  11. ' Scha' (0.005684)
  12. ' Berlin' (0.005057)
  13. ' London' (0.004480)
  14. '.openConnection' (0.003791)
  15. 'capital' (0.003146)
  16. 'Washington' (0.002954)
  17. ' Erotische' (0.002825)
  18. 'urum' (0.002743)
  19. '-NLS' (0.002607)
  20. 'imas' (0.002413)

Layer 21 (entropy: 5.876):
   1. ' Berlin' (0.230554)
   2. ' capital' (0.091921)
   3. ' Capitals' (0.080589)
   4. ' Capital' (0.043245)
   5. ' Washington' (0.037780)
   6. ' Federal' (0.019354)
   7. ' London' (0.008298)
   8. 'Capital' (0.006749)
   9. ' capitals' (0.006106)
  10. ')application' (0.005472)
  11. 'Washington' (0.004964)
  12. ' federal' (0.004482)
  13. ' federally' (0.004240)
  14. 'Federal' (0.003938)
  15. ' none' (0.003749)
  16. 'Berlin' (0.003731)
  17. 'urm' (0.003506)
  18. 'ashington' (0.002887)
  19. ' proficient' (0.002825)
  20. ' CAPITAL' (0.002806)

Layer 22 (entropy: 3.869):
   1. ' Berlin' (0.513955)
   2. ' Capitals' (0.061712)
   3. ' capital' (0.051956)
   4. ' Capital' (0.044759)
   5. ' Washington' (0.021415)
   6. 'Berlin' (0.012492)
   7. ' capitals' (0.007353)
   8. ' London' (0.007255)
   9. 'Capital' (0.007106)
  10. ' CAPITAL' (0.004240)
  11. 'Washington' (0.003454)
  12. ' federally' (0.003429)
  13. 'urm' (0.002899)
  14. '.openConnection' (0.002739)
  15. 'ashington' (0.002640)
  16. ' Federal' (0.002028)
  17. ' none' (0.001902)
  18. ' reun' (0.001859)
  19. ' Rome' (0.001818)
  20. ')application' (0.001772)

Layer 23 (entropy: 1.434):
   1. ' Berlin' (0.808346)
   2. ' Washington' (0.041062)
   3. ' Capitals' (0.023627)
   4. ' capital' (0.018975)
   5. ' Capital' (0.018369)
   6. 'Berlin' (0.011700)
   7. 'ashington' (0.002238)
   8. 'Washington' (0.002104)
   9. 'Capital' (0.001809)
  10. ' CAPITAL' (0.001629)
  11. ' Islamabad' (0.001418)
  12. ' berlin' (0.001388)
  13. ' capitals' (0.001216)
  14. ' London' (0.000868)
  15. ' federally' (0.000862)
  16. ' Federal' (0.000663)
  17. ' Bras' (0.000633)
  18. ' Tall' (0.000595)
  19. ' Westminster' (0.000541)
  20. '柏' (0.000532)

Layer 24 (entropy: 0.296):
   1. ' Berlin' (0.963915)
   2. 'Berlin' (0.017430)
   3. ' capital' (0.002497)
   4. ' Capital' (0.001743)
   5. ' Capitals' (0.001314)
   6. ' berlin' (0.000733)
   7. ' Islamabad' (0.000579)
   8. ' Washington' (0.000403)
   9. '柏' (0.000325)
  10. 'Capital' (0.000215)
  11. ' Sans' (0.000211)
  12. ' Bon' (0.000196)
  13. ' Ankara' (0.000190)
  14. ' Charl' (0.000170)
  15. ' federally' (0.000164)
  16. ' CAPITAL' (0.000155)
  17. ' Beijing' (0.000140)
  18. ' Paris' (0.000132)
  19. ' Federal' (0.000132)
  20. ' Canberra' (0.000131)

Layer 25 (entropy: 0.242):
   1. ' Berlin' (0.959347)
   2. 'Berlin' (0.032986)
   3. ' berlin' (0.001137)
   4. ' Islamabad' (0.000403)
   5. ' Capitals' (0.000236)
   6. ' capital' (0.000229)
   7. '柏' (0.000140)
   8. ' Washington' (0.000130)
   9. ' Paris' (0.000118)
  10. ' Capital' (0.000113)
  11. ' Tall' (0.000109)
  12. ' London' (0.000105)
  13. 'AREST' (0.000092)
  14. ' Bon' (0.000081)
  15. ' Ankara' (0.000077)
  16. ' Frankfurt' (0.000070)
  17. ' BER' (0.000067)
  18. '東京' (0.000063)
  19. ' Beijing' (0.000061)
  20. ' Ber' (0.000060)

Layer 26 (entropy: 0.230):
   1. ' Berlin' (0.957070)
   2. 'Berlin' (0.037447)
   3. ' berlin' (0.000963)
   4. ' BER' (0.000276)
   5. ' Bon' (0.000256)
   6. ' Ber' (0.000199)
   7. '柏' (0.000135)
   8. ' Бер' (0.000111)
   9. ' Islamabad' (0.000085)
  10. ' Brand' (0.000084)
  11. ' Capitals' (0.000055)
  12. ' capital' (0.000053)
  13. ' Ankara' (0.000044)
  14. '東京' (0.000044)
  15. 'olis' (0.000036)
  16. ' Sans' (0.000036)
  17. ' Charl' (0.000036)
  18. ' Capital' (0.000032)
  19. ' Madrid' (0.000031)
  20. ' London' (0.000028)

Layer 27 (entropy: 0.259):
   1. ' Berlin' (0.950747)
   2. 'Berlin' (0.042258)
   3. ' BER' (0.000803)
   4. ' berlin' (0.000775)
   5. ' Ber' (0.000659)
   6. ' Бер' (0.000474)
   7. ' Bon' (0.000366)
   8. '柏' (0.000189)
   9. ' Brand' (0.000100)
  10. ' Sans' (0.000072)
  11. 'olis' (0.000040)
  12. ' federally' (0.000036)
  13. ' Charl' (0.000032)
  14. ' Bern' (0.000031)
  15. 'ベル' (0.000031)
  16. ' Capitals' (0.000030)
  17. 'Ber' (0.000029)
  18. ' Tier' (0.000029)
  19. 'Brand' (0.000026)
  20. ' Bras' (0.000025)

Layer 28 (entropy: 0.563):
   1. ' Berlin' (0.912775)
   2. 'Berlin' (0.053021)
   3. ' BER' (0.007896)
   4. ' berlin' (0.002119)
   5. ' Бер' (0.001762)
   6. ' Ber' (0.001588)
   7. '-B' (0.001057)
   8. '柏' (0.000752)
   9. ' Bon' (0.000721)
  10. ' federally' (0.000292)
  11. ' Charl' (0.000289)
  12. 'BER' (0.000285)
  13. ' B' (0.000274)
  14. 'ob' (0.000248)
  15. 'Ber' (0.000215)
  16. 'olis' (0.000207)
  17. ' Tier' (0.000195)
  18. '	B' (0.000175)
  19. ' Brand' (0.000172)
  20. 'ber' (0.000136)

Layer 29 (entropy: 1.135):
   1. ' Berlin' (0.856085)
   2. 'Berlin' (0.040232)
   3. ' BER' (0.015203)
   4. ' Ber' (0.013451)
   5. ' Бер' (0.008101)
   6. ' berlin' (0.004651)
   7. ' Bon' (0.002847)
   8. '-B' (0.002592)
   9. '柏' (0.002516)
  10. 'Ber' (0.001831)
  11. ' ber' (0.001489)
  12. ' B' (0.000978)
  13. '	B' (0.000834)
  14. '_B' (0.000543)
  15. ' Tier' (0.000537)
  16. '_ber' (0.000446)
  17. ' Bern' (0.000433)
  18. ' бер' (0.000388)
  19. ' federally' (0.000334)
  20. ' capital' (0.000310)

Layer 30 (entropy: 0.805):
   1. ' Berlin' (0.907571)
   2. 'Berlin' (0.019560)
   3. ' BER' (0.011193)
   4. ' Ber' (0.008518)
   5. ' berlin' (0.003076)
   6. ' Бер' (0.002577)
   7. ' ber' (0.002205)
   8. ' Bern' (0.001925)
   9. ' Bon' (0.001641)
  10. ' None' (0.001521)
  11. '	B' (0.000818)
  12. ' ' (0.000624)
  13. ' none' (0.000605)
  14. '柏' (0.000565)
  15. ' Germany' (0.000565)
  16. 'Ber' (0.000534)
  17. ' Capital' (0.000455)
  18. 'ber' (0.000418)
  19. ' capital' (0.000386)
  20. ' бер' (0.000328)

Layer 31 (entropy: 1.181):
   1. ' Berlin' (0.856932)
   2. ' Germany' (0.017879)
   3. ' The' (0.014445)
   4. ' ' (0.006587)
   5. ' Ber' (0.004067)
   6. ' This' (0.003294)
   7. ' ' (0.002823)
   8. 'Berlin' (0.002782)
   9. ' (' (0.002779)
  10. ' None' (0.002749)
  11. ' A' (0.002430)
  12. ' Bon' (0.002336)
  13. ' What' (0.002304)
  14. ' It' (0.001996)
  15. ' the' (0.001585)
  16. ' In' (0.001454)
  17. '<|end_of_text|>' (0.001390)
  18. ' There' (0.001248)
  19. ' 
' (0.001221)
  20. ' Frankfurt' (0.001177)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 1.181):
   1. ' Berlin' (0.856933)
   2. ' Germany' (0.017879)
   3. ' The' (0.014445)
   4. ' ' (0.006587)
   5. ' Ber' (0.004067)
   6. ' This' (0.003294)
   7. ' ' (0.002823)
   8. 'Berlin' (0.002782)
   9. ' (' (0.002779)
  10. ' None' (0.002749)
  11. ' A' (0.002430)
  12. ' Bon' (0.002336)
  13. ' What' (0.002304)
  14. ' It' (0.001996)
  15. ' the' (0.001585)
  16. ' In' (0.001454)
  17. '<|end_of_text|>' (0.001390)
  18. ' There' (0.001248)
  19. ' 
' (0.001221)
  20. ' Frankfurt' (0.001177)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 4.174)
   1. ' a' (0.287620)
   2. ' one' (0.058417)
   3. ' the' (0.052589)
   4. ' also' (0.049548)
   5. ' home' (0.032243)
   6. ' known' (0.027372)
   7. ' an' (0.026675)
   8. ' famous' (0.024187)
   9. ' full' (0.021176)
  10. ' located' (0.015285)

Prompt: 'Berlin is the capital of'
  (entropy: 0.643)
   1. ' Germany' (0.895522)
   2. ' the' (0.052471)
   3. ' and' (0.007549)
   4. ' germany' (0.003360)
   5. ' modern' (0.002971)
   6. ' Berlin' (0.002895)
   7. ' united' (0.002628)
   8. ' German' (0.002495)
   9. ' Europe' (0.002210)
  10. ' a' (0.002209)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 5.066)
   1. ' Berlin' (0.149279)
   2. ' The' (0.052448)
   3. ' Which' (0.051117)
   4. ' If' (0.045922)
   5. ' What' (0.045821)
   6. ' (' (0.022196)
   7. ' Now' (0.018856)
   8. ' You' (0.018445)
   9. ' Or' (0.016232)
  10. ' How' (0.015910)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000)
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. ' The' (0.000000)
   4. ' ' (0.000000)
   5. ' Ber' (0.000000)
   6. ' This' (0.000000)
   7. ' ' (0.000000)
   8. 'Berlin' (0.000000)
   9. ' (' (0.000000)
  10. ' None' (0.000000)
  11. ' A' (0.000000)
  12. ' Bon' (0.000000)
  13. ' What' (0.000000)
  14. ' It' (0.000000)
  15. ' the' (0.000000)

Temperature 2.0:
  (entropy: 10.065)
   1. ' Berlin' (0.034909)
   2. ' Germany' (0.005042)
   3. ' The' (0.004532)
   4. ' ' (0.003061)
   5. ' Ber' (0.002405)
   6. ' This' (0.002164)
   7. ' ' (0.002004)
   8. 'Berlin' (0.001989)
   9. ' (' (0.001988)
  10. ' None' (0.001977)
  11. ' A' (0.001859)
  12. ' Bon' (0.001823)
  13. ' What' (0.001810)
  14. ' It' (0.001685)
  15. ' the' (0.001501)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 32
Model dimension: 4096
Number of heads: 32
Vocab size: 128256
Context length: 8192
=== END OF MODEL STATS ========

