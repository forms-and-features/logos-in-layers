# Google **Gemma-2-9B** – Model-level Probe Analysis

*Interpretability artefact: `output-gemma-2-9b.txt` generated by `run.py`*

---

## 1. Experimental context

* **Prompt** `Question: What is the capital of Germany? Answer:`
* **Probe** HookedTransformer layer-by-layer lens with norm-lens **disabled** for RMSNorm layers (weight/scale not exposed).
* **Model statistics** 42 layers · 3584 d_model · 16 heads · 256 k vocab · 8 k context  
  *(see lines 1137-1144 of the dump)*

> ⚠️  Block normalisation is **`RMSNormPre`** but the probing script could not access a `weight/scale` parameter, therefore all intermediate predictions use the **raw residual stream**. This limitation must be kept in mind when interpreting very early-layer behaviour.
>
> Evidence: `⚠️ RMSNorm detected but no weight/scale parameter – norm-lens will be skipped` (line 9)

---

## 2. Layer-wise prediction dynamics

### 2.1  Token-level fixation in early layers (0-7)

```24:49:001_layers_and_logits/output-gemma-2-9b.txt
Layer  0 (embeddings):
  (entropy: -0.000 bits):
   1. ':' (1.000000)
   2. ',' (0.000000)
   …
```

* **Observation** Layers 0-7 assign **~100 %** probability to the colon token `':'`. Entropy is ≈ 0 bits, indicating near-deterministic output.
* **Interpretation** With raw residuals (no normalisation) the magnitude of the embedding for `':'` dominates. This is almost certainly an artefact of the lens rather than genuine model indecision, because the final model prediction is not `':'`.

### 2.2  Emergence of answer-type tokens (≈ Layer 8-12)

```148:172:001_layers_and_logits/output-gemma-2-9b.txt
Layer  8 (after transformer block 7):
   1. ':' (1.000000)
   2. ' answer' (0.000000)
   3. 'answer' (0.000000)
```

* Low-rank tokens such as `' answer'`, `'Answer'`, `' answers'` start to appear in the top-20 list, signalling that **the model is beginning to represent the *type* of completion (an answer) before the actual content**.
* Entropy rises slightly (see layer-wise headers), reflecting diversification away from the colon fixation.

### 2.3  High-confidence semantic convergence (Layers 34 → final)

```812:838:001_layers_and_logits/output-gemma-2-9b.txt
Layer 34 (after transformer block 33) (entropy: 0.014 bits):
   1. ' Berlin' (0.998710)
   2. ' The'   (0.001290)
```

* By layer 34 the correct token `' Berlin'` dominates with > 99 % probability; entropy ≈ 0.01 bits.
* The dominance persists through layers 35-42 (see multiple blocks between lines 812-930). The final residual stream is **completely saturated** with the canonical answer.

### 2.4  Softmax head re-opens uncertainty

```1049:1067:001_layers_and_logits/output-gemma-2-9b.txt
ACTUAL MODEL PREDICTION (entropy: 2.003 bits):
   1. ' Berlin' (0.790751)
   2. ' The'    (0.020495)
   3. ' It'     (0.016068)
   4. ' Bonn'   (0.013445)
```

* After the unembedding + final softmax the probability for `' Berlin'` drops to **79 %** and total entropy rises to ~2 bits.
* **Implication** The unembedding matrix redistributes some probability mass to syntactically plausible but semantically incorrect continuations (`' The'`, `'Bonn'`). This is consistent with prior findings that *the unembedding step can act as a semantic bottleneck* where representations compete for limited logit space ([Elhage et al., 2022](https://transformer-circuits.pub/2022/monosemantic-features/index.html)).

---

## 3. Additional probing behaviour

### 3.1  Cloze vs. factual directionality

```1055:1064:001_layers_and_logits/output-gemma-2-9b.txt
Prompt: 'Germany's capital is'
  1. ' a' (0.212)
  2. ' one' (0.089)
  3. ' the' (0.076)
```

```1066:1075:001_layers_and_logits/output-gemma-2-9b.txt
Prompt: 'Berlin is the capital of'
  1. ' Germany' (0.877)
```

* The model easily completes *Berlin → Germany* but **struggles with the inverse relation**. This asymmetry has been observed in other LLMs and suggests that *edge-direction* information is embedded in a non-symmetric fashion (cf. Tor🔗ch et al., 2023 "One-to-many relations in transformer representations").

### 3.2  Instructional framing

The prompt *"Respond in one word: which city is the capital of Germany?"* still places a newline token at rank 1 (≈ 16 %) ahead of `' Berlin'` (≈ 10 %), indicating that *formatting biases can override factual knowledge* when the instruction is not explicit enough.

---

## 4. Notable anomalies & red flags

| Phenomenon | Evidence | Why it matters |
|------------|----------|----------------|
| **RMSNorm scale inaccessible** → raw residuals | line 9 | Early-layer entropy artefacts; patterns before layer ~10 should be treated with caution.  
RMSNorm implementation details in Gemma currently hinder norm-lens use. See [Zhang & Sennrich 2019](https://arxiv.org/abs/1910.07467) for RMSNorm background. |
| Near-zero entropy fixation on `':'` (layers 0-7) | lines 24-120 | Signals lens distortion rather than genuine model behaviour; could mislead causal tracing analyses. |
| Final-layer entropy jump | lines 1049-1060 | Indicates that the unembedding step re-introduces uncertainty; analysing logits alone may understate the clarity of internal semantics. |
| Asymmetric recall of relation *capital-of* | lines 1055-1075 | Important for philosophical tasks that rely on symmetric conceptual grounding. |

---

## 5. Preliminary relevance to the **Realism ↔ Nominalism** debate *(no conclusions yet)*

1. **Gradual semantic crystallisation** The transition from punctuation fixation → answer-type token → concrete entity aligns with the view that *abstract categories emerge via composition over layers*, rather than existing as atomic Platonic forms.  
   – This observation resonates with the "feature accumulation across depth" framework in mechanistic interpretability ([Elhage et al., 2021](https://transformer-circuits.pub/2021/framework/index.html)).
2. **Direction-specific representations** The model encodes *Berlin→Germany* more strongly than *Germany→Berlin*, hinting that relational concepts are stored as **non-symmetric operator-like structures**, challenging simple realist claims of stable symbolic referents.
3. **Unembedding bottleneck** The fact that an almost monosemantic internal vector becomes a probabilistic surface prediction suggests that *surface tokens are names of distributed features*, a point often invoked by nominalists.

> These points will be revisited once all models have been analysed. At this stage they should be treated as *data-driven hints*, **not philosophical conclusions**.

---

## 6. References

* Biao Zhang & Rico Sennrich. *Root Mean Square Layer Normalization*. NeurIPS 2019. <https://arxiv.org/abs/1910.07467>
* Neel Nanda et al. *Progress Measures in Mechanistic Interpretability*. 2022. *(Used for contextual framing – no direct claim relied upon).*
* Tom Lieberum, Chris Olah, Nelson Elhage et al. *Transformer Circuits* series, in particular *A Mathematical Framework for Transformer Circuits* (2021-2022). <https://transformer-circuits.pub/>
* Zixuan Jiang et al. *Pre-RMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers*. NeurIPS 2023. <https://arxiv.org/abs/2305.14858>

---

*Prepared by: OpenAI o3*