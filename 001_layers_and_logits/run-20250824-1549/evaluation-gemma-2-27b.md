# Evaluation Report: google/gemma-2-27b

*Run executed on: 2025-08-24 15:49:44*
**Overview**
- Model: google/gemma-2-27b (27B), run date 2025-08-24 15:49:44.
- Probe captures layer-wise next-token distributions via a norm lens, tracking entropy, copy/filler collapse, and emergence of the factual answer token.

**Method Sanity-Check**
The JSON confirms the intended norm lens and rotary positional handling are active: "use_norm_lens": true [L807], and "layer0_position_info": "token_only_rotary_model" [L816]. The context prompt ends exactly with â€œcalled simplyâ€ and no trailing space: "context_prompt": "Give the city name only, plain text. The capital of Germany is called simply" [L4]. Diagnostics include required fields: "unembed_dtype": "torch.float32" [L809], "L_copy": 0 [L819], "L_copy_H": 0 [L820], "L_semantic": 46 [L821], "delta_layers": 46 [L822]. Copy-collapse was flagged early (Ï„=0.90, Î´=0.05): first True row in pure CSV is layer 0: topâ€‘1 â€˜ simplyâ€™, p1 = 0.999976; topâ€‘2 â€˜ merelyâ€™, p2 = 7.52eâ€‘06 (row 2 in CSV) â€” âœ“ rule satisfied.

**Quantitative Findings**
- L 0 â€” entropy 0.000497 bits, topâ€‘1 â€˜ simplyâ€™
- L 1 â€” entropy 8.758229 bits, topâ€‘1 â€˜â€™
- L 2 â€” entropy 8.764487 bits, topâ€‘1 â€˜â€™
- L 3 â€” entropy 0.885666 bits, topâ€‘1 â€˜ simplyâ€™
- L 4 â€” entropy 0.618273 bits, topâ€‘1 â€˜ simplyâ€™
- L 5 â€” entropy 8.520256 bits, topâ€‘1 â€˜à¹²â€™
- L 6 â€” entropy 8.553085 bits, topâ€‘1 â€˜ï•€â€™
- L 7 â€” entropy 8.546973 bits, topâ€‘1 â€˜î«¤â€™
- L 8 â€” entropy 8.528743 bits, topâ€‘1 â€˜ïŒâ€™
- L 9 â€” entropy 8.523797 bits, topâ€‘1 â€˜ğ†£â€™
- L 10 â€” entropy 8.345239 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 11 â€” entropy 8.492760 bits, topâ€‘1 â€˜ğ†£â€™
- L 12 â€” entropy 8.324418 bits, topâ€‘1 â€˜î«¤â€™
- L 13 â€” entropy 8.222488 bits, topâ€‘1 â€˜î«¤â€™
- L 14 â€” entropy 7.876609 bits, topâ€‘1 â€˜î«¤â€™
- L 15 â€” entropy 7.792481 bits, topâ€‘1 â€˜î«¤â€™
- L 16 â€” entropy 7.974840 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 17 â€” entropy 7.785551 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 18 â€” entropy 7.299926 bits, topâ€‘1 â€˜Å¿ichtâ€™
- L 19 â€” entropy 7.527773 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 20 â€” entropy 6.209991 bits, topâ€‘1 â€˜Å¿ichtâ€™
- L 21 â€” entropy 6.456000 bits, topâ€‘1 â€˜Å¿ichtâ€™
- L 22 â€” entropy 6.378438 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 23 â€” entropy 7.010409 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 24 â€” entropy 6.497042 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 25 â€” entropy 6.994874 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 26 â€” entropy 6.219814 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 27 â€” entropy 6.700720 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 28 â€” entropy 7.140120 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 29 â€” entropy 7.574150 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 30 â€” entropy 7.330207 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 31 â€” entropy 7.565168 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 32 â€” entropy 8.873556 bits, topâ€‘1 â€˜ zuÅ¿ammenâ€™
- L 33 â€” entropy 6.944745 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 34 â€” entropy 7.738321 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 35 â€” entropy 7.650662 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 36 â€” entropy 7.657739 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 37 â€” entropy 7.572387 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 38 â€” entropy 7.553552 bits, topâ€‘1 â€˜ ãƒ‘ãƒ³ãƒãƒ©â€™
- L 39 â€” entropy 7.232440 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 40 â€” entropy 8.710523 bits, topâ€‘1 â€˜ å±•æ¿â€™
- L 41 â€” entropy 7.081689 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 42 â€” entropy 7.056524 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 43 â€” entropy 7.088928 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 44 â€” entropy 7.568330 bits, topâ€‘1 â€˜ dieÅ¿emâ€™
- L 45 â€” entropy 7.140568 bits, topâ€‘1 â€˜ GeÅ¿châ€™
- **L 46 â€” entropy 0.118048 bits, topâ€‘1 â€˜Berlinâ€™**

Î”H (bits) = entropy(L_copy) âˆ’ entropy(L_semantic) = 0.000497 âˆ’ 0.118048 â‰ˆ âˆ’0.118.

Confidence milestones:
- p > 0.30 at layer 46,  p > 0.60 at layer 46,  finalâ€‘layer p = 0.9841.

**Qualitative Patterns & Anomalies**
The early stack shows strong copy reflex on the final prompt token â€œsimplyâ€ (layers 0, 3, 4 marked copy_collapse = True), then drifts into highâ€‘entropy distributions dominated by nonâ€‘Latin artifacts and archaic German orthography (e.g., â€˜ dieÅ¿emâ€™, â€˜Å¿ichtâ€™), consistent with lensing across partly formed features before consolidation (cf. tuned-lens, arXiv:2303.08112). By the final layer the answer consolidates sharply: â€œBerlinâ€ becomes topâ€‘1 with p = 0.984 (entropy 0.118 bits), and also lights up as topâ€‘1 when lensing earlier positions near the answer span: â€œisâ€ â†’ â€œ Berlinâ€ (0.999998) and â€œcalledâ€ â†’ â€œ Berlinâ€ (0.999868) [rows 804â€“806 in records.csv].

Negative control shows no leakage: topâ€‘5 for â€œBerlin is the capital ofâ€ are â€œ Germanyâ€ (0.8676), â€œ theâ€ (0.0650), â€œ andâ€ (0.0065), â€œ aâ€ (0.0062), â€œ Europeâ€ (0.0056) [L10â€“L31 in JSON], so no â€œBerlinâ€ appears â€” no semantic leakage.

Temperature robustness: at T = 0.1 the model is extremely confident â€” â€œ Berlinâ€ rank 1 (p = 0.9898; entropy 0.082 bits) [L670â€“L676]; at T = 2.0 â€œ Berlinâ€ remains rank 1 but much flatter (p = 0.0492; entropy 12.631 bits) [L737â€“L743], consistent with the expected entropy inflation under high temperature.

Important-word trajectory: In the prompt positions, â€œcapitalâ€ is already highly anchored at layer 0 (p â‰ˆ 0.9994) [row 13 in records.csv], and â€œGermanyâ€ is salient (p â‰ˆ 0.436) [row 15], while the NEXT-token prediction initially copies â€œsimplyâ€ at L0 with overwhelming confidence (0.999976) [row 2 in pure CSV]. â€œBerlinâ€ first enters any topâ€‘5 only at the very end: at layer 46 it dominates across the tail of the prompt as well as NEXT (e.g., â€œâ€¦ (â€˜Berlinâ€™, 0.984)â€ [row 48 in pure CSV]; and â€œisâ€â†’â€œ Berlinâ€, 0.999998 [row 804 in records.csv]). This late emergence aligns with lateâ€‘stack semantic consolidation reported in tuned-lens analyses (arXiv:2303.08112).

Prompt variants show the expected behavior: removing the oneâ€‘word instruction and targeting the country instead yields â€œ Germanyâ€ topâ€‘1 for â€œGive the country name only, plain text. Berlin is the capital ofâ€ (p = 0.449) [L437â€“L439]. For cityâ€‘targeted rephrasings without â€œsimplyâ€, e.g., â€œThe capital city of Germany is named simplyâ€, â€œ Berlinâ€ remains topâ€‘1 (p = 0.4316) [L483â€“L486]. The JSON does not report separate collapseâ€‘layer indices for these variants, so any shift in L_semantic is n.a.

Restâ€‘mass sanity: Rest_mass falls steadily as the answer consolidates; final layer rest_mass = 1.99eâ€‘07 (row 48 in pure CSV). No spikes after L_semantic (n.a. since L_semantic = final layer), suggesting no precision loss.

Quotes
> â€œcontext_prompt â€¦ called simplyâ€ [L4]; â€œuse_norm_lensâ€: true; â€œlayer0_position_infoâ€: â€œtoken_only_rotary_modelâ€ [L807, L816].
> â€œBerlin is the capital of â€¦ (â€˜ Germanyâ€™, 0.8676, â€¦ â€˜ Europeâ€™, 0.0056)â€ [L10â€“L31].
> â€œT=0.1 â€¦ (â€˜ Berlinâ€™, 0.9898) â€¦ T=2.0 â€¦ (â€˜ Berlinâ€™, 0.0492)â€ [L670â€“L676; L737â€“L743].
> â€œâ€¦ (â€˜Berlinâ€™, 0.984)â€ [row 48 in pure CSV]; â€œis â†’ â€˜ Berlinâ€™ (0.999998)â€ [row 804 in records.csv].

Checklist
- RMS lens?: âœ“ (RMSNorm model; norm lens active) [L807, L810â€“L814]
- LayerNorm bias removed?: âœ“/n.a. (â€œnot_needed_rms_modelâ€) [L812]
- Entropy rise at unembed?: âœ“ (lens L46 0.118 bits vs final_prediction 2.886 bits) [row 48 in pure CSV; L826]
- FP32 unâ€‘embed promoted?: âœ“ (decoding in fp32; "unembed_dtype": "torch.float32") [L809]
- Punctuation / markup anchoring?: âœ— (NEXT-token early layers dominated by copy and orthographic artifacts, not punctuation)
- Copyâ€‘reflex?: âœ“ (copy_collapse True in layers 0â€“4; e.g., L0) [row 2 in pure CSV]
- Grammatical filler anchoring?: âœ— (no â€˜is/the/a/ofâ€™ as topâ€‘1 in L0â€“L5 of pure CSV)

**Limitations & Data Quirks**
- High midâ€‘stack rest_mass (~0.96â€“0.98), indicating heavy tail mass outside topâ€‘20; lens readings are still coherent but sparse in topâ€‘k coverage.
- Topâ€‘1 tokens in midâ€‘layers include nonâ€‘Latin/orthographic artifacts (e.g., â€˜ dieÅ¿emâ€™, â€˜Å¿ichtâ€™, â€˜ ãƒ‘ãƒ³ãƒãƒ©â€™), a common lens artifact rather than literal semantics.
- L_semantic coincides with the final layer; absence of postâ€‘semantic layers prevents â€œmax after L_semanticâ€ restâ€‘mass checks.

**Model Fingerprint**
Gemmaâ€‘2â€‘27B: collapse at L 46; final lens entropy 0.118 bits; â€œBerlinâ€ only appears as topâ€‘1 at the last layer, with strong early copy reflex on â€œsimplyâ€.

---
Produced by OpenAI GPT-5
