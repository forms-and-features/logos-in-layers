
============================================================
EVALUATING MODEL: mistralai/Mistral-7B-v0.1
============================================================
Loading model: mistralai/Mistral-7B-v0.1...
Loaded pretrained model mistralai/Mistral-7B-v0.1 into HookedTransformer

=== NORMALIZATION ANALYSIS ========
Block normalization type: RMSNormPre
⚠️  RMSNorm detected but no weight/scale parameter - norm-lens will be skipped
Final normalization type: RMSNormPre
=== END NORMALIZATION ANALYSIS ====


=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['<s>', 'Question', ':', 'What', 'is', 'the', 'capital', 'of', 'Germany', '?', 'Answer', ':']
Computing layer-wise predictions (memory-efficient targeted caching)...

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using RAW residual stream (unsupported normalization, skipping to avoid distortion)
Note: Shown probabilities are from full softmax (calibrated and comparable)
------------------------------------------------------------
Layer  0 (embeddings):
  (entropy: 14.966 bits):
   1. 'laug' (0.000032)
   2. 'avax' (0.000032)
   3. 'auf' (0.000032)
   4. 'erre' (0.000032)
   5. 'voir' (0.000032)
   6. 'istribute' (0.000032)
   7. 'innen' (0.000032)
   8. 'ueto' (0.000031)
   9. 'aliment' (0.000031)
  10. 'onces' (0.000031)
  11. 'usqu' (0.000031)
  12. 'ettes' (0.000031)
  13. 'abi' (0.000031)
  14. 'ffen' (0.000031)
  15. 'holm' (0.000031)
  16. 'Sink' (0.000031)
  17. 'lette' (0.000031)
  18. 'EV' (0.000031)
  19. 'thous' (0.000031)
  20. 'serts' (0.000031)

Layer  1 (after transformer block 0) (entropy: 14.966 bits):
   1. 'zo' (0.000032)
   2. 'l' (0.000032)
   3. 'env' (0.000032)
   4. 'ladies' (0.000032)
   5. 'qu' (0.000032)
   6. 'dhd' (0.000032)
   7. 'wards' (0.000032)
   8. 'mur' (0.000032)
   9. 'ts' (0.000032)
  10. 'inf' (0.000032)
  11. 'dropped' (0.000032)
  12. 'sto' (0.000032)
  13. 'neighbor' (0.000032)
  14. 'listop' (0.000032)
  15. 'crashed' (0.000032)
  16. 'opt' (0.000032)
  17. 'jar' (0.000032)
  18. 'janu' (0.000032)
  19. 'licht' (0.000032)
  20. 'æ' (0.000032)

Layer  2 (after transformer block 1) (entropy: 14.966 bits):
   1. 'ts' (0.000032)
   2. 'richt' (0.000032)
   3. 'reck' (0.000032)
   4. 'thoroughly' (0.000032)
   5. 'ffen' (0.000032)
   6. 'rig' (0.000032)
   7. 'rike' (0.000032)
   8. 'tered' (0.000032)
   9. 'unk' (0.000032)
  10. 'aja' (0.000032)
  11. 'bal' (0.000032)
  12. 'oya' (0.000032)
  13. 'ts' (0.000032)
  14. 'quet' (0.000032)
  15. 'battery' (0.000032)
  16. 'soci' (0.000032)
  17. 'reck' (0.000032)
  18. 'disagree' (0.000032)
  19. 'abs' (0.000032)
  20. 'Ts' (0.000032)

Layer  3 (after transformer block 2) (entropy: 14.966 bits):
   1. 'richt' (0.000033)
   2. 'aiser' (0.000033)
   3. 'rig' (0.000033)
   4. 'ts' (0.000033)
   5. 'Â' (0.000033)
   6. 'reck' (0.000033)
   7. 'reck' (0.000033)
   8. 'lo' (0.000033)
   9. 'thoroughly' (0.000033)
  10. 'ffen' (0.000033)
  11. 'amber' (0.000033)
  12. 'pel' (0.000033)
  13. 'nevertheless' (0.000032)
  14. 'reich' (0.000032)
  15. 'serts' (0.000032)
  16. 'ENDOR' (0.000032)
  17. '�' (0.000032)
  18. 'ilen' (0.000032)
  19. 'Griff' (0.000032)
  20. 'anne' (0.000032)

Layer  4 (after transformer block 3) (entropy: 14.966 bits):
   1. 'amber' (0.000033)
   2. 'aiser' (0.000033)
   3. 'thoroughly' (0.000033)
   4. 'situ' (0.000033)
   5. 'ople' (0.000033)
   6. 'ves' (0.000033)
   7. 'lo' (0.000033)
   8. 'reck' (0.000033)
   9. 'rig' (0.000033)
  10. 'Â' (0.000033)
  11. 'reck' (0.000033)
  12. 'bes' (0.000033)
  13. 'pronounced' (0.000033)
  14. 'illes' (0.000033)
  15. 'anne' (0.000033)
  16. 'ilen' (0.000033)
  17. 'richt' (0.000033)
  18. 'ner' (0.000033)
  19. 'ENDOR' (0.000033)
  20. 'Coord' (0.000033)

Layer  5 (after transformer block 4) (entropy: 14.966 bits):
   1. 'aiser' (0.000034)
   2. 'stadt' (0.000034)
   3. 'amber' (0.000034)
   4. 'arta' (0.000034)
   5. 'ople' (0.000034)
   6. 'rien' (0.000034)
   7. 'ady' (0.000034)
   8. 'ffen' (0.000034)
   9. 'ieden' (0.000034)
  10. 'ais' (0.000034)
  11. 'stract' (0.000034)
  12. 'illes' (0.000034)
  13. 'haus' (0.000034)
  14. 'cert' (0.000034)
  15. 'tober' (0.000034)
  16. 'smith' (0.000034)
  17. 'iani' (0.000034)
  18. 'avia' (0.000034)
  19. 'located' (0.000034)
  20. 'odes' (0.000034)

Layer  6 (after transformer block 5) (entropy: 14.965 bits):
   1. 'amber' (0.000035)
   2. 'adt' (0.000035)
   3. 'tober' (0.000035)
   4. 'stadt' (0.000034)
   5. 'lagen' (0.000034)
   6. 'Answer' (0.000034)
   7. 'aises' (0.000034)
   8. 'precisely' (0.000034)
   9. 'rais' (0.000034)
  10. 'answered' (0.000034)
  11. 'ais' (0.000034)
  12. 'Â' (0.000034)
  13. 'AIN' (0.000034)
  14. 'nes' (0.000034)
  15. 'rezent' (0.000034)
  16. 'swer' (0.000034)
  17. 'answer' (0.000034)
  18. 'odes' (0.000034)
  19. 'attan' (0.000034)
  20. 'ши' (0.000034)

Layer  7 (after transformer block 6) (entropy: 14.965 bits):
   1. 'nab' (0.000036)
   2. 'stadt' (0.000036)
   3. 'ь' (0.000035)
   4. 'adt' (0.000035)
   5. 'swer' (0.000035)
   6. 'amber' (0.000035)
   7. 'Â' (0.000035)
   8. 'trag' (0.000035)
   9. 'unden' (0.000035)
  10. 'tober' (0.000035)
  11. 'Kot' (0.000035)
  12. 'ei' (0.000035)
  13. 'aises' (0.000035)
  14. 'stract' (0.000035)
  15. 'alike' (0.000035)
  16. 'civilian' (0.000035)
  17. 'nan' (0.000035)
  18. 'werken' (0.000035)
  19. 'ppe' (0.000035)
  20. 'nes' (0.000035)

Layer  8 (after transformer block 7) (entropy: 14.965 bits):
   1. 'amber' (0.000037)
   2. 'answer' (0.000036)
   3. 'anel' (0.000036)
   4. 'spy' (0.000036)
   5. 'alike' (0.000036)
   6. 'Answer' (0.000036)
   7. 'ner' (0.000036)
   8. 'stract' (0.000036)
   9. 'bekan' (0.000036)
  10. 'кта' (0.000035)
  11. 'adt' (0.000035)
  12. 'avia' (0.000035)
  13. 'ei' (0.000035)
  14. 'Gott' (0.000035)
  15. 'NER' (0.000035)
  16. 'nab' (0.000035)
  17. 'esis' (0.000035)
  18. 'stadt' (0.000035)
  19. 'ungen' (0.000035)
  20. 'ias' (0.000035)

Layer  9 (after transformer block 8) (entropy: 14.965 bits):
   1. 'answer' (0.000039)
   2. 'Answer' (0.000038)
   3. 'answered' (0.000037)
   4. 'unden' (0.000037)
   5. 'stadt' (0.000037)
   6. 'NER' (0.000037)
   7. 'owi' (0.000037)
   8. 'LE' (0.000037)
   9. 'aval' (0.000037)
  10. 'twice' (0.000037)
  11. 'seh' (0.000037)
  12. 'tour' (0.000036)
  13. 'igd' (0.000036)
  14. 'answering' (0.000036)
  15. 'yes' (0.000036)
  16. 'aris' (0.000036)
  17. 'amber' (0.000036)
  18. 'ilon' (0.000036)
  19. 'adrat' (0.000036)
  20. 'ema' (0.000036)

Layer 10 (after transformer block 9) (entropy: 14.964 bits):
   1. 'answer' (0.000041)
   2. 'Answer' (0.000041)
   3. 'unden' (0.000040)
   4. '/******/' (0.000040)
   5. 'answer' (0.000039)
   6. 'answered' (0.000039)
   7. 'stadt' (0.000039)
   8. 'ora' (0.000039)
   9. 'Arthur' (0.000038)
  10. 'answers' (0.000038)
  11. '答' (0.000038)
  12. 'icz' (0.000038)
  13. 'eph' (0.000038)
  14. 'iesz' (0.000038)
  15. 'iels' (0.000038)
  16. 'swer' (0.000038)
  17. 'än' (0.000037)
  18. 'ót' (0.000037)
  19. 'lio' (0.000037)
  20. 'ilia' (0.000037)

Layer 11 (after transformer block 10) (entropy: 14.963 bits):
   1. 'Answer' (0.000042)
   2. 'answer' (0.000042)
   3. 'stadt' (0.000041)
   4. 'rium' (0.000040)
   5. 'indow' (0.000039)
   6. 'edes' (0.000039)
   7. 'ír' (0.000039)
   8. 'answer' (0.000039)
   9. 'swer' (0.000039)
  10. 'answered' (0.000039)
  11. 'inder' (0.000039)
  12. 'astr' (0.000039)
  13. 'ora' (0.000039)
  14. '/******/' (0.000039)
  15. 'woh' (0.000039)
  16. 'amber' (0.000038)
  17. 'Zwe' (0.000038)
  18. 'iba' (0.000038)
  19. 'над' (0.000038)
  20. 'Arthur' (0.000038)

Layer 12 (after transformer block 11) (entropy: 14.963 bits):
   1. '/******/' (0.000045)
   2. 'шта' (0.000041)
   3. 'ilor' (0.000040)
   4. 'swer' (0.000040)
   5. 'amber' (0.000040)
   6. 'answer' (0.000040)
   7. 'zw' (0.000040)
   8. '带' (0.000040)
   9. 'Answer' (0.000040)
  10. 'stadt' (0.000040)
  11. 'än' (0.000039)
  12. 'cius' (0.000039)
  13. 'ześ' (0.000039)
  14. 'ír' (0.000039)
  15. 'spieler' (0.000039)
  16. 'een' (0.000039)
  17. 'comerc' (0.000039)
  18. 'imal' (0.000039)
  19. 'uilder' (0.000039)
  20. 'anel' (0.000039)

Layer 13 (after transformer block 12) (entropy: 14.963 bits):
   1. 'ír' (0.000042)
   2. 'Answer' (0.000041)
   3. 'шта' (0.000041)
   4. 'htt' (0.000041)
   5. 'swer' (0.000040)
   6. 'answer' (0.000040)
   7. 'arden' (0.000040)
   8. 'zw' (0.000040)
   9. 'iger' (0.000040)
  10. 'шти' (0.000039)
  11. 'än' (0.000039)
  12. 'град' (0.000039)
  13. 'Films' (0.000039)
  14. 'égal' (0.000039)
  15. 'stadt' (0.000039)
  16. 'answered' (0.000039)
  17. 'ischer' (0.000039)
  18. 'plural' (0.000039)
  19. 'unden' (0.000039)
  20. 'dru' (0.000039)

Layer 14 (after transformer block 13) (entropy: 14.962 bits):
   1. 'Answer' (0.000044)
   2. 'ieden' (0.000043)
   3. 'amber' (0.000043)
   4. 'ír' (0.000043)
   5. 'swer' (0.000043)
   6. '........' (0.000043)
   7. 'anel' (0.000042)
   8. 'officially' (0.000042)
   9. 'usch' (0.000041)
  10. 'htt' (0.000041)
  11. 'iger' (0.000041)
  12. '................' (0.000041)
  13. '...' (0.000041)
  14. 'answer' (0.000041)
  15. '…' (0.000041)
  16. 'ът' (0.000041)
  17. 'ниш' (0.000040)
  18. 'orent' (0.000040)
  19. 'yes' (0.000040)
  20. '花' (0.000040)

Layer 15 (after transformer block 14) (entropy: 14.961 bits):
   1. 'Answer' (0.000050)
   2. 'answer' (0.000046)
   3. 'usch' (0.000046)
   4. '........' (0.000046)
   5. '……' (0.000045)
   6. 'ammen' (0.000044)
   7. 'officially' (0.000044)
   8. 'swer' (0.000044)
   9. 'amber' (0.000044)
  10. 'kommun' (0.000044)
  11. '…' (0.000043)
  12. 'iger' (0.000043)
  13. 'orent' (0.000043)
  14. '…' (0.000043)
  15. '...' (0.000043)
  16. 'zie' (0.000043)
  17. '….' (0.000043)
  18. 'htt' (0.000042)
  19. '....' (0.000042)
  20. 'ниш' (0.000042)

Layer 16 (after transformer block 15) (entropy: 14.958 bits):
   1. 'Answer' (0.000054)
   2. 'answer' (0.000054)
   3. 'cities' (0.000049)
   4. 'amber' (0.000049)
   5. 'answered' (0.000048)
   6. '........' (0.000048)
   7. 'WC' (0.000048)
   8. 'answer' (0.000047)
   9. 'swer' (0.000046)
  10. '……' (0.000046)
  11. 'Computer' (0.000046)
  12. 'officially' (0.000046)
  13. 'citizen' (0.000045)
  14. '答' (0.000045)
  15. 'Neither' (0.000045)
  16. '…' (0.000045)
  17. 'none' (0.000045)
  18. 'zie' (0.000045)
  19. 'ieden' (0.000045)
  20. 'kommun' (0.000045)

Layer 17 (after transformer block 16) (entropy: 14.953 bits):
   1. 'Answer' (0.000068)
   2. 'answer' (0.000067)
   3. 'cities' (0.000062)
   4. 'swer' (0.000060)
   5. 'answered' (0.000058)
   6. 'answer' (0.000057)
   7. '答' (0.000056)
   8. 'answ' (0.000054)
   9. 'ieden' (0.000054)
  10. 'qpoint' (0.000053)
  11. 'opyright' (0.000052)
  12. 'unden' (0.000052)
  13. 'ště' (0.000052)
  14. 'iem' (0.000052)
  15. 'orent' (0.000052)
  16. 'ammen' (0.000051)
  17. 'answering' (0.000051)
  18. 'tober' (0.000051)
  19. '........' (0.000051)
  20. 'esen' (0.000051)

Layer 18 (after transformer block 17) (entropy: 14.951 bits):
   1. 'cities' (0.000070)
   2. 'Answer' (0.000069)
   3. 'answer' (0.000067)
   4. 'unden' (0.000061)
   5. 'headquarters' (0.000061)
   6. 'swer' (0.000058)
   7. 'answer' (0.000057)
   8. 'towns' (0.000057)
   9. 'Capital' (0.000057)
  10. '…' (0.000055)
  11. '答' (0.000055)
  12. '……' (0.000054)
  13. 'None' (0.000054)
  14. 'officially' (0.000054)
  15. 'capital' (0.000054)
  16. 'answered' (0.000052)
  17. 'esch' (0.000052)
  18. 'città' (0.000052)
  19. 'Probably' (0.000052)
  20. 'city' (0.000051)

Layer 19 (after transformer block 18) (entropy: 14.945 bits):
   1. 'cities' (0.000087)
   2. 'Germany' (0.000080)
   3. 'Answer' (0.000079)
   4. 'answer' (0.000077)
   5. 'headquarters' (0.000071)
   6. 'Berlin' (0.000070)
   7. 'swer' (0.000069)
   8. 'Frankfurt' (0.000069)
   9. 'answer' (0.000065)
  10. 'towns' (0.000065)
  11. 'opyright' (0.000064)
  12. 'esch' (0.000064)
  13. 'Deutschland' (0.000064)
  14. 'Capital' (0.000064)
  15. '…' (0.000063)
  16. '➤' (0.000063)
  17. 'capital' (0.000062)
  18. '……' (0.000061)
  19. 'answered' (0.000061)
  20. 'Neither' (0.000059)

Layer 20 (after transformer block 19) (entropy: 14.932 bits):
   1. 'cities' (0.000131)
   2. 'capital' (0.000123)
   3. 'Germany' (0.000120)
   4. 'Capital' (0.000111)
   5. 'Berlin' (0.000100)
   6. 'answer' (0.000093)
   7. 'capit' (0.000088)
   8. 'city' (0.000083)
   9. 'Answer' (0.000083)
  10. '/******/' (0.000082)
  11. 'Deutschland' (0.000081)
  12. 'Frankfurt' (0.000080)
  13. 'parliament' (0.000080)
  14. 'Parliament' (0.000079)
  15. 'headquarters' (0.000078)
  16. '…' (0.000078)
  17. 'towns' (0.000077)
  18. 'swer' (0.000077)
  19. 'opyright' (0.000076)
  20. 'Washington' (0.000075)

Layer 21 (after transformer block 20) (entropy: 14.919 bits):
   1. 'capital' (0.000209)
   2. 'Berlin' (0.000191)
   3. 'Capital' (0.000191)
   4. 'Germany' (0.000180)
   5. 'cities' (0.000160)
   6. 'capit' (0.000137)
   7. 'answer' (0.000119)
   8. 'Deutschland' (0.000110)
   9. 'Frankfurt' (0.000107)
  10. 'Parliament' (0.000106)
  11. 'Washington' (0.000105)
  12. 'parliament' (0.000104)
  13. 'Answer' (0.000102)
  14. 'Paris' (0.000099)
  15. 'city' (0.000096)
  16. 'Government' (0.000094)
  17. 'Tokyo' (0.000094)
  18. 'politicians' (0.000092)
  19. 'Leip' (0.000092)
  20. 'Probably' (0.000091)

Layer 22 (after transformer block 21) (entropy: 14.912 bits):
   1. 'Berlin' (0.000312)
   2. 'Germany' (0.000266)
   3. 'capital' (0.000206)
   4. 'Capital' (0.000188)
   5. 'Frankfurt' (0.000158)
   6. 'cities' (0.000146)
   7. 'capit' (0.000140)
   8. 'Paris' (0.000132)
   9. 'Washington' (0.000132)
  10. 'Deutschland' (0.000125)
  11. 'answer' (0.000123)
  12. 'parliament' (0.000111)
  13. 'Parliament' (0.000111)
  14. 'German' (0.000108)
  15. 'London' (0.000107)
  16. 'Tokyo' (0.000106)
  17. 'Answer' (0.000106)
  18. 'Leip' (0.000105)
  19. 'None' (0.000104)
  20. 'technically' (0.000102)

Layer 23 (after transformer block 22) (entropy: 14.900 bits):
   1. 'Washington' (0.000537)
   2. 'Berlin' (0.000501)
   3. 'capital' (0.000289)
   4. 'Germany' (0.000275)
   5. 'Capital' (0.000267)
   6. 'capit' (0.000203)
   7. 'Frankfurt' (0.000188)
   8. 'washing' (0.000179)
   9. 'Paris' (0.000175)
  10. 'Beijing' (0.000170)
  11. 'Rome' (0.000156)
  12. 'cities' (0.000155)
  13. 'London' (0.000155)
  14. 'Moscow' (0.000153)
  15. 'ashington' (0.000142)
  16. 'DC' (0.000138)
  17. 'parliament' (0.000137)
  18. 'Tokyo' (0.000137)
  19. 'DC' (0.000136)
  20. 'Parliament' (0.000134)

Layer 24 (after transformer block 23) (entropy: 14.891 bits):
   1. 'Berlin' (0.000705)
   2. 'Washington' (0.000541)
   3. 'Germany' (0.000477)
   4. 'capital' (0.000275)
   5. 'Frankfurt' (0.000243)
   6. 'Capital' (0.000223)
   7. 'washing' (0.000187)
   8. 'German' (0.000184)
   9. 'capit' (0.000183)
  10. 'Paris' (0.000182)
  11. 'Deutschland' (0.000173)
  12. 'Beijing' (0.000168)
  13. 'Germans' (0.000159)
  14. 'cities' (0.000156)
  15. 'Ber' (0.000155)
  16. 'Moscow' (0.000150)
  17. 'Rome' (0.000147)
  18. 'Tokyo' (0.000142)
  19. 'Parliament' (0.000141)
  20. 'Jerusalem' (0.000140)

Layer 25 (after transformer block 24) (entropy: 14.842 bits):
   1. 'Berlin' (0.002891)
   2. 'Germany' (0.001431)
   3. 'capital' (0.000679)
   4. 'Frankfurt' (0.000551)
   5. 'German' (0.000537)
   6. 'Washington' (0.000508)
   7. 'Capital' (0.000500)
   8. 'Deutschland' (0.000401)
   9. 'Germans' (0.000399)
  10. 'capit' (0.000370)
  11. 'Ber' (0.000315)
  12. 'Paris' (0.000286)
  13. 'germ' (0.000251)
  14. 'Tokyo' (0.000248)
  15. 'Leip' (0.000244)
  16. 'Germ' (0.000232)
  17. 'Rome' (0.000228)
  18. 'Hamburg' (0.000210)
  19. 'cities' (0.000209)
  20. 'Moscow' (0.000207)

Layer 26 (after transformer block 25) (entropy: 14.809 bits):
   1. 'Berlin' (0.004747)
   2. 'Germany' (0.001644)
   3. 'Frankfurt' (0.000721)
   4. 'capital' (0.000672)
   5. 'German' (0.000644)
   6. 'Washington' (0.000493)
   7. 'Germans' (0.000476)
   8. 'Capital' (0.000471)
   9. 'Ber' (0.000445)
  10. 'Deutschland' (0.000425)
  11. 'Paris' (0.000411)
  12. 'Tokyo' (0.000312)
  13. 'Leip' (0.000293)
  14. 'germ' (0.000276)
  15. 'capit' (0.000273)
  16. 'Hamburg' (0.000271)
  17. 'Beijing' (0.000254)
  18. 'Germ' (0.000233)
  19. 'cities' (0.000232)
  20. 'Moscow' (0.000214)

Layer 27 (after transformer block 26) (entropy: 14.755 bits):
   1. 'Berlin' (0.009002)
   2. 'Germany' (0.001938)
   3. 'Ber' (0.000937)
   4. 'Frankfurt' (0.000926)
   5. 'German' (0.000638)
   6. 'capital' (0.000513)
   7. 'Deutschland' (0.000497)
   8. 'Germans' (0.000435)
   9. 'Paris' (0.000378)
  10. 'Washington' (0.000372)
  11. 'Capital' (0.000346)
  12. 'ber' (0.000345)
  13. 'Ber' (0.000340)
  14. 'Tokyo' (0.000331)
  15. 'Hamburg' (0.000309)
  16. 'germ' (0.000298)
  17. 'Leip' (0.000292)
  18. 'Бер' (0.000268)
  19. 'BER' (0.000267)
  20. 'cities' (0.000266)

Layer 28 (after transformer block 27) (entropy: 14.496 bits):
   1. 'Berlin' (0.025899)
   2. 'Ber' (0.005163)
   3. 'Ber' (0.002712)
   4. 'ber' (0.002508)
   5. 'Germany' (0.002103)
   6. 'Бер' (0.001056)
   7. 'Frankfurt' (0.001032)
   8. 'BER' (0.000978)
   9. 'ber' (0.000670)
  10. 'German' (0.000652)
  11. 'capital' (0.000591)
  12. 'Deutschland' (0.000524)
  13. 'Germans' (0.000414)
  14. 'Capital' (0.000370)
  15. 'Washington' (0.000364)
  16. 'Bern' (0.000363)
  17. 'Tokyo' (0.000350)
  18. 'Paris' (0.000327)
  19. 'It' (0.000314)
  20. 'There' (0.000305)

Layer 29 (after transformer block 28) (entropy: 14.032 bits):
   1. 'Berlin' (0.051379)
   2. 'Ber' (0.007342)
   3. 'Ber' (0.004269)
   4. 'ber' (0.003446)
   5. 'Germany' (0.003268)
   6. 'Frankfurt' (0.001461)
   7. 'German' (0.001198)
   8. 'It' (0.001080)
   9. 'There' (0.001034)
  10. 'BER' (0.001007)
  11. 'Бер' (0.000990)
  12. 'Bon' (0.000919)
  13. 'ber' (0.000894)
  14. 'capital' (0.000867)
  15. 'The' (0.000825)
  16. 'it' (0.000605)
  17. 'well' (0.000561)
  18. 'there' (0.000546)
  19. 'Its' (0.000500)
  20. 'That' (0.000492)

Layer 30 (after transformer block 29) (entropy: 13.268 bits):
   1. 'Berlin' (0.078473)
   2. 'Germany' (0.007485)
   3. 'Ber' (0.005726)
   4. 'It' (0.003709)
   5. 'Ber' (0.003668)
   6. 'The' (0.003657)
   7. 'There' (0.003102)
   8. 'Frankfurt' (0.002040)
   9. 'German' (0.002006)
  10. 'ber' (0.001880)
  11. 'Bon' (0.001548)
  12. 'That' (0.001547)
  13. 'This' (0.001426)
  14. 'it' (0.001301)
  15. 'If' (0.001269)
  16. '(' (0.001243)
  17. 'What' (0.001196)
  18. 'well' (0.001175)
  19. 'there' (0.001141)
  20. '
' (0.001107)

Layer 31 (after transformer block 30) (entropy: 11.722 bits):
   1. 'Berlin' (0.141611)
   2. 'Germany' (0.015716)
   3. 'The' (0.009987)
   4. 'It' (0.008308)
   5. 'There' (0.007171)
   6. 'That' (0.003711)
   7. 'Bon' (0.003589)
   8. 'Ber' (0.003423)
   9. 'This' (0.002917)
  10. 'None' (0.002718)
  11. 'If' (0.002489)
  12. 'What' (0.002366)
  13. 'In' (0.002178)
  14. '' (0.002135)
  15. 'Frankfurt' (0.002128)
  16. 'No' (0.002083)
  17. 'According' (0.002078)
  18. 'German' (0.002022)
  19. 'Well' (0.002013)
  20. '(' (0.001969)

Layer 32 (after transformer block 31) (entropy: 8.254 bits):
   1. 'Berlin' (0.295650)
   2. 'The' (0.024890)
   3. 'Germany' (0.023074)
   4. 'It' (0.019576)
   5. 'There' (0.013481)
   6. 'That' (0.007964)
   7. 'Bon' (0.006834)
   8. '
' (0.005916)
   9. 'Frankfurt' (0.005849)
  10. 'I' (0.005810)
  11. '' (0.005420)
  12. 'Well' (0.005265)
  13. 'B' (0.005265)
  14. 'This' (0.005115)
  15. 'What' (0.005007)
  16. 'A' (0.004461)
  17. 'If' (0.004448)
  18. 'No' (0.004415)
  19. 'You' (0.004362)
  20. '(' (0.004251)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 1.800 bits):
   1. 'Berlin' (0.831533)
   2. 'The' (0.021823)
   3. 'Germany' (0.019522)
   4. 'It' (0.015328)
   5. 'There' (0.008855)
   6. 'That' (0.004083)
   7. 'Bon' (0.003260)
   8. '
' (0.002636)
   9. 'Frankfurt' (0.002593)
  10. 'I' (0.002567)
  11. '' (0.002318)
  12. 'Well' (0.002221)
  13. 'B' (0.002221)
  14. 'This' (0.002129)
  15. 'What' (0.002063)
  16. 'A' (0.001741)
  17. 'If' (0.001733)
  18. 'No' (0.001714)
  19. 'You' (0.001684)
  20. '(' (0.001621)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 6.677 bits)
   1. 'a' (0.187836)
   2. 'one' (0.089726)
   3. 'the' (0.051249)
   4. 'known' (0.042725)
   5. 'home' (0.039110)
   6. 'an' (0.028183)
   7. 'not' (0.021238)
   8. 'famous' (0.021132)
   9. 'full' (0.018003)
  10. 'set' (0.010509)

Prompt: 'Berlin is the capital of'
  (entropy: 0.950 bits)
   1. 'Germany' (0.896618)
   2. 'the' (0.053939)
   3. 'both' (0.004361)
   4. 'a' (0.003802)
   5. 'Europe' (0.003114)
   6. 'Berlin' (0.002841)
   7. 'German' (0.002442)
   8. 'modern' (0.001617)
   9. 'and' (0.001473)
  10. 'one' (0.001109)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 4.562 bits)
   1. '
' (0.453120)
   2. 'Berlin' (0.143818)
   3. 'If' (0.027228)
   4. 'The' (0.023025)
   5. 'You' (0.014681)
   6. '(' (0.014457)
   7. '' (0.014370)
   8. 'What' (0.011041)
   9. 'Bon' (0.010859)
  10. 'I' (0.009624)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000 bits)
   1. 'Berlin' (1.000000)
   2. 'The' (0.000000)
   3. 'Germany' (0.000000)
   4. 'It' (0.000000)
   5. 'There' (0.000000)
   6. 'That' (0.000000)
   7. 'Bon' (0.000000)
   8. '
' (0.000000)
   9. 'Frankfurt' (0.000000)
  10. 'I' (0.000000)
  11. '' (0.000000)
  12. 'Well' (0.000000)
  13. 'B' (0.000000)
  14. 'This' (0.000000)
  15. 'What' (0.000000)

Temperature 2.0:
  (entropy: 12.151 bits)
   1. 'Berlin' (0.058676)
   2. 'The' (0.009506)
   3. 'Germany' (0.008990)
   4. 'It' (0.007966)
   5. 'There' (0.006055)
   6. 'That' (0.004111)
   7. 'Bon' (0.003674)
   8. '
' (0.003304)
   9. 'Frankfurt' (0.003276)
  10. 'I' (0.003260)
  11. '' (0.003098)
  12. 'Well' (0.003033)
  13. 'B' (0.003032)
  14. 'This' (0.002969)
  15. 'What' (0.002922)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 32
Model dimension: 4096
Number of heads: 32
Vocab size: 32000
Context length: 2048
=== END OF MODEL STATS ========

