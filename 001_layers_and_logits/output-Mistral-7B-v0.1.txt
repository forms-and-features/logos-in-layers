
============================================================
EVALUATING MODEL: mistralai/Mistral-7B-v0.1
============================================================
Loading model: mistralai/Mistral-7B-v0.1...
Loaded pretrained model mistralai/Mistral-7B-v0.1 into HookedTransformer

=== NORMALIZATION ANALYSIS ========
Block LayerNorm type: RMSNormPre
⚠️  Non-vanilla norm detected (RMSNormPre) - norm-lens will be skipped to avoid distortion
Final LayerNorm type: RMSNormPre
=== END NORMALIZATION ANALYSIS ====


=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['<s>', 'Question', ':', 'What', 'is', 'the', 'capital', 'of', 'Germany', '?', 'Answer', ':']
Computing layer-wise predictions (memory-efficient targeted caching)...

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using RAW residual stream (non-vanilla norms detected, skipping normalization to avoid distortion)
Note: Shown probabilities are softmax over top-k only (don't sum to 1)
------------------------------------------------------------
Layer  0 (embeddings):
  (entropy: 10.373):
   1. 'laug' (0.050049)
   2. 'avax' (0.050049)
   3. 'auf' (0.050041)
   4. 'erre' (0.050033)
   5. 'voir' (0.050023)
   6. 'istribute' (0.050018)
   7. 'innen' (0.050010)
   8. 'ueto' (0.050003)
   9. 'aliment' (0.049995)
  10. 'onces' (0.049991)
  11. 'usqu' (0.049991)
  12. 'ettes' (0.049989)
  13. 'abi' (0.049987)
  14. 'ffen' (0.049987)
  15. 'holm' (0.049978)
  16. 'Sink' (0.049978)
  17. 'lette' (0.049972)
  18. 'EV' (0.049970)
  19. 'thous' (0.049969)
  20. 'serts' (0.049967)

Layer  1 (after block 0) (entropy: 10.373):
   1. 'zo' (0.050108)
   2. 'l' (0.050073)
   3. 'env' (0.050067)
   4. 'ladies' (0.050052)
   5. 'qu' (0.050028)
   6. 'dhd' (0.050006)
   7. 'wards' (0.050004)
   8. 'mur' (0.050003)
   9. 'ts' (0.049998)
  10. 'inf' (0.049992)
  11. 'dropped' (0.049989)
  12. 'sto' (0.049982)
  13. 'neighbor' (0.049973)
  14. 'listop' (0.049969)
  15. 'crashed' (0.049966)
  16. 'opt' (0.049962)
  17. 'jar' (0.049961)
  18. 'janu' (0.049960)
  19. 'licht' (0.049954)
  20. 'æ' (0.049954)

Layer  2 (after block 1) (entropy: 10.373):
   1. 'ts' (0.050453)
   2. 'richt' (0.050283)
   3. 'reck' (0.050261)
   4. 'thoroughly' (0.050204)
   5. 'ffen' (0.050159)
   6. 'rig' (0.050137)
   7. 'rike' (0.050096)
   8. 'tered' (0.050010)
   9. 'unk' (0.049986)
  10. 'aja' (0.049961)
  11. 'bal' (0.049949)
  12. 'oya' (0.049903)
  13. 'ts' (0.049874)
  14. 'quet' (0.049859)
  15. 'battery' (0.049850)
  16. 'soci' (0.049836)
  17. 'reck' (0.049810)
  18. 'disagree' (0.049794)
  19. 'abs' (0.049788)
  20. 'Ts' (0.049787)

Layer  3 (after block 2) (entropy: 10.373):
   1. 'richt' (0.050696)
   2. 'aiser' (0.050418)
   3. 'rig' (0.050299)
   4. 'ts' (0.050223)
   5. 'Â' (0.050129)
   6. 'reck' (0.050080)
   7. 'reck' (0.050053)
   8. 'lo' (0.050046)
   9. 'thoroughly' (0.050044)
  10. 'ffen' (0.050027)
  11. 'amber' (0.049982)
  12. 'pel' (0.049917)
  13. 'nevertheless' (0.049848)
  14. 'reich' (0.049801)
  15. 'serts' (0.049793)
  16. 'ENDOR' (0.049744)
  17. '�' (0.049727)
  18. 'ilen' (0.049727)
  19. 'Griff' (0.049724)
  20. 'anne' (0.049722)

Layer  4 (after block 3) (entropy: 10.373):
   1. 'amber' (0.050697)
   2. 'aiser' (0.050410)
   3. 'thoroughly' (0.050217)
   4. 'situ' (0.050120)
   5. 'ople' (0.050112)
   6. 'ves' (0.050103)
   7. 'lo' (0.050085)
   8. 'reck' (0.050079)
   9. 'rig' (0.050055)
  10. 'Â' (0.050030)
  11. 'reck' (0.049940)
  12. 'bes' (0.049901)
  13. 'pronounced' (0.049882)
  14. 'illes' (0.049796)
  15. 'anne' (0.049782)
  16. 'ilen' (0.049777)
  17. 'richt' (0.049776)
  18. 'ner' (0.049774)
  19. 'ENDOR' (0.049737)
  20. 'Coord' (0.049728)

Layer  5 (after block 4) (entropy: 10.373):
   1. 'aiser' (0.050835)
   2. 'stadt' (0.050728)
   3. 'amber' (0.050287)
   4. 'arta' (0.050150)
   5. 'ople' (0.050109)
   6. 'rien' (0.050062)
   7. 'ady' (0.050057)
   8. 'ffen' (0.050021)
   9. 'ieden' (0.049968)
  10. 'ais' (0.049967)
  11. 'stract' (0.049964)
  12. 'illes' (0.049824)
  13. 'haus' (0.049796)
  14. 'cert' (0.049786)
  15. 'tober' (0.049775)
  16. 'smith' (0.049761)
  17. 'iani' (0.049760)
  18. 'avia' (0.049736)
  19. 'located' (0.049706)
  20. 'odes' (0.049706)

Layer  6 (after block 5) (entropy: 10.373):
   1. 'amber' (0.050582)
   2. 'adt' (0.050540)
   3. 'tober' (0.050518)
   4. 'stadt' (0.050300)
   5. 'lagen' (0.050218)
   6. 'Answer' (0.050206)
   7. 'aises' (0.050075)
   8. 'precisely' (0.049964)
   9. 'rais' (0.049914)
  10. 'answered' (0.049906)
  11. 'ais' (0.049893)
  12. 'Â' (0.049881)
  13. 'AIN' (0.049851)
  14. 'nes' (0.049843)
  15. 'rezent' (0.049787)
  16. 'swer' (0.049746)
  17. 'answer' (0.049730)
  18. 'odes' (0.049716)
  19. 'attan' (0.049706)
  20. 'ши' (0.049623)

Layer  7 (after block 6) (entropy: 10.373):
   1. 'nab' (0.051168)
   2. 'stadt' (0.050782)
   3. 'ь' (0.050716)
   4. 'adt' (0.050642)
   5. 'swer' (0.050391)
   6. 'amber' (0.050257)
   7. 'Â' (0.050082)
   8. 'trag' (0.049963)
   9. 'unden' (0.049962)
  10. 'tober' (0.049876)
  11. 'Kot' (0.049865)
  12. 'ei' (0.049822)
  13. 'aises' (0.049696)
  14. 'stract' (0.049617)
  15. 'alike' (0.049616)
  16. 'civilian' (0.049584)
  17. 'nan' (0.049569)
  18. 'werken' (0.049550)
  19. 'ppe' (0.049426)
  20. 'nes' (0.049417)

Layer  8 (after block 7) (entropy: 10.373):
   1. 'amber' (0.051495)
   2. 'answer' (0.051022)
   3. 'anel' (0.050531)
   4. 'spy' (0.050237)
   5. 'alike' (0.050209)
   6. 'Answer' (0.050207)
   7. 'ner' (0.050097)
   8. 'stract' (0.049980)
   9. 'bekan' (0.049948)
  10. 'кта' (0.049879)
  11. 'adt' (0.049829)
  12. 'avia' (0.049807)
  13. 'ei' (0.049772)
  14. 'Gott' (0.049767)
  15. 'NER' (0.049664)
  16. 'nab' (0.049660)
  17. 'esis' (0.049610)
  18. 'stadt' (0.049518)
  19. 'ungen' (0.049409)
  20. 'ias' (0.049358)

Layer  9 (after block 8) (entropy: 10.373):
   1. 'answer' (0.052831)
   2. 'Answer' (0.051824)
   3. 'answered' (0.050988)
   4. 'unden' (0.050901)
   5. 'stadt' (0.050605)
   6. 'NER' (0.049954)
   7. 'owi' (0.049874)
   8. 'LE' (0.049819)
   9. 'aval' (0.049780)
  10. 'twice' (0.049708)
  11. 'seh' (0.049686)
  12. 'tour' (0.049584)
  13. 'igd' (0.049497)
  14. 'answering' (0.049492)
  15. 'yes' (0.049470)
  16. 'aris' (0.049371)
  17. 'amber' (0.049223)
  18. 'ilon' (0.049175)
  19. 'adrat' (0.049121)
  20. 'ema' (0.049097)

Layer 10 (after block 9) (entropy: 10.372):
   1. 'answer' (0.053801)
   2. 'Answer' (0.053630)
   3. 'unden' (0.051709)
   4. '/******/' (0.051456)
   5. 'answer' (0.050369)
   6. 'answered' (0.050279)
   7. 'stadt' (0.050268)
   8. 'ora' (0.050128)
   9. 'Arthur' (0.049662)
  10. 'answers' (0.049508)
  11. '答' (0.049291)
  12. 'icz' (0.049260)
  13. 'eph' (0.048955)
  14. 'iesz' (0.048940)
  15. 'iels' (0.048937)
  16. 'swer' (0.048883)
  17. 'än' (0.048796)
  18. 'ót' (0.048736)
  19. 'lio' (0.048712)
  20. 'ilia' (0.048679)

Layer 11 (after block 10) (entropy: 10.372):
   1. 'Answer' (0.053683)
   2. 'answer' (0.052934)
   3. 'stadt' (0.052026)
   4. 'rium' (0.051594)
   5. 'indow' (0.050197)
   6. 'edes' (0.050144)
   7. 'ír' (0.049955)
   8. 'answer' (0.049815)
   9. 'swer' (0.049694)
  10. 'answered' (0.049433)
  11. 'inder' (0.049411)
  12. 'astr' (0.049313)
  13. 'ora' (0.049221)
  14. '/******/' (0.049178)
  15. 'woh' (0.049135)
  16. 'amber' (0.048908)
  17. 'Zwe' (0.048880)
  18. 'iba' (0.048831)
  19. 'над' (0.048829)
  20. 'Arthur' (0.048821)

Layer 12 (after block 11) (entropy: 10.372):
   1. '/******/' (0.057096)
   2. 'шта' (0.051009)
   3. 'ilor' (0.050830)
   4. 'swer' (0.050750)
   5. 'amber' (0.050078)
   6. 'answer' (0.050070)
   7. 'zw' (0.050061)
   8. '带' (0.049843)
   9. 'Answer' (0.049757)
  10. 'stadt' (0.049718)
  11. 'än' (0.049649)
  12. 'cius' (0.049481)
  13. 'ześ' (0.049472)
  14. 'ír' (0.049313)
  15. 'spieler' (0.049162)
  16. 'een' (0.049116)
  17. 'comerc' (0.048715)
  18. 'imal' (0.048711)
  19. 'uilder' (0.048670)
  20. 'anel' (0.048501)

Layer 13 (after block 12) (entropy: 10.371):
   1. 'ír' (0.052360)
   2. 'Answer' (0.051435)
   3. 'шта' (0.051250)
   4. 'htt' (0.051122)
   5. 'swer' (0.051007)
   6. 'answer' (0.050768)
   7. 'arden' (0.050699)
   8. 'zw' (0.050407)
   9. 'iger' (0.049984)
  10. 'шти' (0.049834)
  11. 'än' (0.049592)
  12. 'град' (0.049520)
  13. 'Films' (0.049328)
  14. 'égal' (0.049238)
  15. 'stadt' (0.049166)
  16. 'answered' (0.049003)
  17. 'ischer' (0.048886)
  18. 'plural' (0.048870)
  19. 'unden' (0.048818)
  20. 'dru' (0.048713)

Layer 14 (after block 13) (entropy: 10.371):
   1. 'Answer' (0.053041)
   2. 'ieden' (0.052105)
   3. 'amber' (0.052052)
   4. 'ír' (0.051812)
   5. 'swer' (0.051132)
   6. '........' (0.051036)
   7. 'anel' (0.049853)
   8. 'officially' (0.049808)
   9. 'usch' (0.049756)
  10. 'htt' (0.049725)
  11. 'iger' (0.049692)
  12. '................' (0.049416)
  13. '...' (0.049404)
  14. 'answer' (0.049324)
  15. '…' (0.048989)
  16. 'ът' (0.048735)
  17. 'ниш' (0.048548)
  18. 'orent' (0.048540)
  19. 'yes' (0.048539)
  20. '花' (0.048494)

Layer 15 (after block 14) (entropy: 10.370):
   1. 'Answer' (0.056669)
   2. 'answer' (0.052341)
   3. 'usch' (0.052055)
   4. '........' (0.051781)
   5. '……' (0.050590)
   6. 'ammen' (0.049997)
   7. 'officially' (0.049893)
   8. 'swer' (0.049679)
   9. 'amber' (0.049625)
  10. 'kommun' (0.049532)
  11. '…' (0.049344)
  12. 'iger' (0.049277)
  13. 'orent' (0.049250)
  14. '…' (0.049130)
  15. '...' (0.048995)
  16. 'zie' (0.048812)
  17. '….' (0.048519)
  18. 'htt' (0.048185)
  19. '....' (0.048171)
  20. 'ниш' (0.048156)

Layer 16 (after block 15) (entropy: 10.368):
   1. 'Answer' (0.057755)
   2. 'answer' (0.057245)
   3. 'cities' (0.052358)
   4. 'amber' (0.052151)
   5. 'answered' (0.051231)
   6. '........' (0.051008)
   7. 'WC' (0.050824)
   8. 'answer' (0.049903)
   9. 'swer' (0.049125)
  10. '……' (0.049003)
  11. 'Computer' (0.048934)
  12. 'officially' (0.048831)
  13. 'citizen' (0.047897)
  14. '答' (0.047811)
  15. 'Neither' (0.047777)
  16. '…' (0.047762)
  17. 'none' (0.047691)
  18. 'zie' (0.047600)
  19. 'ieden' (0.047571)
  20. 'kommun' (0.047526)

Layer 17 (after block 16) (entropy: 10.364):
   1. 'Answer' (0.061976)
   2. 'answer' (0.060759)
   3. 'cities' (0.055960)
   4. 'swer' (0.054048)
   5. 'answered' (0.052227)
   6. 'answer' (0.052071)
   7. '答' (0.050844)
   8. 'answ' (0.049172)
   9. 'ieden' (0.049020)
  10. 'qpoint' (0.047785)
  11. 'opyright' (0.047407)
  12. 'unden' (0.047238)
  13. 'ště' (0.047233)
  14. 'iem' (0.047216)
  15. 'orent' (0.046977)
  16. 'ammen' (0.046421)
  17. 'answering' (0.046034)
  18. 'tober' (0.045993)
  19. '........' (0.045837)
  20. 'esen' (0.045785)

Layer 18 (after block 17) (entropy: 10.363):
   1. 'cities' (0.061433)
   2. 'Answer' (0.060236)
   3. 'answer' (0.059000)
   4. 'unden' (0.053356)
   5. 'headquarters' (0.053022)
   6. 'swer' (0.051029)
   7. 'answer' (0.050147)
   8. 'towns' (0.049858)
   9. 'Capital' (0.049651)
  10. '…' (0.047960)
  11. '答' (0.047831)
  12. '……' (0.047616)
  13. 'None' (0.047325)
  14. 'officially' (0.047062)
  15. 'capital' (0.046997)
  16. 'answered' (0.045882)
  17. 'esch' (0.045827)
  18. 'città' (0.045713)
  19. 'Probably' (0.045102)
  20. 'city' (0.044953)

Layer 19 (after block 18) (entropy: 10.359):
   1. 'cities' (0.064383)
   2. 'Germany' (0.058782)
   3. 'Answer' (0.058428)
   4. 'answer' (0.057022)
   5. 'headquarters' (0.052395)
   6. 'Berlin' (0.051517)
   7. 'swer' (0.051242)
   8. 'Frankfurt' (0.050752)
   9. 'answer' (0.047837)
  10. 'towns' (0.047819)
  11. 'opyright' (0.047357)
  12. 'esch' (0.047338)
  13. 'Deutschland' (0.046930)
  14. 'Capital' (0.046923)
  15. '…' (0.046423)
  16. '➤' (0.046117)
  17. 'capital' (0.045734)
  18. '……' (0.044728)
  19. 'answered' (0.044702)
  20. 'Neither' (0.043572)

Layer 20 (after block 19) (entropy: 10.350):
   1. 'cities' (0.073063)
   2. 'capital' (0.068304)
   3. 'Germany' (0.067088)
   4. 'Capital' (0.061624)
   5. 'Berlin' (0.055686)
   6. 'answer' (0.051954)
   7. 'capit' (0.049286)
   8. 'city' (0.046194)
   9. 'Answer' (0.046170)
  10. '/******/' (0.045683)
  11. 'Deutschland' (0.045272)
  12. 'Frankfurt' (0.044607)
  13. 'parliament' (0.044392)
  14. 'Parliament' (0.044185)
  15. 'headquarters' (0.043747)
  16. '…' (0.043272)
  17. 'towns' (0.042899)
  18. 'swer' (0.042843)
  19. 'opyright' (0.042109)
  20. 'Washington' (0.041622)

Layer 21 (after block 20) (entropy: 10.341):
   1. 'capital' (0.084407)
   2. 'Berlin' (0.076974)
   3. 'Capital' (0.076951)
   4. 'Germany' (0.072535)
   5. 'cities' (0.064610)
   6. 'capit' (0.055240)
   7. 'answer' (0.048130)
   8. 'Deutschland' (0.044322)
   9. 'Frankfurt' (0.043295)
  10. 'Parliament' (0.042925)
  11. 'Washington' (0.042151)
  12. 'parliament' (0.041972)
  13. 'Answer' (0.041076)
  14. 'Paris' (0.039964)
  15. 'city' (0.038552)
  16. 'Government' (0.038080)
  17. 'Tokyo' (0.037851)
  18. 'politicians' (0.037120)
  19. 'Leip' (0.037013)
  20. 'Probably' (0.036834)

Layer 22 (after block 21) (entropy: 10.336):
   1. 'Berlin' (0.108113)
   2. 'Germany' (0.092165)
   3. 'capital' (0.071215)
   4. 'Capital' (0.064941)
   5. 'Frankfurt' (0.054608)
   6. 'cities' (0.050613)
   7. 'capit' (0.048470)
   8. 'Paris' (0.045876)
   9. 'Washington' (0.045737)
  10. 'Deutschland' (0.043124)
  11. 'answer' (0.042495)
  12. 'parliament' (0.038602)
  13. 'Parliament' (0.038594)
  14. 'German' (0.037379)
  15. 'London' (0.036943)
  16. 'Tokyo' (0.036855)
  17. 'Answer' (0.036684)
  18. 'Leip' (0.036275)
  19. 'None' (0.036018)
  20. 'technically' (0.035291)

Layer 23 (after block 22) (entropy: 10.328):
   1. 'Washington' (0.127034)
   2. 'Berlin' (0.118623)
   3. 'capital' (0.068374)
   4. 'Germany' (0.065104)
   5. 'Capital' (0.063238)
   6. 'capit' (0.047950)
   7. 'Frankfurt' (0.044381)
   8. 'washing' (0.042344)
   9. 'Paris' (0.041358)
  10. 'Beijing' (0.040181)
  11. 'Rome' (0.036856)
  12. 'cities' (0.036679)
  13. 'London' (0.036663)
  14. 'Moscow' (0.036272)
  15. 'ashington' (0.033631)
  16. 'DC' (0.032675)
  17. 'parliament' (0.032492)
  18. 'Tokyo' (0.032383)
  19. 'DC' (0.032171)
  20. 'Parliament' (0.031591)

Layer 24 (after block 23) (entropy: 10.321):
   1. 'Berlin' (0.149095)
   2. 'Washington' (0.114304)
   3. 'Germany' (0.100914)
   4. 'capital' (0.058130)
   5. 'Frankfurt' (0.051375)
   6. 'Capital' (0.047111)
   7. 'washing' (0.039458)
   8. 'German' (0.038921)
   9. 'capit' (0.038682)
  10. 'Paris' (0.038541)
  11. 'Deutschland' (0.036601)
  12. 'Beijing' (0.035446)
  13. 'Germans' (0.033566)
  14. 'cities' (0.032949)
  15. 'Ber' (0.032756)
  16. 'Moscow' (0.031669)
  17. 'Rome' (0.031150)
  18. 'Tokyo' (0.029990)
  19. 'Parliament' (0.029716)
  20. 'Jerusalem' (0.029627)

Layer 25 (after block 24) (entropy: 10.288):
   1. 'Berlin' (0.270170)
   2. 'Germany' (0.133785)
   3. 'capital' (0.063489)
   4. 'Frankfurt' (0.051486)
   5. 'German' (0.050144)
   6. 'Washington' (0.047518)
   7. 'Capital' (0.046764)
   8. 'Deutschland' (0.037518)
   9. 'Germans' (0.037318)
  10. 'capit' (0.034625)
  11. 'Ber' (0.029485)
  12. 'Paris' (0.026763)
  13. 'germ' (0.023428)
  14. 'Tokyo' (0.023154)
  15. 'Leip' (0.022787)
  16. 'Germ' (0.021708)
  17. 'Rome' (0.021342)
  18. 'Hamburg' (0.019637)
  19. 'cities' (0.019566)
  20. 'Moscow' (0.019313)

Layer 26 (after block 25) (entropy: 10.264):
   1. 'Berlin' (0.351422)
   2. 'Germany' (0.121684)
   3. 'Frankfurt' (0.053391)
   4. 'capital' (0.049782)
   5. 'German' (0.047700)
   6. 'Washington' (0.036484)
   7. 'Germans' (0.035235)
   8. 'Capital' (0.034878)
   9. 'Ber' (0.032943)
  10. 'Deutschland' (0.031491)
  11. 'Paris' (0.030423)
  12. 'Tokyo' (0.023086)
  13. 'Leip' (0.021705)
  14. 'germ' (0.020397)
  15. 'capit' (0.020238)
  16. 'Hamburg' (0.020098)
  17. 'Beijing' (0.018790)
  18. 'Germ' (0.017232)
  19. 'cities' (0.017174)
  20. 'Moscow' (0.015848)

Layer 27 (after block 26) (entropy: 10.227):
   1. 'Berlin' (0.481452)
   2. 'Germany' (0.103621)
   3. 'Ber' (0.050110)
   4. 'Frankfurt' (0.049546)
   5. 'German' (0.034118)
   6. 'capital' (0.027452)
   7. 'Deutschland' (0.026577)
   8. 'Germans' (0.023268)
   9. 'Paris' (0.020218)
  10. 'Washington' (0.019897)
  11. 'Capital' (0.018531)
  12. 'ber' (0.018446)
  13. 'Ber' (0.018176)
  14. 'Tokyo' (0.017723)
  15. 'Hamburg' (0.016513)
  16. 'germ' (0.015922)
  17. 'Leip' (0.015625)
  18. 'Бер' (0.014330)
  19. 'BER' (0.014263)
  20. 'cities' (0.014212)

Layer 28 (after block 27) (entropy: 10.048):
   1. 'Berlin' (0.554646)
   2. 'Ber' (0.110567)
   3. 'Ber' (0.058083)
   4. 'ber' (0.053709)
   5. 'Germany' (0.045039)
   6. 'Бер' (0.022609)
   7. 'Frankfurt' (0.022103)
   8. 'BER' (0.020941)
   9. 'ber' (0.014341)
  10. 'German' (0.013960)
  11. 'capital' (0.012667)
  12. 'Deutschland' (0.011230)
  13. 'Germans' (0.008856)
  14. 'Capital' (0.007934)
  15. 'Washington' (0.007804)
  16. 'Bern' (0.007774)
  17. 'Tokyo' (0.007490)
  18. 'Paris' (0.006996)
  19. 'It' (0.006728)
  20. 'There' (0.006524)

Layer 29 (after block 28) (entropy: 9.726):
   1. 'Berlin' (0.621414)
   2. 'Ber' (0.088798)
   3. 'Ber' (0.051635)
   4. 'ber' (0.041674)
   5. 'Germany' (0.039523)
   6. 'Frankfurt' (0.017671)
   7. 'German' (0.014486)
   8. 'It' (0.013063)
   9. 'There' (0.012500)
  10. 'BER' (0.012182)
  11. 'Бер' (0.011969)
  12. 'Bon' (0.011114)
  13. 'ber' (0.010811)
  14. 'capital' (0.010490)
  15. 'The' (0.009972)
  16. 'it' (0.007319)
  17. 'well' (0.006784)
  18. 'there' (0.006599)
  19. 'Its' (0.006044)
  20. 'That' (0.005952)

Layer 30 (after block 29) (entropy: 9.197):
   1. 'Berlin' (0.629301)
   2. 'Germany' (0.060026)
   3. 'Ber' (0.045920)
   4. 'It' (0.029740)
   5. 'Ber' (0.029413)
   6. 'The' (0.029329)
   7. 'There' (0.024876)
   8. 'Frankfurt' (0.016358)
   9. 'German' (0.016084)
  10. 'ber' (0.015073)
  11. 'Bon' (0.012412)
  12. 'That' (0.012408)
  13. 'This' (0.011438)
  14. 'it' (0.010435)
  15. 'If' (0.010175)
  16. '(' (0.009968)
  17. 'What' (0.009593)
  18. 'well' (0.009426)
  19. 'there' (0.009152)
  20. '
' (0.008876)

Layer 31 (after block 30) (entropy: 8.125):
   1. 'Berlin' (0.641896)
   2. 'Germany' (0.071239)
   3. 'The' (0.045269)
   4. 'It' (0.037659)
   5. 'There' (0.032504)
   6. 'That' (0.016822)
   7. 'Bon' (0.016268)
   8. 'Ber' (0.015515)
   9. 'This' (0.013224)
  10. 'None' (0.012320)
  11. 'If' (0.011284)
  12. 'What' (0.010726)
  13. 'In' (0.009873)
  14. '' (0.009677)
  15. 'Frankfurt' (0.009644)
  16. 'No' (0.009444)
  17. 'According' (0.009421)
  18. 'German' (0.009164)
  19. 'Well' (0.009126)
  20. '(' (0.008924)

Layer 32 (after block 31) (entropy: 5.721):
   1. 'Berlin' (0.646858)
   2. 'The' (0.054458)
   3. 'Germany' (0.050485)
   4. 'It' (0.042831)
   5. 'There' (0.029496)
   6. 'That' (0.017425)
   7. 'Bon' (0.014953)
   8. '
' (0.012944)
   9. 'Frankfurt' (0.012797)
  10. 'I' (0.012712)
  11. '' (0.011859)
  12. 'Well' (0.011520)
  13. 'B' (0.011519)
  14. 'This' (0.011192)
  15. 'What' (0.010955)
  16. 'A' (0.009761)
  17. 'If' (0.009731)
  18. 'No' (0.009661)
  19. 'You' (0.009543)
  20. '(' (0.009300)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 1.248):
   1. 'Berlin' (0.892544)
   2. 'The' (0.023425)
   3. 'Germany' (0.020954)
   4. 'It' (0.016453)
   5. 'There' (0.009505)
   6. 'That' (0.004382)
   7. 'Bon' (0.003499)
   8. '
' (0.002830)
   9. 'Frankfurt' (0.002783)
  10. 'I' (0.002756)
  11. '' (0.002488)
  12. 'Well' (0.002384)
  13. 'B' (0.002384)
  14. 'This' (0.002285)
  15. 'What' (0.002214)
  16. 'A' (0.001868)
  17. 'If' (0.001860)
  18. 'No' (0.001840)
  19. 'You' (0.001807)
  20. '(' (0.001740)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 4.628)
   1. 'a' (0.368515)
   2. 'one' (0.176034)
   3. 'the' (0.100546)
   4. 'known' (0.083822)
   5. 'home' (0.076729)
   6. 'an' (0.055293)
   7. 'not' (0.041666)
   8. 'famous' (0.041458)
   9. 'full' (0.035319)
  10. 'set' (0.020617)

Prompt: 'Berlin is the capital of'
  (entropy: 0.659)
   1. 'Germany' (0.923096)
   2. 'the' (0.055532)
   3. 'both' (0.004489)
   4. 'a' (0.003914)
   5. 'Europe' (0.003206)
   6. 'Berlin' (0.002925)
   7. 'German' (0.002514)
   8. 'modern' (0.001665)
   9. 'and' (0.001516)
  10. 'one' (0.001142)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 3.162)
   1. '
' (0.627396)
   2. 'Berlin' (0.199133)
   3. 'If' (0.037700)
   4. 'The' (0.031881)
   5. 'You' (0.020328)
   6. '(' (0.020017)
   7. '' (0.019896)
   8. 'What' (0.015288)
   9. 'Bon' (0.015036)
  10. 'I' (0.013326)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000)
   1. 'Berlin' (1.000000)
   2. 'The' (0.000000)
   3. 'Germany' (0.000000)
   4. 'It' (0.000000)
   5. 'There' (0.000000)
   6. 'That' (0.000000)
   7. 'Bon' (0.000000)
   8. '
' (0.000000)
   9. 'Frankfurt' (0.000000)
  10. 'I' (0.000000)
  11. '' (0.000000)
  12. 'Well' (0.000000)
  13. 'B' (0.000000)
  14. 'This' (0.000000)
  15. 'What' (0.000000)

Temperature 2.0:
  (entropy: 8.422)
   1. 'Berlin' (0.473678)
   2. 'The' (0.076737)
   3. 'Germany' (0.072577)
   4. 'It' (0.064311)
   5. 'There' (0.048882)
   6. 'That' (0.033190)
   7. 'Bon' (0.029657)
   8. '
' (0.026671)
   9. 'Frankfurt' (0.026449)
  10. 'I' (0.026319)
  11. '' (0.025009)
  12. 'Well' (0.024481)
  13. 'B' (0.024480)
  14. 'This' (0.023966)
  15. 'What' (0.023592)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 32
Model dimension: 4096
Number of heads: 32
Vocab size: 32000
Context length: 2048
=== END OF MODEL STATS ========

