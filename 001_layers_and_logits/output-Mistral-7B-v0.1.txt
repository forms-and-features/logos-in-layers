
============================================================
EVALUATING MODEL: mistralai/Mistral-7B-v0.1
============================================================
Loading model: mistralai/Mistral-7B-v0.1...
Loaded pretrained model mistralai/Mistral-7B-v0.1 into HookedTransformer

=== NORMALIZATION ANALYSIS ========
Block normalization type: RMSNormPre
Using NORMALISED residual stream (RMS, no learnable scale)
Final normalization type: RMSNormPre
=== END NORMALIZATION ANALYSIS ====


=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['<s>', 'Question', ':', 'What', 'is', 'the', 'capital', 'of', 'Germany', '?', 'Answer', ':']
Computing layer-wise predictions (memory-efficient targeted caching)...

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using NORMALIZED residual stream (RMS, no learnable scale)
Note: Shown probabilities are from full softmax (calibrated and comparable)
------------------------------------------------------------
Layer  0 (embeddings):
[diagnostic] No separate positional embedding hook; using only token embeddings for layer 0 residual.
  (entropy: 14.722 bits):
   1. 'laug' (0.065350)
   2. 'avax' (0.065067)
   3. 'auf' (0.062302)
   4. 'erre' (0.059691)
   5. 'voir' (0.056367)
   6. 'istribute' (0.054837)
   7. 'innen' (0.052384)
   8. 'ueto' (0.050185)
   9. 'aliment' (0.047929)
  10. 'onces' (0.046960)
  11. 'usqu' (0.046864)
  12. 'ettes' (0.046563)
  13. 'abi' (0.045932)
  14. 'ffen' (0.045856)
  15. 'holm' (0.043698)
  16. 'Sink' (0.043537)
  17. 'lette' (0.042145)
  18. 'EV' (0.041837)
  19. 'thous' (0.041421)
  20. 'serts' (0.041075)

Layer  1 (after transformer block 0) (entropy: 14.440 bits):
   1. 'zo' (0.079043)
   2. 'l' (0.067730)
   3. 'env' (0.066134)
   4. 'ladies' (0.061741)
   5. 'qu' (0.055599)
   6. 'dhd' (0.050354)
   7. 'wards' (0.049852)
   8. 'mur' (0.049729)
   9. 'ts' (0.048563)
  10. 'inf' (0.047248)
  11. 'dropped' (0.046651)
  12. 'sto' (0.045348)
  13. 'neighbor' (0.043567)
  14. 'listop' (0.042678)
  15. 'crashed' (0.042154)
  16. 'opt' (0.041334)
  17. 'jar' (0.041249)
  18. 'janu' (0.040974)
  19. 'licht' (0.040045)
  20. 'æ' (0.040007)

Layer  2 (after transformer block 1) (entropy: 14.049 bits):
   1. 'ts' (0.159134)
   2. 'richt' (0.096757)
   3. 'reck' (0.090630)
   4. 'thoroughly' (0.076800)
   5. 'ffen' (0.067277)
   6. 'rig' (0.062920)
   7. 'rike' (0.055910)
   8. 'tered' (0.043355)
   9. 'unk' (0.040365)
  10. 'aja' (0.037522)
  11. 'bal' (0.036228)
  12. 'oya' (0.031585)
  13. 'ts' (0.028981)
  14. 'quet' (0.027753)
  15. 'battery' (0.027008)
  16. 'soci' (0.025963)
  17. 'reck' (0.023983)
  18. 'disagree' (0.022903)
  19. 'abs' (0.022490)
  20. 'Ts' (0.022435)

Layer  3 (after transformer block 2) (entropy: 13.871 bits):
   1. 'richt' (0.192032)
   2. 'aiser' (0.104263)
   3. 'rig' (0.080260)
   4. 'ts' (0.067802)
   5. 'Â' (0.055016)
   6. 'reck' (0.049344)
   7. 'reck' (0.046520)
   8. 'lo' (0.045822)
   9. 'thoroughly' (0.045643)
  10. 'ffen' (0.043961)
  11. 'amber' (0.039755)
  12. 'pel' (0.034427)
  13. 'nevertheless' (0.029478)
  14. 'reich' (0.026568)
  15. 'serts' (0.026096)
  16. 'ENDOR' (0.023387)
  17. '�' (0.022503)
  18. 'ilen' (0.022501)
  19. 'Griff' (0.022358)
  20. 'anne' (0.022263)

Layer  4 (after transformer block 3) (entropy: 13.821 bits):
   1. 'amber' (0.150272)
   2. 'aiser' (0.091760)
   3. 'thoroughly' (0.065739)
   4. 'situ' (0.055566)
   5. 'ople' (0.054882)
   6. 'ves' (0.053986)
   7. 'lo' (0.052300)
   8. 'reck' (0.051782)
   9. 'rig' (0.049665)
  10. 'Â' (0.047575)
  11. 'reck' (0.040675)
  12. 'bes' (0.038013)
  13. 'pronounced' (0.036766)
  14. 'illes' (0.031694)
  15. 'anne' (0.030930)
  16. 'ilen' (0.030654)
  17. 'richt' (0.030593)
  18. 'ner' (0.030475)
  19. 'ENDOR' (0.028562)
  20. 'Coord' (0.028111)

Layer  5 (after transformer block 4) (entropy: 13.738 bits):
   1. 'aiser' (0.127433)
   2. 'stadt' (0.111880)
   3. 'amber' (0.065423)
   4. 'arta' (0.055301)
   5. 'ople' (0.052602)
   6. 'rien' (0.049642)
   7. 'ady' (0.049360)
   8. 'ffen' (0.047215)
   9. 'ieden' (0.044247)
  10. 'ais' (0.044184)
  11. 'stract' (0.043995)
  12. 'illes' (0.037029)
  13. 'haus' (0.035774)
  14. 'cert' (0.035322)
  15. 'tober' (0.034868)
  16. 'smith' (0.034255)
  17. 'iani' (0.034224)
  18. 'avia' (0.033214)
  19. 'located' (0.032017)
  20. 'odes' (0.032012)

Layer  6 (after transformer block 5) (entropy: 13.732 bits):
   1. 'amber' (0.087294)
   2. 'adt' (0.083557)
   3. 'tober' (0.081733)
   4. 'stadt' (0.065183)
   5. 'lagen' (0.059853)
   6. 'Answer' (0.059112)
   7. 'aises' (0.051575)
   8. 'precisely' (0.045887)
   9. 'rais' (0.043554)
  10. 'answered' (0.043189)
  11. 'ais' (0.042617)
  12. 'Â' (0.042082)
  13. 'AIN' (0.040798)
  14. 'nes' (0.040453)
  15. 'rezent' (0.038134)
  16. 'swer' (0.036543)
  17. 'answer' (0.035926)
  18. 'odes' (0.035383)
  19. 'attan' (0.035040)
  20. 'ши' (0.032084)

Layer  7 (after transformer block 6) (entropy: 13.636 bits):
   1. 'nab' (0.125586)
   2. 'stadt' (0.089760)
   3. 'ь' (0.084758)
   4. 'adt' (0.079455)
   5. 'swer' (0.063764)
   6. 'amber' (0.056657)
   7. 'Â' (0.048571)
   8. 'trag' (0.043711)
   9. 'unden' (0.043662)
  10. 'tober' (0.040469)
  11. 'Kot' (0.040058)
  12. 'ei' (0.038569)
  13. 'aises' (0.034470)
  14. 'stract' (0.032149)
  15. 'alike' (0.032096)
  16. 'civilian' (0.031209)
  17. 'nan' (0.030789)
  18. 'werken' (0.030279)
  19. 'ppe' (0.027103)
  20. 'nes' (0.026886)

Layer  8 (after transformer block 7) (entropy: 13.616 bits):
   1. 'amber' (0.141231)
   2. 'answer' (0.099203)
   3. 'anel' (0.068512)
   4. 'spy' (0.054811)
   5. 'alike' (0.053656)
   6. 'Answer' (0.053563)
   7. 'ner' (0.049237)
   8. 'stract' (0.045035)
   9. 'bekan' (0.043961)
  10. 'кта' (0.041693)
  11. 'adt' (0.040096)
  12. 'avia' (0.039427)
  13. 'ei' (0.038397)
  14. 'Gott' (0.038253)
  15. 'NER' (0.035330)
  16. 'nab' (0.035223)
  17. 'esis' (0.033897)
  18. 'stadt' (0.031569)
  19. 'ungen' (0.029022)
  20. 'ias' (0.027884)

Layer  9 (after transformer block 8) (entropy: 13.582 bits):
   1. 'answer' (0.215598)
   2. 'Answer' (0.120361)
   3. 'answered' (0.073513)
   4. 'unden' (0.069821)
   5. 'stadt' (0.058519)
   6. 'NER' (0.039537)
   7. 'owi' (0.037655)
   8. 'LE' (0.036404)
   9. 'aval' (0.035569)
  10. 'twice' (0.034038)
  11. 'seh' (0.033579)
  12. 'tour' (0.031558)
  13. 'igd' (0.029918)
  14. 'answering' (0.029834)
  15. 'yes' (0.029421)
  16. 'aris' (0.027689)
  17. 'amber' (0.025280)
  18. 'ilon' (0.024558)
  19. 'adrat' (0.023745)
  20. 'ema' (0.023402)

Layer 10 (after transformer block 9) (entropy: 13.322 bits):
   1. 'answer' (0.228539)
   2. 'Answer' (0.210442)
   3. 'unden' (0.081759)
   4. '/******/' (0.072015)
   5. 'answer' (0.041397)
   6. 'answered' (0.039523)
   7. 'stadt' (0.039308)
   8. 'ora' (0.036569)
   9. 'Arthur' (0.028695)
  10. 'answers' (0.026477)
  11. '答' (0.023636)
  12. 'icz' (0.023249)
  13. 'eph' (0.019794)
  14. 'iesz' (0.019634)
  15. 'iels' (0.019608)
  16. 'swer' (0.019047)
  17. 'än' (0.018196)
  18. 'ót' (0.017625)
  19. 'lio' (0.017396)
  20. 'ilia' (0.017091)

Layer 11 (after transformer block 10) (entropy: 13.479 bits):
   1. 'Answer' (0.196084)
   2. 'answer' (0.142857)
   3. 'stadt' (0.096768)
   4. 'rium' (0.080230)
   5. 'indow' (0.043223)
   6. 'edes' (0.042221)
   7. 'ír' (0.038767)
   8. 'answer' (0.036400)
   9. 'swer' (0.034458)
  10. 'answered' (0.030604)
  11. 'inder' (0.030301)
  12. 'astr' (0.028978)
  13. 'ora' (0.027785)
  14. '/******/' (0.027241)
  15. 'woh' (0.026711)
  16. 'amber' (0.024064)
  17. 'Zwe' (0.023756)
  18. 'iba' (0.023224)
  19. 'над' (0.023206)
  20. 'Arthur' (0.023122)

Layer 12 (after transformer block 11) (entropy: 13.298 bits):
   1. '/******/' (0.508019)
   2. 'шта' (0.044677)
   3. 'ilor' (0.041417)
   4. 'swer' (0.040028)
   5. 'amber' (0.030029)
   6. 'answer' (0.029926)
   7. 'zw' (0.029811)
   8. '带' (0.027134)
   9. 'Answer' (0.026143)
  10. 'stadt' (0.025710)
  11. 'än' (0.024945)
  12. 'cius' (0.023185)
  13. 'ześ' (0.023094)
  14. 'ír' (0.021549)
  15. 'spieler' (0.020165)
  16. 'een' (0.019770)
  17. 'comerc' (0.016561)
  18. 'imal' (0.016534)
  19. 'uilder' (0.016238)
  20. 'anel' (0.015065)

Layer 13 (after transformer block 12) (entropy: 13.657 bits):
   1. 'ír' (0.117317)
   2. 'Answer' (0.081515)
   3. 'шта' (0.075716)
   4. 'htt' (0.071940)
   5. 'swer' (0.068725)
   6. 'answer' (0.062426)
   7. 'arden' (0.060733)
   8. 'zw' (0.053968)
   9. 'iger' (0.045428)
  10. 'шти' (0.042726)
  11. 'än' (0.038685)
  12. 'град' (0.037555)
  13. 'Films' (0.034686)
  14. 'égal' (0.033419)
  15. 'stadt' (0.032428)
  16. 'answered' (0.030303)
  17. 'ischer' (0.028862)
  18. 'plural' (0.028670)
  19. 'unden' (0.028050)
  20. 'dru' (0.026847)

Layer 14 (after transformer block 13) (entropy: 13.668 bits):
   1. 'Answer' (0.123424)
   2. 'ieden' (0.090964)
   3. 'amber' (0.089381)
   4. 'ír' (0.082581)
   5. 'swer' (0.065846)
   6. '........' (0.063778)
   7. 'anel' (0.042657)
   8. 'officially' (0.042006)
   9. 'usch' (0.041262)
  10. 'htt' (0.040820)
  11. 'iger' (0.040356)
  12. '................' (0.036689)
  13. '...' (0.036529)
  14. 'answer' (0.035536)
  15. '…' (0.031614)
  16. 'ът' (0.028923)
  17. 'ниш' (0.027077)
  18. 'orent' (0.027001)
  19. 'yes' (0.026993)
  20. '花' (0.026565)

Layer 15 (after transformer block 14) (entropy: 13.553 bits):
   1. 'Answer' (0.263965)
   2. 'answer' (0.079964)
   3. 'usch' (0.073631)
   4. '........' (0.068021)
   5. '……' (0.047942)
   6. 'ammen' (0.040158)
   7. 'officially' (0.038919)
   8. 'swer' (0.036475)
   9. 'amber' (0.035888)
  10. 'kommun' (0.034885)
  11. '…' (0.032953)
  12. 'iger' (0.032287)
  13. 'orent' (0.032022)
  14. '…' (0.030864)
  15. '...' (0.029613)
  16. 'zie' (0.027995)
  17. '….' (0.025571)
  18. 'htt' (0.023055)
  19. '....' (0.022948)
  20. 'ниш' (0.022844)

Layer 16 (after transformer block 15) (entropy: 13.436 bits):
   1. 'Answer' (0.225208)
   2. 'answer' (0.201273)
   3. 'cities' (0.064932)
   4. 'amber' (0.061755)
   5. 'answered' (0.049286)
   6. '........' (0.046631)
   7. 'WC' (0.044541)
   8. 'answer' (0.035324)
   9. 'swer' (0.028947)
  10. '……' (0.028045)
  11. 'Computer' (0.027552)
  12. 'officially' (0.026821)
  13. 'citizen' (0.020996)
  14. '答' (0.020524)
  15. 'Neither' (0.020339)
  16. '…' (0.020259)
  17. 'none' (0.019880)
  18. 'zie' (0.019405)
  19. 'ieden' (0.019257)
  20. 'kommun' (0.019026)

Layer 17 (after transformer block 16) (entropy: 12.921 bits):
   1. 'Answer' (0.266628)
   2. 'answer' (0.218335)
   3. 'cities' (0.095286)
   4. 'swer' (0.067130)
   5. 'answered' (0.047521)
   6. 'answer' (0.046108)
   7. '答' (0.036258)
   8. 'answ' (0.025889)
   9. 'ieden' (0.025090)
  10. 'qpoint' (0.019402)
  11. 'opyright' (0.017910)
  12. 'unden' (0.017277)
  13. 'ště' (0.017257)
  14. 'iem' (0.017196)
  15. 'orent' (0.016337)
  16. 'ammen' (0.014491)
  17. 'answering' (0.013318)
  18. 'tober' (0.013199)
  19. '........' (0.012757)
  20. 'esen' (0.012611)

Layer 18 (after transformer block 17) (entropy: 13.221 bits):
   1. 'cities' (0.216302)
   2. 'Answer' (0.180992)
   3. 'answer' (0.149974)
   4. 'unden' (0.060295)
   5. 'headquarters' (0.056953)
   6. 'swer' (0.040244)
   7. 'answer' (0.034366)
   8. 'towns' (0.032608)
   9. 'Capital' (0.031405)
  10. '…' (0.022942)
  11. '答' (0.022389)
  12. '……' (0.021491)
  13. 'None' (0.020329)
  14. 'officially' (0.019327)
  15. 'capital' (0.019086)
  16. 'answered' (0.015356)
  17. 'esch' (0.015189)
  18. 'città' (0.014848)
  19. 'Probably' (0.013146)
  20. 'city' (0.012757)

Layer 19 (after transformer block 18) (entropy: 12.736 bits):
   1. 'cities' (0.250466)
   2. 'Germany' (0.123310)
   3. 'Answer' (0.117653)
   4. 'answer' (0.097332)
   5. 'headquarters' (0.050366)
   6. 'Berlin' (0.044155)
   7. 'swer' (0.042353)
   8. 'Frankfurt' (0.039302)
   9. 'answer' (0.024797)
  10. 'towns' (0.024725)
  11. 'opyright' (0.022925)
  12. 'esch' (0.022852)
  13. 'Deutschland' (0.021363)
  14. 'Capital' (0.021341)
  15. '…' (0.019632)
  16. '➤' (0.018647)
  17. 'capital' (0.017473)
  18. '……' (0.014695)
  19. 'answered' (0.014628)
  20. 'Neither' (0.011985)

Layer 20 (after transformer block 19) (entropy: 11.430 bits):
   1. 'cities' (0.282674)
   2. 'capital' (0.185554)
   3. 'Germany' (0.165852)
   4. 'Capital' (0.097538)
   5. 'Berlin' (0.051781)
   6. 'answer' (0.033568)
   7. 'capit' (0.024145)
   8. 'city' (0.016107)
   9. 'Answer' (0.016055)
  10. '/******/' (0.015024)
  11. 'Deutschland' (0.014200)
  12. 'Frankfurt' (0.012945)
  13. 'parliament' (0.012561)
  14. 'Parliament' (0.012199)
  15. 'headquarters' (0.011463)
  16. '…' (0.010707)
  17. 'towns' (0.010142)
  18. 'swer' (0.010060)
  19. 'opyright' (0.009030)
  20. 'Washington' (0.008397)

Layer 21 (after transformer block 20) (entropy: 8.484 bits):
   1. 'capital' (0.302961)
   2. 'Berlin' (0.184660)
   3. 'Capital' (0.184369)
   4. 'Germany' (0.134231)
   5. 'cities' (0.072109)
   6. 'capit' (0.031084)
   7. 'answer' (0.014832)
   8. 'Deutschland' (0.009527)
   9. 'Frankfurt' (0.008400)
  10. 'Parliament' (0.008021)
  11. 'Washington' (0.007275)
  12. 'parliament' (0.007110)
  13. 'Answer' (0.006333)
  14. 'Paris' (0.005464)
  15. 'city' (0.004505)
  16. 'Government' (0.004216)
  17. 'Tokyo' (0.004082)
  18. 'politicians' (0.003676)
  19. 'Leip' (0.003619)
  20. 'Probably' (0.003526)

Layer 22 (after transformer block 21) (entropy: 6.564 bits):
   1. 'Berlin' (0.542577)
   2. 'Germany' (0.247804)
   3. 'capital' (0.069852)
   4. 'Capital' (0.044411)
   5. 'Frankfurt' (0.018962)
   6. 'cities' (0.013058)
   7. 'capit' (0.010559)
   8. 'Paris' (0.008060)
   9. 'Washington' (0.007940)
  10. 'Deutschland' (0.005948)
  11. 'answer' (0.005534)
  12. 'parliament' (0.003453)
  13. 'Parliament' (0.003449)
  14. 'German' (0.002948)
  15. 'London' (0.002783)
  16. 'Tokyo' (0.002750)
  17. 'Answer' (0.002688)
  18. 'Leip' (0.002544)
  19. 'None' (0.002457)
  20. 'technically' (0.002223)

Layer 23 (after transformer block 22) (entropy: 3.159 bits):
   1. 'Washington' (0.511929)
   2. 'Berlin' (0.375689)
   3. 'capital' (0.031190)
   4. 'Germany' (0.024994)
   5. 'Capital' (0.021918)
   6. 'capit' (0.006279)
   7. 'Frankfurt' (0.004428)
   8. 'washing' (0.003581)
   9. 'Paris' (0.003220)
  10. 'Beijing' (0.002826)
  11. 'Rome' (0.001913)
  12. 'cities' (0.001872)
  13. 'London' (0.001868)
  14. 'Moscow' (0.001780)
  15. 'ashington' (0.001265)
  16. 'DC' (0.001110)
  17. 'parliament' (0.001083)
  18. 'Tokyo' (0.001066)
  19. 'DC' (0.001035)
  20. 'Parliament' (0.000953)

Layer 24 (after transformer block 23) (entropy: 2.882 bits):
   1. 'Berlin' (0.630573)
   2. 'Washington' (0.205369)
   3. 'Germany' (0.121365)
   4. 'capital' (0.011825)
   5. 'Frankfurt' (0.007019)
   6. 'Capital' (0.004869)
   7. 'washing' (0.002304)
   8. 'German' (0.002174)
   9. 'capit' (0.002118)
  10. 'Paris' (0.002086)
  11. 'Deutschland' (0.001677)
  12. 'Beijing' (0.001465)
  13. 'Germans' (0.001164)
  14. 'cities' (0.001076)
  15. 'Ber' (0.001050)
  16. 'Moscow' (0.000910)
  17. 'Rome' (0.000849)
  18. 'Tokyo' (0.000723)
  19. 'Parliament' (0.000696)
  20. 'Jerusalem' (0.000687)

Layer 25 (after transformer block 24) (entropy: 0.558 bits):
   1. 'Berlin' (0.920277)
   2. 'Germany' (0.066725)
   3. 'capital' (0.004127)
   4. 'Frankfurt' (0.001887)
   5. 'German' (0.001710)
   6. 'Washington' (0.001399)
   7. 'Capital' (0.001318)
   8. 'Deutschland' (0.000579)
   9. 'Germans' (0.000568)
  10. 'capit' (0.000429)
  11. 'Ber' (0.000235)
  12. 'Paris' (0.000164)
  13. 'germ' (0.000100)
  14. 'Tokyo' (0.000096)
  15. 'Leip' (0.000090)
  16. 'Germ' (0.000075)
  17. 'Rome' (0.000070)
  18. 'Hamburg' (0.000052)
  19. 'cities' (0.000051)
  20. 'Moscow' (0.000049)

Layer 26 (after transformer block 25) (entropy: 0.282 bits):
   1. 'Berlin' (0.969429)
   2. 'Germany' (0.024835)
   3. 'Frankfurt' (0.001442)
   4. 'capital' (0.001132)
   5. 'German' (0.000977)
   6. 'Washington' (0.000387)
   7. 'Germans' (0.000343)
   8. 'Capital' (0.000331)
   9. 'Ber' (0.000272)
  10. 'Deutschland' (0.000233)
  11. 'Paris' (0.000206)
  12. 'Tokyo' (0.000080)
  13. 'Leip' (0.000064)
  14. 'germ' (0.000052)
  15. 'capit' (0.000050)
  16. 'Hamburg' (0.000049)
  17. 'Beijing' (0.000039)
  18. 'Germ' (0.000029)
  19. 'cities' (0.000029)
  20. 'Moscow' (0.000022)

Layer 27 (after transformer block 26) (entropy: 0.104 bits):
   1. 'Berlin' (0.991145)
   2. 'Germany' (0.006890)
   3. 'Ber' (0.000657)
   4. 'Frankfurt' (0.000633)
   5. 'German' (0.000190)
   6. 'capital' (0.000094)
   7. 'Deutschland' (0.000084)
   8. 'Germans' (0.000055)
   9. 'Paris' (0.000035)
  10. 'Washington' (0.000033)
  11. 'Capital' (0.000026)
  12. 'ber' (0.000026)
  13. 'Ber' (0.000025)
  14. 'Tokyo' (0.000023)
  15. 'Hamburg' (0.000018)
  16. 'germ' (0.000016)
  17. 'Leip' (0.000015)
  18. 'Бер' (0.000011)
  19. 'BER' (0.000011)
  20. 'cities' (0.000011)

Layer 28 (after transformer block 27) (entropy: 0.135 bits):
   1. 'Berlin' (0.985930)
   2. 'Ber' (0.009989)
   3. 'Ber' (0.001598)
   4. 'ber' (0.001278)
   5. 'Germany' (0.000774)
   6. 'Бер' (0.000109)
   7. 'Frankfurt' (0.000102)
   8. 'BER' (0.000087)
   9. 'ber' (0.000030)
  10. 'German' (0.000028)
  11. 'capital' (0.000021)
  12. 'Deutschland' (0.000015)
  13. 'Germans' (0.000008)
  14. 'Capital' (0.000006)
  15. 'Washington' (0.000005)
  16. 'Bern' (0.000005)
  17. 'Tokyo' (0.000005)
  18. 'Paris' (0.000004)
  19. 'It' (0.000003)
  20. 'There' (0.000003)

Layer 29 (after transformer block 28) (entropy: 0.123 bits):
   1. 'Berlin' (0.988693)
   2. 'Ber' (0.007046)
   3. 'Ber' (0.001777)
   4. 'ber' (0.001031)
   5. 'Germany' (0.000901)
   6. 'Frankfurt' (0.000117)
   7. 'German' (0.000070)
   8. 'It' (0.000054)
   9. 'There' (0.000048)
  10. 'BER' (0.000045)
  11. 'Бер' (0.000043)
  12. 'Bon' (0.000036)
  13. 'ber' (0.000033)
  14. 'capital' (0.000031)
  15. 'The' (0.000027)
  16. 'it' (0.000012)
  17. 'well' (0.000010)
  18. 'there' (0.000010)
  19. 'Its' (0.000008)
  20. 'That' (0.000007)

Layer 30 (after transformer block 29) (entropy: 0.352 bits):
   1. 'Berlin' (0.979748)
   2. 'Germany' (0.007116)
   3. 'Ber' (0.004059)
   4. 'It' (0.001633)
   5. 'Ber' (0.001596)
   6. 'The' (0.001586)
   7. 'There' (0.001123)
   8. 'Frankfurt' (0.000467)
   9. 'German' (0.000450)
  10. 'ber' (0.000393)
  11. 'Bon' (0.000262)
  12. 'That' (0.000261)
  13. 'This' (0.000220)
  14. 'it' (0.000182)
  15. 'If' (0.000172)
  16. '(' (0.000165)
  17. 'What' (0.000152)
  18. 'well' (0.000147)
  19. 'there' (0.000138)
  20. '
' (0.000130)

Layer 31 (after transformer block 30) (entropy: 0.791 bits):
   1. 'Berlin' (0.954919)
   2. 'Germany' (0.017635)
   3. 'The' (0.007742)
   4. 'It' (0.005542)
   5. 'There' (0.004243)
   6. 'That' (0.001283)
   7. 'Bon' (0.001207)
   8. 'Ber' (0.001108)
   9. 'This' (0.000829)
  10. 'None' (0.000729)
  11. 'If' (0.000621)
  12. 'What' (0.000567)
  13. 'In' (0.000488)
  14. '' (0.000470)
  15. 'Frankfurt' (0.000467)
  16. 'No' (0.000450)
  17. 'According' (0.000448)
  18. 'German' (0.000426)
  19. 'Well' (0.000423)
  20. '(' (0.000406)

Layer 32 (after transformer block 31) (entropy: 1.800 bits):
   1. 'Berlin' (0.892544)
   2. 'The' (0.023425)
   3. 'Germany' (0.020954)
   4. 'It' (0.016453)
   5. 'There' (0.009505)
   6. 'That' (0.004382)
   7. 'Bon' (0.003499)
   8. '
' (0.002830)
   9. 'Frankfurt' (0.002783)
  10. 'I' (0.002756)
  11. '' (0.002488)
  12. 'Well' (0.002384)
  13. 'B' (0.002384)
  14. 'This' (0.002285)
  15. 'What' (0.002214)
  16. 'A' (0.001868)
  17. 'If' (0.001860)
  18. 'No' (0.001840)
  19. 'You' (0.001807)
  20. '(' (0.001740)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 1.800 bits):
   1. 'Berlin' (0.831533)
   2. 'The' (0.021823)
   3. 'Germany' (0.019522)
   4. 'It' (0.015328)
   5. 'There' (0.008855)
   6. 'That' (0.004083)
   7. 'Bon' (0.003260)
   8. '
' (0.002636)
   9. 'Frankfurt' (0.002593)
  10. 'I' (0.002567)
  11. '' (0.002318)
  12. 'Well' (0.002221)
  13. 'B' (0.002221)
  14. 'This' (0.002129)
  15. 'What' (0.002063)
  16. 'A' (0.001741)
  17. 'If' (0.001733)
  18. 'No' (0.001714)
  19. 'You' (0.001684)
  20. '(' (0.001621)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 6.677 bits)
   1. 'a' (0.187836)
   2. 'one' (0.089726)
   3. 'the' (0.051249)
   4. 'known' (0.042725)
   5. 'home' (0.039110)
   6. 'an' (0.028183)
   7. 'not' (0.021238)
   8. 'famous' (0.021132)
   9. 'full' (0.018003)
  10. 'set' (0.010509)

Prompt: 'Berlin is the capital of'
  (entropy: 0.950 bits)
   1. 'Germany' (0.896618)
   2. 'the' (0.053939)
   3. 'both' (0.004361)
   4. 'a' (0.003802)
   5. 'Europe' (0.003114)
   6. 'Berlin' (0.002841)
   7. 'German' (0.002442)
   8. 'modern' (0.001617)
   9. 'and' (0.001473)
  10. 'one' (0.001109)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 4.562 bits)
   1. '
' (0.453120)
   2. 'Berlin' (0.143818)
   3. 'If' (0.027228)
   4. 'The' (0.023025)
   5. 'You' (0.014681)
   6. '(' (0.014457)
   7. '' (0.014370)
   8. 'What' (0.011041)
   9. 'Bon' (0.010859)
  10. 'I' (0.009624)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000 bits)
   1. 'Berlin' (1.000000)
   2. 'The' (0.000000)
   3. 'Germany' (0.000000)
   4. 'It' (0.000000)
   5. 'There' (0.000000)
   6. 'That' (0.000000)
   7. 'Bon' (0.000000)
   8. '
' (0.000000)
   9. 'Frankfurt' (0.000000)
  10. 'I' (0.000000)
  11. '' (0.000000)
  12. 'Well' (0.000000)
  13. 'B' (0.000000)
  14. 'This' (0.000000)
  15. 'What' (0.000000)

Temperature 2.0:
  (entropy: 12.151 bits)
   1. 'Berlin' (0.058676)
   2. 'The' (0.009506)
   3. 'Germany' (0.008990)
   4. 'It' (0.007966)
   5. 'There' (0.006055)
   6. 'That' (0.004111)
   7. 'Bon' (0.003674)
   8. '
' (0.003304)
   9. 'Frankfurt' (0.003276)
  10. 'I' (0.003260)
  11. '' (0.003098)
  12. 'Well' (0.003033)
  13. 'B' (0.003032)
  14. 'This' (0.002969)
  15. 'What' (0.002922)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 32
Model dimension: 4096
Number of heads: 32
Vocab size: 32000
Context length: 2048
=== END OF MODEL STATS ========

