
============================================================
EVALUATING MODEL: meta-llama/Meta-Llama-3-8B
============================================================
Loading model: meta-llama/Meta-Llama-3-8B...
Loaded pretrained model meta-llama/Meta-Llama-3-8B into HookedTransformer

=== NORMALIZATION ANALYSIS ========
Block normalization type: RMSNormPre
Using NORMALISED residual stream (RMS, no learnable scale)
Final normalization type: RMSNormPre
=== END NORMALIZATION ANALYSIS ====


=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['<|begin_of_text|>', 'Question', ':', ' What', ' is', ' the', ' capital', ' of', ' Germany', '?', ' Answer', ':']
Computing layer-wise predictions (memory-efficient targeted caching)...

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using NORMALIZED residual stream (RMS, no learnable scale)
Note: Shown probabilities are from full softmax (calibrated and comparable)
------------------------------------------------------------
Layer  0 (embeddings):
[diagnostic] No separate positional embedding hook; using only token embeddings for layer 0 residual.
  (entropy: 14.520 bits):
   1. 'oren' (0.165227)
   2. 'nton' (0.108704)
   3. '977' (0.086333)
   4. 'aland' (0.072403)
   5. '賀' (0.050403)
   6. 'ceso' (0.048441)
   7. 'adil' (0.041334)
   8. 'nesc' (0.040540)
   9. 'anson' (0.040119)
  10. 'anter' (0.038947)
  11. ' Pun' (0.038673)
  12. 'ění' (0.035117)
  13. 'Ｌ' (0.032234)
  14. ' arası' (0.031462)
  15. 'ophon' (0.030878)
  16. ' Pension' (0.028852)
  17. 'SetName' (0.028628)
  18. ' edin' (0.027586)
  19. 'writeln' (0.027293)
  20. 'undler' (0.026826)

Layer  1 (after transformer block 0) (entropy: 13.638 bits):
   1. 'ря' (0.128317)
   2. 'стров' (0.086232)
   3. 'chine' (0.085048)
   4. 'Leap' (0.064830)
   5. 'itler' (0.063148)
   6. 'üstü' (0.053562)
   7. '妙' (0.052110)
   8. 'ezi' (0.050981)
   9. 'elter' (0.044514)
  10. 'oğ' (0.039988)
  11. 'WR' (0.039356)
  12. 'يون' (0.038766)
  13. 'WWW' (0.036625)
  14. ' Cater' (0.034076)
  15. ' Ven' (0.033119)
  16. ' ​​' (0.032249)
  17. 'CompatActivity' (0.030710)
  18. 'ATOR' (0.030239)
  19. 'createFrom' (0.028380)
  20. 'ilt' (0.027747)

Layer  2 (after transformer block 1) (entropy: 13.617 bits):
   1. ' ​​' (0.119012)
   2. 'aten' (0.108609)
   3. 'atten' (0.090604)
   4. '691' (0.063257)
   5. 'seys' (0.056975)
   6. 'eden' (0.052308)
   7. 'bai' (0.051237)
   8. 'dna' (0.048628)
   9. 'ルド' (0.047898)
  10. '851' (0.044266)
  11. 'bette' (0.041007)
  12. 'building' (0.035054)
  13. '-letter' (0.032683)
  14. ' klu' (0.032335)
  15. 'рів' (0.031839)
  16. 'ilt' (0.030401)
  17. 'aniem' (0.030121)
  18. 'dsl' (0.028405)
  19. 'ActiveSheet' (0.028136)
  20. ' Opr' (0.027223)

Layer  3 (after transformer block 2) (entropy: 13.543 bits):
   1. 'atten' (0.169672)
   2. ' Ven' (0.139201)
   3. 'เ' (0.124858)
   4. 'BA' (0.077663)
   5. '.Decode' (0.042535)
   6. 'orz' (0.039328)
   7. '536' (0.037518)
   8. '﻿#' (0.036484)
   9. 'imas' (0.036186)
  10. '施' (0.034555)
  11. 'ird' (0.031356)
  12. 'aska' (0.029290)
  13. '.WinForms' (0.028622)
  14. '.stride' (0.026501)
  15. 'ba' (0.026045)
  16. 'Ven' (0.026042)
  17. '-letter' (0.024858)
  18. 'rud' (0.023391)
  19. ' PIE' (0.023329)
  20. 'erah' (0.022565)

Layer  4 (after transformer block 3) (entropy: 13.076 bits):
   1. 'adal' (0.175919)
   2. 'chine' (0.090247)
   3. 'ADDE' (0.083509)
   4. 'Atoms' (0.066922)
   5. '�' (0.064279)
   6. '.Decode' (0.052076)
   7. 'heimer' (0.051826)
   8. '.Areas' (0.051204)
   9. 'ソン' (0.034480)
  10. 'bai' (0.033905)
  11. 'rana' (0.032915)
  12. ' ​​' (0.032457)
  13. ' Podesta' (0.031602)
  14. ' Coach' (0.030872)
  15. 'Capabilities' (0.030746)
  16. 'こんにちは' (0.030652)
  17. 'reator' (0.029130)
  18. '/Users' (0.026821)
  19. '/operator' (0.025260)
  20. '/Area' (0.025179)

Layer  5 (after transformer block 4) (entropy: 13.324 bits):
   1. 'chine' (0.246275)
   2. 'olg' (0.073662)
   3. 'XC' (0.065343)
   4. 'istrovství' (0.063911)
   5. 'IZER' (0.058731)
   6. ' Obr' (0.050733)
   7. 'เ' (0.044865)
   8. 'ekler' (0.038146)
   9. '�' (0.035915)
  10. 'RIPT' (0.034213)
  11. 'ADDE' (0.034182)
  12. 'cheng' (0.032984)
  13. 'adal' (0.032694)
  14. '.Generated' (0.032248)
  15. 'reator' (0.026347)
  16. ' Sov' (0.026159)
  17. 'STRU' (0.026053)
  18. ' bearer' (0.025910)
  19. 'OVE' (0.025817)
  20. 'еди' (0.025811)

Layer  6 (after transformer block 5) (entropy: 13.282 bits):
   1. '.Decode' (0.106822)
   2. 'aget' (0.082706)
   3. 'LANG' (0.066881)
   4. 'urname' (0.065015)
   5. 'IDO' (0.059025)
   6. '�' (0.054366)
   7. 'multipart' (0.054247)
   8. 'defgroup' (0.053261)
   9. ' Citizens' (0.048200)
  10. ''gc' (0.045830)
  11. 'остат' (0.045621)
  12. 'STREAM' (0.042304)
  13. '��' (0.041210)
  14. 'OnError' (0.037823)
  15. '_OBS' (0.037026)
  16. 'ulaire' (0.035947)
  17. 'reator' (0.031693)
  18. 'adal' (0.030757)
  19. ' multipart' (0.030719)
  20. ' nonatomic' (0.030547)

Layer  7 (after transformer block 6) (entropy: 13.003 bits):
   1. ' Cem' (0.103475)
   2. ' Citizens' (0.095068)
   3. 'итор' (0.069188)
   4. 'raž' (0.064821)
   5. 'adal' (0.055854)
   6. 'ToObject' (0.050323)
   7. 'ilip' (0.048957)
   8. 'MimeType' (0.048490)
   9. 'mpr' (0.048210)
  10. 'upd' (0.043548)
  11. 'strap' (0.043314)
  12. 'чики' (0.042881)
  13. 'urname' (0.042222)
  14. 'MPI' (0.038648)
  15. 'OnError' (0.036285)
  16. '/Area' (0.036187)
  17. '-Origin' (0.036040)
  18. ' Civ' (0.033894)
  19. 'มน' (0.031855)
  20. 'RuntimeObject' (0.030741)

Layer  8 (after transformer block 7) (entropy: 13.129 bits):
   1. 'AutoSize' (0.091564)
   2. ' RoundedRectangle' (0.084027)
   3. 'ilt' (0.063455)
   4. 'enville' (0.057683)
   5. '963' (0.054100)
   6. 'xFFF' (0.051685)
   7. 'strap' (0.050674)
   8. 'PCM' (0.049234)
   9. 'ANGED' (0.045815)
  10. 'setFlash' (0.045118)
  11. 'こんにちは' (0.043092)
  12. 'کان' (0.042237)
  13. 'LTR' (0.042184)
  14. '.GetById' (0.041791)
  15. 'ermo' (0.041709)
  16. '�' (0.041674)
  17. 'imeo' (0.039148)
  18. '/Area' (0.038611)
  19. 'wie' (0.038355)
  20. '。。

' (0.037841)

Layer  9 (after transformer block 8) (entropy: 12.231 bits):
   1. 'enville' (0.202596)
   2. '.onView' (0.087045)
   3. '.netbeans' (0.080135)
   4. 'こんにちは' (0.071565)
   5. 'iversit' (0.067466)
   6. '.xtext' (0.061698)
   7. 'PCM' (0.061359)
   8. 'شی' (0.036487)
   9. '.owl' (0.036054)
  10. 'bcm' (0.035315)
  11. 'ilir' (0.034362)
  12. 'ylland' (0.033820)
  13. 'tracer' (0.031757)
  14. '/Area' (0.029556)
  15. 'entar' (0.028582)
  16. 'VML' (0.024929)
  17. '감' (0.023187)
  18. 'ilan' (0.018272)
  19. 'stral' (0.018175)
  20. 'ینه' (0.017639)

Layer 10 (after transformer block 9) (entropy: 12.581 bits):
   1. 'PCM' (0.306294)
   2. 'iversit' (0.183476)
   3. '�' (0.046628)
   4. '.xtext' (0.043260)
   5. 'erialize' (0.042931)
   6. '孝' (0.039610)
   7. 'vanished' (0.039312)
   8. 'ilt' (0.034889)
   9. 'entes' (0.031447)
  10. 'ystore' (0.029678)
  11. 'olang' (0.025462)
  12. ' Civ' (0.023138)
  13. 'ilir' (0.020595)
  14. 'SPATH' (0.020524)
  15. 'rored' (0.019809)
  16. 'velt' (0.019521)
  17. 'цип' (0.019222)
  18. 'หาย' (0.018136)
  19. 'UILTIN' (0.018046)
  20. '_stdio' (0.018020)

Layer 11 (after transformer block 10) (entropy: 13.088 bits):
   1. 'ystack' (0.176572)
   2. 'PCM' (0.123346)
   3. 'Раз' (0.063693)
   4. ' Woody' (0.059970)
   5. 'iversit' (0.053662)
   6. 'ilir' (0.048441)
   7. 'หาย' (0.044519)
   8. '.xtext' (0.041959)
   9. 'SPATH' (0.040892)
  10. 'ustum' (0.040764)
  11. ' Multiply' (0.038817)
  12. 'рел' (0.037325)
  13. 'ystore' (0.036535)
  14. 'icies' (0.034669)
  15. ' Parenthood' (0.032316)
  16. 'oví' (0.027009)
  17. 'PCP' (0.026016)
  18. 'dik' (0.025275)
  19. '..



' (0.024552)
  20. 'LTR' (0.023670)

Layer 12 (after transformer block 11) (entropy: 12.390 bits):
   1. '.xtext' (0.331664)
   2. 'рел' (0.095941)
   3. 'ystack' (0.062362)
   4. 'PCM' (0.056237)
   5. 'FRING' (0.053875)
   6. 'urement' (0.042620)
   7. 'acific' (0.035225)
   8. '929' (0.034833)
   9. 'rete' (0.033454)
  10. '/Area' (0.027452)
  11. '곤' (0.027095)
  12. 'Advertisements' (0.026781)
  13. 'ormsg' (0.024940)
  14. '_mex' (0.024083)
  15. '539' (0.021898)
  16. ' PartialView' (0.021265)
  17. 'óst' (0.021210)
  18. 'ackbar' (0.020480)
  19. ' geschichten' (0.019613)
  20. '野' (0.018974)

Layer 13 (after transformer block 12) (entropy: 13.361 bits):
   1. 'краї' (0.126613)
   2. ' def' (0.094377)
   3. '_EDEFAULT' (0.078384)
   4. '.xtext' (0.072547)
   5. '@Id' (0.072456)
   6. 'PCM' (0.059421)
   7. '_mex' (0.055552)
   8. 'Архів' (0.044319)
   9. 'erializer' (0.041785)
  10. ' PartialView' (0.040601)
  11. ' Haupt' (0.039158)
  12. ' b' (0.037781)
  13. ' Greenwood' (0.035744)
  14. 'rophy' (0.033079)
  15. '_sid' (0.030782)
  16. '�' (0.029913)
  17. ' reins' (0.028227)
  18. 'celik' (0.027749)
  19. 'edar' (0.026563)
  20. 'remium' (0.024950)

Layer 14 (after transformer block 13) (entropy: 13.351 bits):
   1. '#ab' (0.113202)
   2. 'Архів' (0.097651)
   3. 'erece' (0.078093)
   4. '�回' (0.072939)
   5. '_mex' (0.070712)
   6. 'enderit' (0.067316)
   7. 'argin' (0.062598)
   8. 'ritel' (0.059007)
   9. 'zcze' (0.050846)
  10. 'hek' (0.040748)
  11. ' "}\' (0.033823)
  12. 'INLINE' (0.033667)
  13. 'šak' (0.030782)
  14. ' cố' (0.029204)
  15. ' Mes' (0.027734)
  16. '#ac' (0.026576)
  17. '_EDEFAULT' (0.026479)
  18. '.xtext' (0.026353)
  19. 'เฉ' (0.026210)
  20. 'umin' (0.026060)

Layer 15 (after transformer block 14) (entropy: 12.812 bits):
   1. '#af' (0.093472)
   2. ')frame' (0.090785)
   3. '#ab' (0.078998)
   4. 'enderit' (0.073430)
   5. '.datatables' (0.065423)
   6. ' dõi' (0.064946)
   7. '.utf' (0.063154)
   8. '#ad' (0.057873)
   9. 'ateg' (0.055737)
  10. 'undler' (0.052980)
  11. 'ediği' (0.039025)
  12. 'ritel' (0.037804)
  13. 'enderror' (0.032497)
  14. '#ac' (0.031221)
  15. '재' (0.029271)
  16. 'SPATH' (0.029072)
  17. 'üstü' (0.027453)
  18. 'Sdk' (0.026666)
  19. 'rowsable' (0.025442)
  20. '#index' (0.024750)

Layer 16 (after transformer block 15) (entropy: 13.033 bits):
   1. '#ab' (0.141328)
   2. 'undler' (0.090475)
   3. 'enderror' (0.074628)
   4. '_EDEFAULT' (0.063674)
   5. '.ecore' (0.060154)
   6. 'شف' (0.058084)
   7. 'umin' (0.057130)
   8. 'upo' (0.052765)
   9. '#ad' (0.051652)
  10. '.LoggerFactory' (0.049204)
  11. 'ほ' (0.040586)
  12. 'Cİ' (0.036205)
  13. 'ชอบ' (0.030544)
  14. 'erosis' (0.030027)
  15. '_mA' (0.028163)
  16. 'idal' (0.027935)
  17. '_mB' (0.027666)
  18. 'ootball' (0.027361)
  19. 'eldo' (0.027073)
  20. 'PREFIX' (0.025345)

Layer 17 (after transformer block 16) (entropy: 11.951 bits):
   1. '#ad' (0.217490)
   2. ')application' (0.133277)
   3. '#ab' (0.115240)
   4. 'oplayer' (0.102035)
   5. '/******/' (0.069078)
   6. 'ชอบ' (0.055039)
   7. 'ateg' (0.042685)
   8. '#af' (0.031716)
   9. 'ABCDEFGHIJKLMNOP' (0.029985)
  10. 'undler' (0.029159)
  11. ' thủ' (0.021689)
  12. 'ktop' (0.020299)
  13. 'Insets' (0.020055)
  14. 'imas' (0.018618)
  15. 'SendMessage' (0.017893)
  16. 'sched' (0.016787)
  17. 'cratch' (0.015490)
  18. 'ependency' (0.015273)
  19. 'etros' (0.014382)
  20. '#error' (0.013809)

Layer 18 (after transformer block 17) (entropy: 12.480 bits):
   1. '#ad' (0.234857)
   2. ')application' (0.126914)
   3. 'oplayer' (0.118990)
   4. 'ABCDEFGHIJKLMNOP' (0.085492)
   5. 'imas' (0.064128)
   6. '#ab' (0.043080)
   7. ' capital' (0.038964)
   8. 'hoot' (0.038057)
   9. 'wig' (0.032600)
  10. 'zd' (0.028151)
  11. ' Capitals' (0.023841)
  12. 'sched' (0.022767)
  13. 'ชอบ' (0.021824)
  14. 'NullException' (0.019175)
  15. 'undler' (0.019122)
  16. 'etros' (0.017109)
  17. 'undi' (0.016870)
  18. ' none' (0.016853)
  19. 'SendMessage' (0.015623)
  20. ' capitals' (0.015582)

Layer 19 (after transformer block 18) (entropy: 11.751 bits):
   1. 'ABCDEFGHIJKLMNOP' (0.347737)
   2. ')application' (0.246025)
   3. ' Capitals' (0.050557)
   4. 'oplayer' (0.042271)
   5. ' capital' (0.035150)
   6. 'ittings' (0.034420)
   7. ' Capital' (0.033836)
   8. ' none' (0.027071)
   9. ' (::' (0.024231)
  10. 'sched' (0.023037)
  11. 'imas' (0.019730)
  12. 'ZF' (0.018025)
  13. 'undi' (0.015904)
  14. 'hoot' (0.015093)
  15. 'AGED' (0.014765)
  16. '#ad' (0.013017)
  17. ' capitals' (0.010094)
  18. 'hed' (0.009777)
  19. 'urm' (0.009686)
  20. '.createSequentialGroup' (0.009576)

Layer 20 (after transformer block 19) (entropy: 10.769 bits):
   1. ' Capital' (0.200821)
   2. 'ABCDEFGHIJKLMNOP' (0.163628)
   3. 'urm' (0.145246)
   4. ' Capitals' (0.113647)
   5. ' capital' (0.077043)
   6. ')application' (0.066273)
   7. ' Washington' (0.053720)
   8. ' capitals' (0.041252)
   9. 'ashington' (0.018931)
  10. ' London' (0.016709)
  11. 'sched' (0.013975)
  12. '.openConnection' (0.013110)
  13. 'ittings' (0.011376)
  14. 'Capital' (0.010259)
  15. '-NLS' (0.009821)
  16. '#ad' (0.009536)
  17. 'Washington' (0.009113)
  18. ' Federal' (0.009033)
  19. ' Rome' (0.008869)
  20. ' federally' (0.007638)

Layer 21 (after transformer block 20) (entropy: 9.287 bits):
   1. ' capital' (0.340330)
   2. ' Capital' (0.204022)
   3. ' Capitals' (0.171057)
   4. ' capitals' (0.087776)
   5. 'urm' (0.029836)
   6. ')application' (0.023733)
   7. ' Washington' (0.021997)
   8. 'Capital' (0.018364)
   9. 'ABCDEFGHIJKLMNOP' (0.015556)
  10. ' CAPITAL' (0.015508)
  11. ' Scha' (0.011435)
  12. ' Berlin' (0.010174)
  13. ' London' (0.009012)
  14. '.openConnection' (0.007626)
  15. 'capital' (0.006329)
  16. 'Washington' (0.005943)
  17. ' Erotische' (0.005683)
  18. 'urum' (0.005517)
  19. '-NLS' (0.005246)
  20. 'imas' (0.004854)

Layer 22 (after transformer block 21) (entropy: 8.478 bits):
   1. ' Berlin' (0.406479)
   2. ' capital' (0.162062)
   3. ' Capitals' (0.142080)
   4. ' Capital' (0.076245)
   5. ' Washington' (0.066610)
   6. ' Federal' (0.034123)
   7. ' London' (0.014630)
   8. 'Capital' (0.011899)
   9. ' capitals' (0.010765)
  10. ')application' (0.009648)
  11. 'Washington' (0.008753)
  12. ' federal' (0.007902)
  13. ' federally' (0.007476)
  14. 'Federal' (0.006942)
  15. ' none' (0.006611)
  16. 'Berlin' (0.006577)
  17. 'urm' (0.006182)
  18. 'ashington' (0.005090)
  19. ' proficient' (0.004981)
  20. ' CAPITAL' (0.004948)

Layer 23 (after transformer block 22) (entropy: 5.583 bits):
   1. ' Berlin' (0.679136)
   2. ' Capitals' (0.081544)
   3. ' capital' (0.068653)
   4. ' Capital' (0.059142)
   5. ' Washington' (0.028296)
   6. 'Berlin' (0.016506)
   7. ' capitals' (0.009716)
   8. ' London' (0.009587)
   9. 'Capital' (0.009389)
  10. ' CAPITAL' (0.005603)
  11. 'Washington' (0.004564)
  12. ' federally' (0.004532)
  13. 'urm' (0.003831)
  14. '.openConnection' (0.003619)
  15. 'ashington' (0.003489)
  16. ' Federal' (0.002680)
  17. ' none' (0.002514)
  18. ' reun' (0.002456)
  19. ' Rome' (0.002402)
  20. ')application' (0.002341)

Layer 24 (after transformer block 23) (entropy: 2.068 bits):
   1. ' Berlin' (0.861250)
   2. ' Washington' (0.043750)
   3. ' Capitals' (0.025173)
   4. ' capital' (0.020216)
   5. ' Capital' (0.019571)
   6. 'Berlin' (0.012466)
   7. 'ashington' (0.002385)
   8. 'Washington' (0.002241)
   9. 'Capital' (0.001927)
  10. ' CAPITAL' (0.001736)
  11. ' Islamabad' (0.001510)
  12. ' berlin' (0.001478)
  13. ' capitals' (0.001296)
  14. ' London' (0.000925)
  15. ' federally' (0.000919)
  16. ' Federal' (0.000706)
  17. ' Bras' (0.000674)
  18. ' Tall' (0.000634)
  19. ' Westminster' (0.000576)
  20. '柏' (0.000567)

Layer 25 (after transformer block 24) (entropy: 0.427 bits):
   1. ' Berlin' (0.972887)
   2. 'Berlin' (0.017593)
   3. ' capital' (0.002521)
   4. ' Capital' (0.001760)
   5. ' Capitals' (0.001326)
   6. ' berlin' (0.000740)
   7. ' Islamabad' (0.000585)
   8. ' Washington' (0.000407)
   9. '柏' (0.000328)
  10. 'Capital' (0.000217)
  11. ' Sans' (0.000213)
  12. ' Bon' (0.000198)
  13. ' Ankara' (0.000192)
  14. ' Charl' (0.000172)
  15. ' federally' (0.000165)
  16. ' CAPITAL' (0.000157)
  17. ' Beijing' (0.000142)
  18. ' Paris' (0.000134)
  19. ' Federal' (0.000133)
  20. ' Canberra' (0.000133)

Layer 26 (after transformer block 25) (entropy: 0.349 bits):
   1. ' Berlin' (0.963560)
   2. 'Berlin' (0.033130)
   3. ' berlin' (0.001142)
   4. ' Islamabad' (0.000405)
   5. ' Capitals' (0.000238)
   6. ' capital' (0.000230)
   7. '柏' (0.000141)
   8. ' Washington' (0.000131)
   9. ' Paris' (0.000118)
  10. ' Capital' (0.000114)
  11. ' Tall' (0.000110)
  12. ' London' (0.000106)
  13. 'AREST' (0.000092)
  14. ' Bon' (0.000082)
  15. ' Ankara' (0.000078)
  16. ' Frankfurt' (0.000071)
  17. ' BER' (0.000068)
  18. '東京' (0.000064)
  19. ' Beijing' (0.000062)
  20. ' Ber' (0.000061)

Layer 27 (after transformer block 26) (entropy: 0.331 bits):
   1. ' Berlin' (0.959932)
   2. 'Berlin' (0.037557)
   3. ' berlin' (0.000966)
   4. ' BER' (0.000277)
   5. ' Bon' (0.000257)
   6. ' Ber' (0.000200)
   7. '柏' (0.000135)
   8. ' Бер' (0.000111)
   9. ' Islamabad' (0.000085)
  10. ' Brand' (0.000084)
  11. ' Capitals' (0.000056)
  12. ' capital' (0.000053)
  13. ' Ankara' (0.000045)
  14. '東京' (0.000044)
  15. 'olis' (0.000036)
  16. ' Sans' (0.000036)
  17. ' Charl' (0.000036)
  18. ' Capital' (0.000032)
  19. ' Madrid' (0.000031)
  20. ' London' (0.000028)

Layer 28 (after transformer block 27) (entropy: 0.374 bits):
   1. ' Berlin' (0.953845)
   2. 'Berlin' (0.042396)
   3. ' BER' (0.000806)
   4. ' berlin' (0.000778)
   5. ' Ber' (0.000661)
   6. ' Бер' (0.000476)
   7. ' Bon' (0.000367)
   8. '柏' (0.000189)
   9. ' Brand' (0.000100)
  10. ' Sans' (0.000072)
  11. 'olis' (0.000040)
  12. ' federally' (0.000036)
  13. ' Charl' (0.000033)
  14. ' Bern' (0.000031)
  15. 'ベル' (0.000031)
  16. ' Capitals' (0.000030)
  17. 'Ber' (0.000029)
  18. ' Tier' (0.000029)
  19. 'Brand' (0.000026)
  20. ' Bras' (0.000025)

Layer 29 (after transformer block 28) (entropy: 0.811 bits):
   1. ' Berlin' (0.927450)
   2. 'Berlin' (0.053873)
   3. ' BER' (0.008022)
   4. ' berlin' (0.002153)
   5. ' Бер' (0.001790)
   6. ' Ber' (0.001614)
   7. '-B' (0.001074)
   8. '柏' (0.000764)
   9. ' Bon' (0.000732)
  10. ' federally' (0.000296)
  11. ' Charl' (0.000294)
  12. 'BER' (0.000290)
  13. ' B' (0.000278)
  14. 'ob' (0.000252)
  15. 'Ber' (0.000218)
  16. 'olis' (0.000210)
  17. ' Tier' (0.000198)
  18. '	B' (0.000178)
  19. ' Brand' (0.000175)
  20. 'ber' (0.000139)

Layer 30 (after transformer block 29) (entropy: 1.637 bits):
   1. ' Berlin' (0.897552)
   2. 'Berlin' (0.042181)
   3. ' BER' (0.015939)
   4. ' Ber' (0.014103)
   5. ' Бер' (0.008493)
   6. ' berlin' (0.004876)
   7. ' Bon' (0.002985)
   8. '-B' (0.002717)
   9. '柏' (0.002638)
  10. 'Ber' (0.001919)
  11. ' ber' (0.001561)
  12. ' B' (0.001026)
  13. '	B' (0.000875)
  14. '_B' (0.000569)
  15. ' Tier' (0.000563)
  16. '_ber' (0.000467)
  17. ' Bern' (0.000454)
  18. ' бер' (0.000407)
  19. ' federally' (0.000350)
  20. ' capital' (0.000325)

Layer 31 (after transformer block 30) (entropy: 1.162 bits):
   1. ' Berlin' (0.940405)
   2. 'Berlin' (0.020268)
   3. ' BER' (0.011598)
   4. ' Ber' (0.008826)
   5. ' berlin' (0.003187)
   6. ' Бер' (0.002670)
   7. ' ber' (0.002284)
   8. ' Bern' (0.001995)
   9. ' Bon' (0.001700)
  10. ' None' (0.001577)
  11. '	B' (0.000848)
  12. ' ' (0.000647)
  13. ' none' (0.000626)
  14. '柏' (0.000586)
  15. ' Germany' (0.000586)
  16. 'Ber' (0.000553)
  17. ' Capital' (0.000471)
  18. 'ber' (0.000433)
  19. ' capital' (0.000400)
  20. ' бер' (0.000340)

Layer 32 (after transformer block 31) (entropy: 1.704 bits):
   1. ' Berlin' (0.919971)
   2. ' Germany' (0.019194)
   3. ' The' (0.015508)
   4. ' ' (0.007071)
   5. ' Ber' (0.004366)
   6. ' This' (0.003537)
   7. ' ' (0.003031)
   8. 'Berlin' (0.002987)
   9. ' (' (0.002984)
  10. ' None' (0.002952)
  11. ' A' (0.002609)
  12. ' Bon' (0.002508)
  13. ' What' (0.002473)
  14. ' It' (0.002143)
  15. ' the' (0.001701)
  16. ' In' (0.001561)
  17. '<|end_of_text|>' (0.001492)
  18. ' There' (0.001340)
  19. ' 
' (0.001310)
  20. ' Frankfurt' (0.001263)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 1.704 bits):
   1. ' Berlin' (0.857033)
   2. ' Germany' (0.017881)
   3. ' The' (0.014447)
   4. ' ' (0.006588)
   5. ' Ber' (0.004067)
   6. ' This' (0.003295)
   7. ' ' (0.002823)
   8. 'Berlin' (0.002783)
   9. ' (' (0.002780)
  10. ' None' (0.002750)
  11. ' A' (0.002430)
  12. ' Bon' (0.002337)
  13. ' What' (0.002304)
  14. ' It' (0.001996)
  15. ' the' (0.001585)
  16. ' In' (0.001454)
  17. '<|end_of_text|>' (0.001390)
  18. ' There' (0.001248)
  19. ' 
' (0.001221)
  20. ' Frankfurt' (0.001177)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 6.022 bits)
   1. ' a' (0.287639)
   2. ' one' (0.058422)
   3. ' the' (0.052595)
   4. ' also' (0.049553)
   5. ' home' (0.032247)
   6. ' known' (0.027375)
   7. ' an' (0.026677)
   8. ' famous' (0.024189)
   9. ' full' (0.021177)
  10. ' located' (0.015286)

Prompt: 'Berlin is the capital of'
  (entropy: 0.928 bits)
   1. ' Germany' (0.895601)
   2. ' the' (0.052477)
   3. ' and' (0.007550)
   4. ' germany' (0.003361)
   5. ' modern' (0.002972)
   6. ' Berlin' (0.002895)
   7. ' united' (0.002628)
   8. ' German' (0.002496)
   9. ' Europe' (0.002210)
  10. ' a' (0.002209)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 7.310 bits)
   1. ' Berlin' (0.149300)
   2. ' The' (0.052456)
   3. ' Which' (0.051124)
   4. ' If' (0.045928)
   5. ' What' (0.045826)
   6. ' (' (0.022199)
   7. ' Now' (0.018858)
   8. ' You' (0.018447)
   9. ' Or' (0.016234)
  10. ' How' (0.015913)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000 bits)
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. ' The' (0.000000)
   4. ' ' (0.000000)
   5. ' Ber' (0.000000)
   6. ' This' (0.000000)
   7. ' ' (0.000000)
   8. 'Berlin' (0.000000)
   9. ' (' (0.000000)
  10. ' None' (0.000000)
  11. ' A' (0.000000)
  12. ' Bon' (0.000000)
  13. ' What' (0.000000)
  14. ' It' (0.000000)
  15. ' the' (0.000000)

Temperature 2.0:
  (entropy: 14.520 bits)
   1. ' Berlin' (0.034909)
   2. ' Germany' (0.005042)
   3. ' The' (0.004532)
   4. ' ' (0.003061)
   5. ' Ber' (0.002405)
   6. ' This' (0.002164)
   7. ' ' (0.002004)
   8. 'Berlin' (0.001989)
   9. ' (' (0.001988)
  10. ' None' (0.001977)
  11. ' A' (0.001859)
  12. ' Bon' (0.001823)
  13. ' What' (0.001810)
  14. ' It' (0.001685)
  15. ' the' (0.001501)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 32
Model dimension: 4096
Number of heads: 32
Vocab size: 128256
Context length: 8192
=== END OF MODEL STATS ========

