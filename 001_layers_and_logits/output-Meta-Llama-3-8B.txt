
============================================================
EVALUATING MODEL: meta-llama/Meta-Llama-3-8B
============================================================
Loading model: meta-llama/Meta-Llama-3-8B...
Loaded pretrained model meta-llama/Meta-Llama-3-8B into HookedTransformer

=== NORMALIZATION ANALYSIS ========
Block LayerNorm type: RMSNormPre
⚠️  Non-vanilla norm detected (RMSNormPre) - norm-lens will be skipped to avoid distortion
Final LayerNorm type: RMSNormPre
=== END NORMALIZATION ANALYSIS ====


=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['<|begin_of_text|>', 'Question', ':', ' What', ' is', ' the', ' capital', ' of', ' Germany', '?', ' Answer', ':']
Computing layer-wise predictions (memory-efficient targeted caching)...

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using RAW residual stream (non-vanilla norms detected, skipping normalization to avoid distortion)
Note: Shown probabilities are softmax over top-k only (don't sum to 1)
------------------------------------------------------------
Layer  0 (embeddings):
  (entropy: 11.762):
   1. 'oren' (0.050387)
   2. 'nton' (0.050266)
   3. '977' (0.050199)
   4. 'aland' (0.050149)
   5. '賀' (0.050044)
   6. 'ceso' (0.050033)
   7. 'adil' (0.049987)
   8. 'nesc' (0.049982)
   9. 'anson' (0.049979)
  10. 'anter' (0.049970)
  11. ' Pun' (0.049968)
  12. 'ění' (0.049940)
  13. 'Ｌ' (0.049916)
  14. ' arası' (0.049909)
  15. 'ophon' (0.049903)
  16. ' Pension' (0.049884)
  17. 'SetName' (0.049882)
  18. ' edin' (0.049871)
  19. 'writeln' (0.049868)
  20. 'undler' (0.049863)

Layer  1 (after block 0) (entropy: 11.762):
   1. 'ря' (0.050604)
   2. 'стров' (0.050372)
   3. 'chine' (0.050363)
   4. 'Leap' (0.050205)
   5. 'itler' (0.050190)
   6. 'üstü' (0.050094)
   7. '妙' (0.050078)
   8. 'ezi' (0.050065)
   9. 'elter' (0.049987)
  10. 'oğ' (0.049924)
  11. 'WR' (0.049915)
  12. 'يون' (0.049907)
  13. 'WWW' (0.049874)
  14. ' Cater' (0.049832)
  15. ' Ven' (0.049815)
  16. ' ​​' (0.049800)
  17. 'CompatActivity' (0.049772)
  18. 'ATOR' (0.049763)
  19. 'createFrom' (0.049726)
  20. 'ilt' (0.049713)

Layer  2 (after block 1) (entropy: 11.761):
   1. ' ​​' (0.050913)
   2. 'aten' (0.050827)
   3. 'atten' (0.050655)
   4. '691' (0.050318)
   5. 'seys' (0.050220)
   6. 'eden' (0.050140)
   7. 'bai' (0.050121)
   8. 'dna' (0.050072)
   9. 'ルド' (0.050058)
  10. '851' (0.049984)
  11. 'bette' (0.049913)
  12. 'building' (0.049768)
  13. '-letter' (0.049703)
  14. ' klu' (0.049693)
  15. 'рів' (0.049679)
  16. 'ilt' (0.049636)
  17. 'aniem' (0.049628)
  18. 'dsl' (0.049573)
  19. 'ActiveSheet' (0.049565)
  20. ' Opr' (0.049534)

Layer  3 (after block 2) (entropy: 11.760):
   1. 'atten' (0.052122)
   2. ' Ven' (0.051826)
   3. 'เ' (0.051665)
   4. 'BA' (0.050965)
   5. '.Decode' (0.050091)
   6. 'orz' (0.049979)
   7. '536' (0.049911)
   8. '﻿#' (0.049871)
   9. 'imas' (0.049859)
  10. '施' (0.049793)
  11. 'ird' (0.049655)
  12. 'aska' (0.049557)
  13. '.WinForms' (0.049525)
  14. '.stride' (0.049415)
  15. 'ba' (0.049391)
  16. 'Ven' (0.049390)
  17. '-letter' (0.049324)
  18. 'rud' (0.049238)
  19. ' PIE' (0.049235)
  20. 'erah' (0.049187)

Layer  4 (after block 3) (entropy: 11.759):
   1. 'adal' (0.052591)
   2. 'chine' (0.051345)
   3. 'ADDE' (0.051202)
   4. 'Atoms' (0.050796)
   5. '�' (0.050723)
   6. '.Decode' (0.050340)
   7. 'heimer' (0.050332)
   8. '.Areas' (0.050310)
   9. 'ソン' (0.049600)
  10. 'bai' (0.049570)
  11. 'rana' (0.049517)
  12. ' ​​' (0.049492)
  13. ' Podesta' (0.049445)
  14. ' Coach' (0.049404)
  15. 'Capabilities' (0.049396)
  16. 'こんにちは' (0.049391)
  17. 'reator' (0.049300)
  18. '/Users' (0.049154)
  19. '/operator' (0.049049)
  20. '/Area' (0.049043)

Layer  5 (after block 4) (entropy: 11.758):
   1. 'chine' (0.053947)
   2. 'olg' (0.051250)
   3. 'XC' (0.050990)
   4. 'istrovství' (0.050942)
   5. 'IZER' (0.050759)
   6. ' Obr' (0.050445)
   7. 'เ' (0.050182)
   8. 'ekler' (0.049837)
   9. '�' (0.049710)
  10. 'RIPT' (0.049607)
  11. 'ADDE' (0.049605)
  12. 'cheng' (0.049530)
  13. 'adal' (0.049512)
  14. '.Generated' (0.049483)
  15. 'reator' (0.049060)
  16. ' Sov' (0.049045)
  17. 'STRU' (0.049036)
  18. ' bearer' (0.049025)
  19. 'OVE' (0.049018)
  20. 'еди' (0.049017)

Layer  6 (after block 5) (entropy: 11.756):
   1. '.Decode' (0.052037)
   2. 'aget' (0.051389)
   3. 'LANG' (0.050858)
   4. 'urname' (0.050788)
   5. 'IDO' (0.050548)
   6. '�' (0.050346)
   7. 'multipart' (0.050340)
   8. 'defgroup' (0.050295)
   9. ' Citizens' (0.050050)
  10. ''gc' (0.049927)
  11. 'остат' (0.049916)
  12. 'STREAM' (0.049732)
  13. '��' (0.049668)
  14. 'OnError' (0.049460)
  15. '_OBS' (0.049408)
  16. 'ulaire' (0.049337)
  17. 'reator' (0.049034)
  18. 'adal' (0.048962)
  19. ' multipart' (0.048959)
  20. ' nonatomic' (0.048946)

Layer  7 (after block 6) (entropy: 11.756):
   1. ' Cem' (0.052081)
   2. ' Citizens' (0.051852)
   3. 'итор' (0.051001)
   4. 'raž' (0.050828)
   5. 'adal' (0.050436)
   6. 'ToObject' (0.050163)
   7. 'ilip' (0.050091)
   8. 'MimeType' (0.050066)
   9. 'mpr' (0.050051)
  10. 'upd' (0.049787)
  11. 'strap' (0.049773)
  12. 'чики' (0.049747)
  13. 'urname' (0.049707)
  14. 'MPI' (0.049478)
  15. 'OnError' (0.049316)
  16. '/Area' (0.049309)
  17. '-Origin' (0.049298)
  18. ' Civ' (0.049141)
  19. 'มน' (0.048983)
  20. 'RuntimeObject' (0.048892)

Layer  8 (after block 7) (entropy: 11.754):
   1. 'AutoSize' (0.051878)
   2. ' RoundedRectangle' (0.051620)
   3. 'ilt' (0.050786)
   4. 'enville' (0.050506)
   5. '963' (0.050319)
   6. 'xFFF' (0.050186)
   7. 'strap' (0.050128)
   8. 'PCM' (0.050045)
   9. 'ANGED' (0.049836)
  10. 'setFlash' (0.049792)
  11. 'こんにちは' (0.049660)
  12. 'کان' (0.049602)
  13. 'LTR' (0.049598)
  14. '.GetById' (0.049572)
  15. 'ermo' (0.049566)
  16. '�' (0.049564)
  17. 'imeo' (0.049384)
  18. '/Area' (0.049345)
  19. 'wie' (0.049326)
  20. '。。

' (0.049287)

Layer  9 (after block 8) (entropy: 11.753):
   1. 'enville' (0.055048)
   2. '.onView' (0.052329)
   3. '.netbeans' (0.052070)
   4. 'こんにちは' (0.051718)
   5. 'iversit' (0.051535)
   6. '.xtext' (0.051260)
   7. 'PCM' (0.051243)
   8. 'شی' (0.049670)
   9. '.owl' (0.049635)
  10. 'bcm' (0.049573)
  11. 'ilir' (0.049492)
  12. 'ylland' (0.049445)
  13. 'tracer' (0.049258)
  14. '/Area' (0.049047)
  15. 'entar' (0.048948)
  16. 'VML' (0.048549)
  17. '감' (0.048338)
  18. 'ilan' (0.047652)
  19. 'stral' (0.047637)
  20. 'ینه' (0.047552)

Layer 10 (after block 9) (entropy: 11.753):
   1. 'PCM' (0.057399)
   2. 'iversit' (0.055586)
   3. '�' (0.051017)
   4. '.xtext' (0.050777)
   5. 'erialize' (0.050753)
   6. '孝' (0.050498)
   7. 'vanished' (0.050474)
   8. 'ilt' (0.050098)
   9. 'entes' (0.049773)
  10. 'ystore' (0.049593)
  11. 'olang' (0.049120)
  12. ' Civ' (0.048826)
  13. 'ilir' (0.048471)
  14. 'SPATH' (0.048461)
  15. 'rored' (0.048354)
  16. 'velt' (0.048309)
  17. 'цип' (0.048262)
  18. 'หาย' (0.048087)
  19. 'UILTIN' (0.048072)
  20. '_stdio' (0.048068)

Layer 11 (after block 10) (entropy: 11.752):
   1. 'ystack' (0.055040)
   2. 'PCM' (0.053712)
   3. 'Раз' (0.051347)
   4. ' Woody' (0.051137)
   5. 'iversit' (0.050752)
   6. 'ilir' (0.050399)
   7. 'หาย' (0.050110)
   8. '.xtext' (0.049908)
   9. 'SPATH' (0.049821)
  10. 'ustum' (0.049810)
  11. ' Multiply' (0.049644)
  12. 'рел' (0.049512)
  13. 'ystore' (0.049440)
  14. 'icies' (0.049264)
  15. ' Parenthood' (0.049028)
  16. 'oví' (0.048433)
  17. 'PCP' (0.048309)
  18. 'dik' (0.048214)
  19. '..



' (0.048119)
  20. 'LTR' (0.048000)

Layer 12 (after block 11) (entropy: 11.750):
   1. '.xtext' (0.058642)
   2. 'рел' (0.053647)
   3. 'ystack' (0.052013)
   4. 'PCM' (0.051629)
   5. 'FRING' (0.051470)
   6. 'urement' (0.050611)
   7. 'acific' (0.049924)
   8. '929' (0.049884)
   9. 'rete' (0.049739)
  10. '/Area' (0.049038)
  11. '곤' (0.048992)
  12. 'Advertisements' (0.048951)
  13. 'ormsg' (0.048702)
  14. '_mex' (0.048580)
  15. '539' (0.048249)
  16. ' PartialView' (0.048148)
  17. 'óst' (0.048139)
  18. 'ackbar' (0.048018)
  19. ' geschichten' (0.047869)
  20. '野' (0.047755)

Layer 13 (after block 12) (entropy: 11.750):
   1. 'краї' (0.054107)
   2. ' def' (0.052905)
   3. '_EDEFAULT' (0.052160)
   4. '.xtext' (0.051852)
   5. '@Id' (0.051847)
   6. 'PCM' (0.051067)
   7. '_mex' (0.050805)
   8. 'Архів' (0.049936)
   9. 'erializer' (0.049712)
  10. ' PartialView' (0.049602)
  11. ' Haupt' (0.049466)
  12. ' b' (0.049330)
  13. ' Greenwood' (0.049122)
  14. 'rophy' (0.048832)
  15. '_sid' (0.048564)
  16. '�' (0.048458)
  17. ' reins' (0.048244)
  18. 'celik' (0.048181)
  19. 'edar' (0.048020)
  20. 'remium' (0.047791)

Layer 14 (after block 13) (entropy: 11.746):
   1. '#ab' (0.054256)
   2. 'Архів' (0.053556)
   3. 'erece' (0.052514)
   4. '�回' (0.052199)
   5. '_mex' (0.052057)
   6. 'enderit' (0.051832)
   7. 'argin' (0.051502)
   8. 'ritel' (0.051235)
   9. 'zcze' (0.050569)
  10. 'hek' (0.049594)
  11. ' "}\' (0.048789)
  12. 'INLINE' (0.048769)
  13. 'šak' (0.048386)
  14. ' cố' (0.048163)
  15. ' Mes' (0.047945)
  16. '#ac' (0.047765)
  17. '_EDEFAULT' (0.047750)
  18. '.xtext' (0.047730)
  19. 'เฉ' (0.047707)
  20. 'umin' (0.047683)

Layer 15 (after block 14) (entropy: 11.743):
   1. '#af' (0.053571)
   2. ')frame' (0.053420)
   3. '#ab' (0.052708)
   4. 'enderit' (0.052338)
   5. '.datatables' (0.051758)
   6. ' dõi' (0.051721)
   7. '.utf' (0.051582)
   8. '#ad' (0.051149)
   9. 'ateg' (0.050963)
  10. 'undler' (0.050714)
  11. 'ediği' (0.049240)
  12. 'ritel' (0.049089)
  13. 'enderror' (0.048377)
  14. '#ac' (0.048191)
  15. '재' (0.047891)
  16. 'SPATH' (0.047860)
  17. 'üstü' (0.047596)
  18. 'Sdk' (0.047463)
  19. 'rowsable' (0.047248)
  20. '#index' (0.047122)

Layer 16 (after block 15) (entropy: 11.739):
   1. '#ab' (0.056448)
   2. 'undler' (0.053845)
   3. 'enderror' (0.052758)
   4. '_EDEFAULT' (0.051879)
   5. '.ecore' (0.051568)
   6. 'شف' (0.051377)
   7. 'umin' (0.051287)
   8. 'upo' (0.050857)
   9. '#ad' (0.050742)
  10. '.LoggerFactory' (0.050482)
  11. 'ほ' (0.049464)
  12. 'Cİ' (0.048869)
  13. 'ชอบ' (0.047997)
  14. 'erosis' (0.047910)
  15. '_mA' (0.047587)
  16. 'idal' (0.047545)
  17. '_mB' (0.047497)
  18. 'ootball' (0.047441)
  19. 'eldo' (0.047388)
  20. 'PREFIX' (0.047058)

Layer 17 (after block 16) (entropy: 11.729):
   1. '#ad' (0.062757)
   2. ')application' (0.059037)
   3. '#ab' (0.057975)
   4. 'oplayer' (0.057101)
   5. '/******/' (0.054388)
   6. 'ชอบ' (0.052868)
   7. 'ateg' (0.051217)
   8. '#af' (0.049353)
   9. 'ABCDEFGHIJKLMNOP' (0.049009)
  10. 'undler' (0.048838)
  11. ' thủ' (0.047067)
  12. 'ktop' (0.046680)
  13. 'Insets' (0.046609)
  14. 'imas' (0.046179)
  15. 'SendMessage' (0.045950)
  16. 'sched' (0.045586)
  17. 'cratch' (0.045131)
  18. 'ependency' (0.045051)
  19. 'etros' (0.044715)
  20. '#error' (0.044488)

Layer 18 (after block 17) (entropy: 11.720):
   1. '#ad' (0.065378)
   2. ')application' (0.059859)
   3. 'oplayer' (0.059308)
   4. 'ABCDEFGHIJKLMNOP' (0.056564)
   5. 'imas' (0.054280)
   6. '#ab' (0.051273)
   7. ' capital' (0.050540)
   8. 'hoot' (0.050369)
   9. 'wig' (0.049265)
  10. 'zd' (0.048240)
  11. ' Capitals' (0.047105)
  12. 'sched' (0.046794)
  13. 'ชอบ' (0.046511)
  14. 'NullException' (0.045657)
  15. 'undler' (0.045639)
  16. 'etros' (0.044917)
  17. 'undi' (0.044826)
  18. ' none' (0.044820)
  19. 'SendMessage' (0.044336)
  20. ' capitals' (0.044319)

Layer 19 (after block 18) (entropy: 11.716):
   1. 'ABCDEFGHIJKLMNOP' (0.073631)
   2. ')application' (0.069830)
   3. ' Capitals' (0.054801)
   4. 'oplayer' (0.053319)
   5. ' capital' (0.051834)
   6. 'ittings' (0.051667)
   7. ' Capital' (0.051532)
   8. ' none' (0.049801)
   9. ' (::' (0.048963)
  10. 'sched' (0.048586)
  11. 'imas' (0.047446)
  12. 'ZF' (0.046794)
  13. 'undi' (0.045905)
  14. 'hoot' (0.045539)
  15. 'AGED' (0.045385)
  16. '#ad' (0.044518)
  17. ' capitals' (0.042818)
  18. 'hed' (0.042609)
  19. 'urm' (0.042548)
  20. '.createSequentialGroup' (0.042473)

Layer 20 (after block 19) (entropy: 11.704):
   1. ' Capital' (0.069651)
   2. 'ABCDEFGHIJKLMNOP' (0.067242)
   3. 'urm' (0.065879)
   4. ' Capitals' (0.063160)
   5. ' capital' (0.059079)
   6. ')application' (0.057570)
   7. ' Washington' (0.055530)
   8. ' capitals' (0.053067)
   9. 'ashington' (0.046419)
  10. ' London' (0.045435)
  11. 'sched' (0.044061)
  12. '.openConnection' (0.043579)
  13. 'ittings' (0.042531)
  14. 'Capital' (0.041782)
  15. '-NLS' (0.041470)
  16. '#ad' (0.041261)
  17. 'Washington' (0.040940)
  18. ' Federal' (0.040878)
  19. ' Rome' (0.040750)
  20. ' federally' (0.039717)

Layer 21 (after block 20) (entropy: 11.695):
   1. ' capital' (0.083333)
   2. ' Capital' (0.075808)
   3. ' Capitals' (0.073377)
   4. ' capitals' (0.064858)
   5. 'urm' (0.053124)
   6. ')application' (0.050923)
   7. ' Washington' (0.050212)
   8. 'Capital' (0.048564)
   9. 'ABCDEFGHIJKLMNOP' (0.047096)
  10. ' CAPITAL' (0.047069)
  11. ' Scha' (0.044490)
  12. ' Berlin' (0.043539)
  13. ' London' (0.042573)
  14. '.openConnection' (0.041278)
  15. 'capital' (0.039879)
  16. 'Washington' (0.039418)
  17. ' Erotische' (0.039093)
  18. 'urum' (0.038879)
  19. '-NLS' (0.038518)
  20. 'imas' (0.037969)

Layer 22 (after block 21) (entropy: 11.672):
   1. ' Berlin' (0.095062)
   2. ' capital' (0.077939)
   3. ' Capitals' (0.075755)
   4. ' Capital' (0.066226)
   5. ' Washington' (0.064321)
   6. ' Federal' (0.055670)
   7. ' London' (0.046364)
   8. 'Capital' (0.044340)
   9. ' capitals' (0.043392)
  10. ')application' (0.042377)
  11. 'Washington' (0.041495)
  12. ' federal' (0.040589)
  13. ' federally' (0.040106)
  14. 'Federal' (0.039470)
  15. ' none' (0.039055)
  16. 'Berlin' (0.039012)
  17. 'urm' (0.038493)
  18. 'ashington' (0.036911)
  19. ' proficient' (0.036738)
  20. ' CAPITAL' (0.036685)

Layer 23 (after block 22) (entropy: 11.661):
   1. ' Berlin' (0.124832)
   2. ' Capitals' (0.076461)
   3. ' capital' (0.073478)
   4. ' Capital' (0.070987)
   5. ' Washington' (0.059861)
   6. 'Berlin' (0.052846)
   7. ' capitals' (0.046750)
   8. ' London' (0.046605)
   9. 'Capital' (0.046382)
  10. ' CAPITAL' (0.041161)
  11. 'Washington' (0.039256)
  12. ' federally' (0.039190)
  13. 'urm' (0.037698)
  14. '.openConnection' (0.037206)
  15. 'ashington' (0.036891)
  16. ' Federal' (0.034707)
  17. ' none' (0.034197)
  18. ' reun' (0.034015)
  19. ' Rome' (0.033838)
  20. ')application' (0.033639)

Layer 24 (after block 23) (entropy: 11.625):
   1. ' Berlin' (0.186696)
   2. ' Washington' (0.085043)
   3. ' Capitals' (0.073501)
   4. ' capital' (0.069369)
   5. ' Capital' (0.068778)
   6. 'Berlin' (0.061061)
   7. 'ashington' (0.039465)
   8. 'Washington' (0.038825)
   9. 'Capital' (0.037309)
  10. ' CAPITAL' (0.036293)
  11. ' Islamabad' (0.034985)
  12. ' berlin' (0.034788)
  13. ' capitals' (0.033597)
  14. ' London' (0.030737)
  15. ' federally' (0.030683)
  16. ' Federal' (0.028624)
  17. ' Bras' (0.028279)
  18. ' Tall' (0.027819)
  19. ' Westminster' (0.027132)
  20. '柏' (0.027016)

Layer 25 (after block 24) (entropy: 11.588):
   1. ' Berlin' (0.315813)
   2. 'Berlin' (0.099531)
   3. ' capital' (0.056906)
   4. ' Capital' (0.051315)
   5. ' Capitals' (0.047300)
   6. ' berlin' (0.039993)
   7. ' Islamabad' (0.037372)
   8. ' Washington' (0.033676)
   9. '柏' (0.031634)
  10. 'Capital' (0.028097)
  11. ' Sans' (0.027940)
  12. ' Bon' (0.027373)
  13. ' Ankara' (0.027125)
  14. ' Charl' (0.026271)
  15. ' federally' (0.025988)
  16. ' CAPITAL' (0.025585)
  17. ' Beijing' (0.024851)
  18. ' Paris' (0.024436)
  19. ' Federal' (0.024405)
  20. ' Canberra' (0.024390)

Layer 26 (after block 25) (entropy: 11.550):
   1. ' Berlin' (0.404527)
   2. 'Berlin' (0.140861)
   3. ' berlin' (0.049087)
   4. ' Islamabad' (0.035489)
   5. ' Capitals' (0.030026)
   6. ' capital' (0.029738)
   7. '柏' (0.025484)
   8. ' Washington' (0.024927)
   9. ' Paris' (0.024145)
  10. ' Capital' (0.023859)
  11. ' Tall' (0.023578)
  12. ' London' (0.023317)
  13. 'AREST' (0.022318)
  14. ' Bon' (0.021495)
  15. ' Ankara' (0.021167)
  16. ' Frankfurt' (0.020546)
  17. ' BER' (0.020275)
  18. '東京' (0.019892)
  19. ' Beijing' (0.019694)
  20. ' Ber' (0.019575)

Layer 27 (after block 26) (entropy: 11.484):
   1. ' Berlin' (0.488773)
   2. 'Berlin' (0.158301)
   3. ' berlin' (0.044301)
   4. ' BER' (0.028679)
   5. ' Bon' (0.027942)
   6. ' Ber' (0.025600)
   7. '柏' (0.022372)
   8. ' Бер' (0.020865)
   9. ' Islamabad' (0.019061)
  10. ' Brand' (0.018933)
  11. ' Capitals' (0.016412)
  12. ' capital' (0.016119)
  13. ' Ankara' (0.015204)
  14. '東京' (0.015096)
  15. 'olis' (0.014163)
  16. ' Sans' (0.014114)
  17. ' Charl' (0.014087)
  18. ' Capital' (0.013564)
  19. ' Madrid' (0.013419)
  20. ' London' (0.012994)

Layer 28 (after block 27) (entropy: 11.412):
   1. ' Berlin' (0.517558)
   2. 'Berlin' (0.160793)
   3. ' BER' (0.036310)
   4. ' berlin' (0.035836)
   5. ' Ber' (0.033707)
   6. ' Бер' (0.029792)
   7. ' Bon' (0.027023)
   8. '柏' (0.021075)
   9. ' Brand' (0.016602)
  10. ' Sans' (0.014646)
  11. 'olis' (0.011797)
  12. ' federally' (0.011349)
  13. ' Charl' (0.010886)
  14. ' Bern' (0.010694)
  15. 'ベル' (0.010664)
  16. ' Capitals' (0.010603)
  17. 'Ber' (0.010417)
  18. ' Tier' (0.010372)
  19. 'Brand' (0.009985)
  20. ' Bras' (0.009892)

Layer 29 (after block 28) (entropy: 11.324):
   1. ' Berlin' (0.466619)
   2. 'Berlin' (0.140699)
   3. ' BER' (0.063076)
   4. ' berlin' (0.036243)
   5. ' Бер' (0.033528)
   6. ' Ber' (0.032096)
   7. '-B' (0.027035)
   8. '柏' (0.023424)
   9. ' Bon' (0.023006)
  10. ' federally' (0.015714)
  11. ' Charl' (0.015661)
  12. 'BER' (0.015574)
  13. ' B' (0.015304)
  14. 'ob' (0.014673)
  15. 'Ber' (0.013815)
  16. 'olis' (0.013591)
  17. ' Tier' (0.013260)
  18. '	B' (0.012676)
  19. ' Brand' (0.012595)
  20. 'ber' (0.011412)

Layer 30 (after block 29) (entropy: 11.085):
   1. ' Berlin' (0.474737)
   2. 'Berlin' (0.102062)
   3. ' BER' (0.062573)
   4. ' Ber' (0.058839)
   5. ' Бер' (0.045596)
   6. ' berlin' (0.034498)
   7. ' Bon' (0.026957)
   8. '-B' (0.025712)
   9. '柏' (0.025332)
  10. 'Ber' (0.021588)
  11. ' ber' (0.019456)
  12. ' B' (0.015756)
  13. '	B' (0.014542)
  14. '_B' (0.011718)
  15. ' Tier' (0.011649)
  16. '_ber' (0.010613)
  17. ' Bern' (0.010455)
  18. ' бер' (0.009895)
  19. ' federally' (0.009179)
  20. ' capital' (0.008845)

Layer 31 (after block 30) (entropy: 10.157):
   1. ' Berlin' (0.637307)
   2. 'Berlin' (0.069254)
   3. ' BER' (0.050146)
   4. ' Ber' (0.042819)
   5. ' berlin' (0.023756)
   6. ' Бер' (0.021443)
   7. ' ber' (0.019593)
   8. ' Bern' (0.018116)
   9. ' Bon' (0.016517)
  10. ' None' (0.015811)
  11. '	B' (0.011045)
  12. ' ' (0.009446)
  13. ' none' (0.009271)
  14. '柏' (0.008917)
  15. ' Germany' (0.008917)
  16. 'Ber' (0.008627)
  17. ' Capital' (0.007863)
  18. 'ber' (0.007492)
  19. ' capital' (0.007152)
  20. ' бер' (0.006509)

Layer 32 (after block 31) (entropy: 1.357):
   1. ' Berlin' (0.911631)
   2. ' Germany' (0.020692)
   3. ' The' (0.016795)
   4. ' ' (0.007791)
   5. ' Ber' (0.004861)
   6. ' This' (0.003956)
   7. ' ' (0.003401)
   8. 'Berlin' (0.003353)
   9. ' (' (0.003349)
  10. ' None' (0.003314)
  11. ' A' (0.002937)
  12. ' Bon' (0.002826)
  13. ' What' (0.002788)
  14. ' It' (0.002423)
  15. ' the' (0.001933)
  16. ' In' (0.001777)
  17. '<|end_of_text|>' (0.001701)
  18. ' There' (0.001530)
  19. ' 
' (0.001498)
  20. ' Frankfurt' (0.001445)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 1.181):
   1. ' Berlin' (0.919971)
   2. ' Germany' (0.019194)
   3. ' The' (0.015508)
   4. ' ' (0.007071)
   5. ' Ber' (0.004366)
   6. ' This' (0.003537)
   7. ' ' (0.003031)
   8. 'Berlin' (0.002987)
   9. ' (' (0.002984)
  10. ' None' (0.002952)
  11. ' A' (0.002609)
  12. ' Bon' (0.002508)
  13. ' What' (0.002473)
  14. ' It' (0.002143)
  15. ' the' (0.001701)
  16. ' In' (0.001561)
  17. '<|end_of_text|>' (0.001492)
  18. ' There' (0.001340)
  19. ' 
' (0.001310)
  20. ' Frankfurt' (0.001263)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 4.174)
   1. ' a' (0.483297)
   2. ' one' (0.098161)
   3. ' the' (0.088371)
   4. ' also' (0.083260)
   5. ' home' (0.054181)
   6. ' known' (0.045996)
   7. ' an' (0.044824)
   8. ' famous' (0.040644)
   9. ' full' (0.035582)
  10. ' located' (0.025684)

Prompt: 'Berlin is the capital of'
  (entropy: 0.643)
   1. ' Germany' (0.919133)
   2. ' the' (0.053856)
   3. ' and' (0.007748)
   4. ' germany' (0.003449)
   5. ' modern' (0.003050)
   6. ' Berlin' (0.002971)
   7. ' united' (0.002697)
   8. ' German' (0.002561)
   9. ' Europe' (0.002268)
  10. ' a' (0.002267)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 5.067)
   1. ' Berlin' (0.342206)
   2. ' The' (0.120233)
   3. ' Which' (0.117180)
   4. ' If' (0.105270)
   5. ' What' (0.105038)
   6. ' (' (0.050883)
   7. ' Now' (0.043225)
   8. ' You' (0.042282)
   9. ' Or' (0.037211)
  10. ' How' (0.036473)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000)
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. ' The' (0.000000)
   4. ' ' (0.000000)
   5. ' Ber' (0.000000)
   6. ' This' (0.000000)
   7. ' ' (0.000000)
   8. 'Berlin' (0.000000)
   9. ' (' (0.000000)
  10. ' None' (0.000000)
  11. ' A' (0.000000)
  12. ' Bon' (0.000000)
  13. ' What' (0.000000)
  14. ' It' (0.000000)
  15. ' the' (0.000000)

Temperature 2.0:
  (entropy: 10.065)
   1. ' Berlin' (0.507772)
   2. ' Germany' (0.073344)
   3. ' The' (0.065926)
   4. ' ' (0.044518)
   5. ' Ber' (0.034980)
   6. ' This' (0.031483)
   7. ' ' (0.029145)
   8. 'Berlin' (0.028933)
   9. ' (' (0.028918)
  10. ' None' (0.028761)
  11. ' A' (0.027040)
  12. ' Bon' (0.026513)
  13. ' What' (0.026327)
  14. ' It' (0.024505)
  15. ' the' (0.021836)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 32
Model dimension: 4096
Number of heads: 32
Vocab size: 128256
Context length: 8192
=== END OF MODEL STATS ========

