
============================================================
EVALUATING MODEL: meta-llama/Meta-Llama-3-8B
============================================================
Loading model: meta-llama/Meta-Llama-3-8B...
Loaded pretrained model meta-llama/Meta-Llama-3-8B into HookedTransformer

=== NORMALIZATION ANALYSIS ========
Block normalization type: RMSNormPre
⚠️  RMSNorm detected but no weight/scale parameter - norm-lens will be skipped
Final normalization type: RMSNormPre
=== END NORMALIZATION ANALYSIS ====


=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['<|begin_of_text|>', 'Question', ':', ' What', ' is', ' the', ' capital', ' of', ' Germany', '?', ' Answer', ':']
Computing layer-wise predictions (memory-efficient targeted caching)...

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using RAW residual stream (unsupported normalization, skipping to avoid distortion)
Note: Shown probabilities are from full softmax (calibrated and comparable)
------------------------------------------------------------
Layer  0 (embeddings):
  (entropy: 16.969 bits):
   1. 'oren' (0.000008)
   2. 'nton' (0.000008)
   3. '977' (0.000008)
   4. 'aland' (0.000008)
   5. '賀' (0.000008)
   6. 'ceso' (0.000008)
   7. 'adil' (0.000008)
   8. 'nesc' (0.000008)
   9. 'anson' (0.000008)
  10. 'anter' (0.000008)
  11. ' Pun' (0.000008)
  12. 'ění' (0.000008)
  13. 'Ｌ' (0.000008)
  14. ' arası' (0.000008)
  15. 'ophon' (0.000008)
  16. ' Pension' (0.000008)
  17. 'SetName' (0.000008)
  18. ' edin' (0.000008)
  19. 'writeln' (0.000008)
  20. 'undler' (0.000008)

Layer  1 (after transformer block 0) (entropy: 16.968 bits):
   1. 'ря' (0.000009)
   2. 'стров' (0.000009)
   3. 'chine' (0.000009)
   4. 'Leap' (0.000009)
   5. 'itler' (0.000009)
   6. 'üstü' (0.000009)
   7. '妙' (0.000009)
   8. 'ezi' (0.000009)
   9. 'elter' (0.000009)
  10. 'oğ' (0.000009)
  11. 'WR' (0.000009)
  12. 'يون' (0.000009)
  13. 'WWW' (0.000009)
  14. ' Cater' (0.000009)
  15. ' Ven' (0.000009)
  16. ' ​​' (0.000009)
  17. 'CompatActivity' (0.000009)
  18. 'ATOR' (0.000009)
  19. 'createFrom' (0.000009)
  20. 'ilt' (0.000009)

Layer  2 (after transformer block 1) (entropy: 16.968 bits):
   1. ' ​​' (0.000009)
   2. 'aten' (0.000009)
   3. 'atten' (0.000009)
   4. '691' (0.000009)
   5. 'seys' (0.000009)
   6. 'eden' (0.000009)
   7. 'bai' (0.000009)
   8. 'dna' (0.000009)
   9. 'ルド' (0.000009)
  10. '851' (0.000009)
  11. 'bette' (0.000009)
  12. 'building' (0.000009)
  13. '-letter' (0.000009)
  14. ' klu' (0.000009)
  15. 'рів' (0.000009)
  16. 'ilt' (0.000009)
  17. 'aniem' (0.000009)
  18. 'dsl' (0.000009)
  19. 'ActiveSheet' (0.000009)
  20. ' Opr' (0.000009)

Layer  3 (after transformer block 2) (entropy: 16.966 bits):
   1. 'atten' (0.000010)
   2. ' Ven' (0.000010)
   3. 'เ' (0.000010)
   4. 'BA' (0.000010)
   5. '.Decode' (0.000010)
   6. 'orz' (0.000010)
   7. '536' (0.000010)
   8. '﻿#' (0.000010)
   9. 'imas' (0.000010)
  10. '施' (0.000010)
  11. 'ird' (0.000010)
  12. 'aska' (0.000010)
  13. '.WinForms' (0.000010)
  14. '.stride' (0.000010)
  15. 'ba' (0.000010)
  16. 'Ven' (0.000010)
  17. '-letter' (0.000010)
  18. 'rud' (0.000010)
  19. ' PIE' (0.000010)
  20. 'erah' (0.000010)

Layer  4 (after transformer block 3) (entropy: 16.965 bits):
   1. 'adal' (0.000011)
   2. 'chine' (0.000011)
   3. 'ADDE' (0.000011)
   4. 'Atoms' (0.000011)
   5. '�' (0.000011)
   6. '.Decode' (0.000011)
   7. 'heimer' (0.000011)
   8. '.Areas' (0.000011)
   9. 'ソン' (0.000011)
  10. 'bai' (0.000011)
  11. 'rana' (0.000011)
  12. ' ​​' (0.000011)
  13. ' Podesta' (0.000011)
  14. ' Coach' (0.000011)
  15. 'Capabilities' (0.000011)
  16. 'こんにちは' (0.000011)
  17. 'reator' (0.000011)
  18. '/Users' (0.000011)
  19. '/operator' (0.000010)
  20. '/Area' (0.000010)

Layer  5 (after transformer block 4) (entropy: 16.963 bits):
   1. 'chine' (0.000012)
   2. 'olg' (0.000011)
   3. 'XC' (0.000011)
   4. 'istrovství' (0.000011)
   5. 'IZER' (0.000011)
   6. ' Obr' (0.000011)
   7. 'เ' (0.000011)
   8. 'ekler' (0.000011)
   9. '�' (0.000011)
  10. 'RIPT' (0.000011)
  11. 'ADDE' (0.000011)
  12. 'cheng' (0.000011)
  13. 'adal' (0.000011)
  14. '.Generated' (0.000011)
  15. 'reator' (0.000011)
  16. ' Sov' (0.000011)
  17. 'STRU' (0.000011)
  18. ' bearer' (0.000011)
  19. 'OVE' (0.000011)
  20. 'еди' (0.000011)

Layer  6 (after transformer block 5) (entropy: 16.961 bits):
   1. '.Decode' (0.000012)
   2. 'aget' (0.000012)
   3. 'LANG' (0.000012)
   4. 'urname' (0.000012)
   5. 'IDO' (0.000012)
   6. '�' (0.000012)
   7. 'multipart' (0.000012)
   8. 'defgroup' (0.000012)
   9. ' Citizens' (0.000012)
  10. ''gc' (0.000012)
  11. 'остат' (0.000012)
  12. 'STREAM' (0.000012)
  13. '��' (0.000012)
  14. 'OnError' (0.000012)
  15. '_OBS' (0.000012)
  16. 'ulaire' (0.000012)
  17. 'reator' (0.000012)
  18. 'adal' (0.000012)
  19. ' multipart' (0.000012)
  20. ' nonatomic' (0.000012)

Layer  7 (after transformer block 6) (entropy: 16.960 bits):
   1. ' Cem' (0.000013)
   2. ' Citizens' (0.000013)
   3. 'итор' (0.000013)
   4. 'raž' (0.000012)
   5. 'adal' (0.000012)
   6. 'ToObject' (0.000012)
   7. 'ilip' (0.000012)
   8. 'MimeType' (0.000012)
   9. 'mpr' (0.000012)
  10. 'upd' (0.000012)
  11. 'strap' (0.000012)
  12. 'чики' (0.000012)
  13. 'urname' (0.000012)
  14. 'MPI' (0.000012)
  15. 'OnError' (0.000012)
  16. '/Area' (0.000012)
  17. '-Origin' (0.000012)
  18. ' Civ' (0.000012)
  19. 'มน' (0.000012)
  20. 'RuntimeObject' (0.000012)

Layer  8 (after transformer block 7) (entropy: 16.957 bits):
   1. 'AutoSize' (0.000013)
   2. ' RoundedRectangle' (0.000013)
   3. 'ilt' (0.000013)
   4. 'enville' (0.000013)
   5. '963' (0.000013)
   6. 'xFFF' (0.000013)
   7. 'strap' (0.000013)
   8. 'PCM' (0.000013)
   9. 'ANGED' (0.000013)
  10. 'setFlash' (0.000013)
  11. 'こんにちは' (0.000013)
  12. 'کان' (0.000013)
  13. 'LTR' (0.000013)
  14. '.GetById' (0.000013)
  15. 'ermo' (0.000013)
  16. '�' (0.000013)
  17. 'imeo' (0.000013)
  18. '/Area' (0.000013)
  19. 'wie' (0.000013)
  20. '。。

' (0.000013)

Layer  9 (after transformer block 8) (entropy: 16.956 bits):
   1. 'enville' (0.000015)
   2. '.onView' (0.000014)
   3. '.netbeans' (0.000014)
   4. 'こんにちは' (0.000014)
   5. 'iversit' (0.000014)
   6. '.xtext' (0.000014)
   7. 'PCM' (0.000014)
   8. 'شی' (0.000014)
   9. '.owl' (0.000014)
  10. 'bcm' (0.000014)
  11. 'ilir' (0.000014)
  12. 'ylland' (0.000014)
  13. 'tracer' (0.000014)
  14. '/Area' (0.000013)
  15. 'entar' (0.000013)
  16. 'VML' (0.000013)
  17. '감' (0.000013)
  18. 'ilan' (0.000013)
  19. 'stral' (0.000013)
  20. 'ینه' (0.000013)

Layer 10 (after transformer block 9) (entropy: 16.956 bits):
   1. 'PCM' (0.000016)
   2. 'iversit' (0.000015)
   3. '�' (0.000014)
   4. '.xtext' (0.000014)
   5. 'erialize' (0.000014)
   6. '孝' (0.000014)
   7. 'vanished' (0.000014)
   8. 'ilt' (0.000014)
   9. 'entes' (0.000014)
  10. 'ystore' (0.000014)
  11. 'olang' (0.000013)
  12. ' Civ' (0.000013)
  13. 'ilir' (0.000013)
  14. 'SPATH' (0.000013)
  15. 'rored' (0.000013)
  16. 'velt' (0.000013)
  17. 'цип' (0.000013)
  18. 'หาย' (0.000013)
  19. 'UILTIN' (0.000013)
  20. '_stdio' (0.000013)

Layer 11 (after transformer block 10) (entropy: 16.954 bits):
   1. 'ystack' (0.000015)
   2. 'PCM' (0.000015)
   3. 'Раз' (0.000014)
   4. ' Woody' (0.000014)
   5. 'iversit' (0.000014)
   6. 'ilir' (0.000014)
   7. 'หาย' (0.000014)
   8. '.xtext' (0.000014)
   9. 'SPATH' (0.000014)
  10. 'ustum' (0.000014)
  11. ' Multiply' (0.000014)
  12. 'рел' (0.000014)
  13. 'ystore' (0.000014)
  14. 'icies' (0.000014)
  15. ' Parenthood' (0.000014)
  16. 'oví' (0.000014)
  17. 'PCP' (0.000014)
  18. 'dik' (0.000014)
  19. '..



' (0.000014)
  20. 'LTR' (0.000013)

Layer 12 (after transformer block 11) (entropy: 16.952 bits):
   1. '.xtext' (0.000018)
   2. 'рел' (0.000016)
   3. 'ystack' (0.000016)
   4. 'PCM' (0.000015)
   5. 'FRING' (0.000015)
   6. 'urement' (0.000015)
   7. 'acific' (0.000015)
   8. '929' (0.000015)
   9. 'rete' (0.000015)
  10. '/Area' (0.000015)
  11. '곤' (0.000015)
  12. 'Advertisements' (0.000015)
  13. 'ormsg' (0.000015)
  14. '_mex' (0.000015)
  15. '539' (0.000014)
  16. ' PartialView' (0.000014)
  17. 'óst' (0.000014)
  18. 'ackbar' (0.000014)
  19. ' geschichten' (0.000014)
  20. '野' (0.000014)

Layer 13 (after transformer block 12) (entropy: 16.951 bits):
   1. 'краї' (0.000016)
   2. ' def' (0.000016)
   3. '_EDEFAULT' (0.000015)
   4. '.xtext' (0.000015)
   5. '@Id' (0.000015)
   6. 'PCM' (0.000015)
   7. '_mex' (0.000015)
   8. 'Архів' (0.000015)
   9. 'erializer' (0.000015)
  10. ' PartialView' (0.000015)
  11. ' Haupt' (0.000015)
  12. ' b' (0.000015)
  13. ' Greenwood' (0.000015)
  14. 'rophy' (0.000014)
  15. '_sid' (0.000014)
  16. '�' (0.000014)
  17. ' reins' (0.000014)
  18. 'celik' (0.000014)
  19. 'edar' (0.000014)
  20. 'remium' (0.000014)

Layer 14 (after transformer block 13) (entropy: 16.945 bits):
   1. '#ab' (0.000018)
   2. 'Архів' (0.000017)
   3. 'erece' (0.000017)
   4. '�回' (0.000017)
   5. '_mex' (0.000017)
   6. 'enderit' (0.000017)
   7. 'argin' (0.000017)
   8. 'ritel' (0.000017)
   9. 'zcze' (0.000016)
  10. 'hek' (0.000016)
  11. ' "}\' (0.000016)
  12. 'INLINE' (0.000016)
  13. 'šak' (0.000016)
  14. ' cố' (0.000016)
  15. ' Mes' (0.000016)
  16. '#ac' (0.000016)
  17. '_EDEFAULT' (0.000016)
  18. '.xtext' (0.000016)
  19. 'เฉ' (0.000016)
  20. 'umin' (0.000016)

Layer 15 (after transformer block 14) (entropy: 16.941 bits):
   1. '#af' (0.000020)
   2. ')frame' (0.000020)
   3. '#ab' (0.000019)
   4. 'enderit' (0.000019)
   5. '.datatables' (0.000019)
   6. ' dõi' (0.000019)
   7. '.utf' (0.000019)
   8. '#ad' (0.000019)
   9. 'ateg' (0.000019)
  10. 'undler' (0.000019)
  11. 'ediği' (0.000018)
  12. 'ritel' (0.000018)
  13. 'enderror' (0.000018)
  14. '#ac' (0.000018)
  15. '재' (0.000018)
  16. 'SPATH' (0.000018)
  17. 'üstü' (0.000018)
  18. 'Sdk' (0.000017)
  19. 'rowsable' (0.000017)
  20. '#index' (0.000017)

Layer 16 (after transformer block 15) (entropy: 16.935 bits):
   1. '#ab' (0.000022)
   2. 'undler' (0.000021)
   3. 'enderror' (0.000021)
   4. '_EDEFAULT' (0.000020)
   5. '.ecore' (0.000020)
   6. 'شف' (0.000020)
   7. 'umin' (0.000020)
   8. 'upo' (0.000020)
   9. '#ad' (0.000020)
  10. '.LoggerFactory' (0.000020)
  11. 'ほ' (0.000019)
  12. 'Cİ' (0.000019)
  13. 'ชอบ' (0.000019)
  14. 'erosis' (0.000019)
  15. '_mA' (0.000019)
  16. 'idal' (0.000019)
  17. '_mB' (0.000019)
  18. 'ootball' (0.000019)
  19. 'eldo' (0.000019)
  20. 'PREFIX' (0.000018)

Layer 17 (after transformer block 16) (entropy: 16.922 bits):
   1. '#ad' (0.000031)
   2. ')application' (0.000029)
   3. '#ab' (0.000029)
   4. 'oplayer' (0.000028)
   5. '/******/' (0.000027)
   6. 'ชอบ' (0.000026)
   7. 'ateg' (0.000025)
   8. '#af' (0.000024)
   9. 'ABCDEFGHIJKLMNOP' (0.000024)
  10. 'undler' (0.000024)
  11. ' thủ' (0.000023)
  12. 'ktop' (0.000023)
  13. 'Insets' (0.000023)
  14. 'imas' (0.000023)
  15. 'SendMessage' (0.000023)
  16. 'sched' (0.000023)
  17. 'cratch' (0.000022)
  18. 'ependency' (0.000022)
  19. 'etros' (0.000022)
  20. '#error' (0.000022)

Layer 18 (after transformer block 17) (entropy: 16.909 bits):
   1. '#ad' (0.000036)
   2. ')application' (0.000033)
   3. 'oplayer' (0.000033)
   4. 'ABCDEFGHIJKLMNOP' (0.000031)
   5. 'imas' (0.000030)
   6. '#ab' (0.000029)
   7. ' capital' (0.000028)
   8. 'hoot' (0.000028)
   9. 'wig' (0.000027)
  10. 'zd' (0.000027)
  11. ' Capitals' (0.000026)
  12. 'sched' (0.000026)
  13. 'ชอบ' (0.000026)
  14. 'NullException' (0.000025)
  15. 'undler' (0.000025)
  16. 'etros' (0.000025)
  17. 'undi' (0.000025)
  18. ' none' (0.000025)
  19. 'SendMessage' (0.000025)
  20. ' capitals' (0.000025)

Layer 19 (after transformer block 18) (entropy: 16.902 bits):
   1. 'ABCDEFGHIJKLMNOP' (0.000045)
   2. ')application' (0.000043)
   3. ' Capitals' (0.000034)
   4. 'oplayer' (0.000033)
   5. ' capital' (0.000032)
   6. 'ittings' (0.000032)
   7. ' Capital' (0.000032)
   8. ' none' (0.000031)
   9. ' (::' (0.000030)
  10. 'sched' (0.000030)
  11. 'imas' (0.000029)
  12. 'ZF' (0.000029)
  13. 'undi' (0.000028)
  14. 'hoot' (0.000028)
  15. 'AGED' (0.000028)
  16. '#ad' (0.000027)
  17. ' capitals' (0.000026)
  18. 'hed' (0.000026)
  19. 'urm' (0.000026)
  20. '.createSequentialGroup' (0.000026)

Layer 20 (after transformer block 19) (entropy: 16.885 bits):
   1. ' Capital' (0.000055)
   2. 'ABCDEFGHIJKLMNOP' (0.000053)
   3. 'urm' (0.000052)
   4. ' Capitals' (0.000050)
   5. ' capital' (0.000047)
   6. ')application' (0.000046)
   7. ' Washington' (0.000044)
   8. ' capitals' (0.000042)
   9. 'ashington' (0.000037)
  10. ' London' (0.000036)
  11. 'sched' (0.000035)
  12. '.openConnection' (0.000035)
  13. 'ittings' (0.000034)
  14. 'Capital' (0.000033)
  15. '-NLS' (0.000033)
  16. '#ad' (0.000033)
  17. 'Washington' (0.000033)
  18. ' Federal' (0.000033)
  19. ' Rome' (0.000032)
  20. ' federally' (0.000032)

Layer 21 (after transformer block 20) (entropy: 16.872 bits):
   1. ' capital' (0.000078)
   2. ' Capital' (0.000071)
   3. ' Capitals' (0.000068)
   4. ' capitals' (0.000060)
   5. 'urm' (0.000049)
   6. ')application' (0.000047)
   7. ' Washington' (0.000047)
   8. 'Capital' (0.000045)
   9. 'ABCDEFGHIJKLMNOP' (0.000044)
  10. ' CAPITAL' (0.000044)
  11. ' Scha' (0.000041)
  12. ' Berlin' (0.000041)
  13. ' London' (0.000040)
  14. '.openConnection' (0.000038)
  15. 'capital' (0.000037)
  16. 'Washington' (0.000037)
  17. ' Erotische' (0.000036)
  18. 'urum' (0.000036)
  19. '-NLS' (0.000036)
  20. 'imas' (0.000035)

Layer 22 (after transformer block 21) (entropy: 16.839 bits):
   1. ' Berlin' (0.000123)
   2. ' capital' (0.000101)
   3. ' Capitals' (0.000098)
   4. ' Capital' (0.000086)
   5. ' Washington' (0.000083)
   6. ' Federal' (0.000072)
   7. ' London' (0.000060)
   8. 'Capital' (0.000057)
   9. ' capitals' (0.000056)
  10. ')application' (0.000055)
  11. 'Washington' (0.000054)
  12. ' federal' (0.000053)
  13. ' federally' (0.000052)
  14. 'Federal' (0.000051)
  15. ' none' (0.000051)
  16. 'Berlin' (0.000051)
  17. 'urm' (0.000050)
  18. 'ashington' (0.000048)
  19. ' proficient' (0.000048)
  20. ' CAPITAL' (0.000048)

Layer 23 (after transformer block 22) (entropy: 16.823 bits):
   1. ' Berlin' (0.000202)
   2. ' Capitals' (0.000124)
   3. ' capital' (0.000119)
   4. ' Capital' (0.000115)
   5. ' Washington' (0.000097)
   6. 'Berlin' (0.000086)
   7. ' capitals' (0.000076)
   8. ' London' (0.000076)
   9. 'Capital' (0.000075)
  10. ' CAPITAL' (0.000067)
  11. 'Washington' (0.000064)
  12. ' federally' (0.000063)
  13. 'urm' (0.000061)
  14. '.openConnection' (0.000060)
  15. 'ashington' (0.000060)
  16. ' Federal' (0.000056)
  17. ' none' (0.000055)
  18. ' reun' (0.000055)
  19. ' Rome' (0.000055)
  20. ')application' (0.000054)

Layer 24 (after transformer block 23) (entropy: 16.771 bits):
   1. ' Berlin' (0.000526)
   2. ' Washington' (0.000240)
   3. ' Capitals' (0.000207)
   4. ' capital' (0.000196)
   5. ' Capital' (0.000194)
   6. 'Berlin' (0.000172)
   7. 'ashington' (0.000111)
   8. 'Washington' (0.000109)
   9. 'Capital' (0.000105)
  10. ' CAPITAL' (0.000102)
  11. ' Islamabad' (0.000099)
  12. ' berlin' (0.000098)
  13. ' capitals' (0.000095)
  14. ' London' (0.000087)
  15. ' federally' (0.000087)
  16. ' Federal' (0.000081)
  17. ' Bras' (0.000080)
  18. ' Tall' (0.000078)
  19. ' Westminster' (0.000076)
  20. '柏' (0.000076)

Layer 25 (after transformer block 24) (entropy: 16.718 bits):
   1. ' Berlin' (0.001421)
   2. 'Berlin' (0.000448)
   3. ' capital' (0.000256)
   4. ' Capital' (0.000231)
   5. ' Capitals' (0.000213)
   6. ' berlin' (0.000180)
   7. ' Islamabad' (0.000168)
   8. ' Washington' (0.000152)
   9. '柏' (0.000142)
  10. 'Capital' (0.000126)
  11. ' Sans' (0.000126)
  12. ' Bon' (0.000123)
  13. ' Ankara' (0.000122)
  14. ' Charl' (0.000118)
  15. ' federally' (0.000117)
  16. ' CAPITAL' (0.000115)
  17. ' Beijing' (0.000112)
  18. ' Paris' (0.000110)
  19. ' Federal' (0.000110)
  20. ' Canberra' (0.000110)

Layer 26 (after transformer block 25) (entropy: 16.664 bits):
   1. ' Berlin' (0.002726)
   2. 'Berlin' (0.000949)
   3. ' berlin' (0.000331)
   4. ' Islamabad' (0.000239)
   5. ' Capitals' (0.000202)
   6. ' capital' (0.000200)
   7. '柏' (0.000172)
   8. ' Washington' (0.000168)
   9. ' Paris' (0.000163)
  10. ' Capital' (0.000161)
  11. ' Tall' (0.000159)
  12. ' London' (0.000157)
  13. 'AREST' (0.000150)
  14. ' Bon' (0.000145)
  15. ' Ankara' (0.000143)
  16. ' Frankfurt' (0.000138)
  17. ' BER' (0.000137)
  18. '東京' (0.000134)
  19. ' Beijing' (0.000133)
  20. ' Ber' (0.000132)

Layer 27 (after transformer block 26) (entropy: 16.567 bits):
   1. ' Berlin' (0.005738)
   2. 'Berlin' (0.001858)
   3. ' berlin' (0.000520)
   4. ' BER' (0.000337)
   5. ' Bon' (0.000328)
   6. ' Ber' (0.000301)
   7. '柏' (0.000263)
   8. ' Бер' (0.000245)
   9. ' Islamabad' (0.000224)
  10. ' Brand' (0.000222)
  11. ' Capitals' (0.000193)
  12. ' capital' (0.000189)
  13. ' Ankara' (0.000178)
  14. '東京' (0.000177)
  15. 'olis' (0.000166)
  16. ' Sans' (0.000166)
  17. ' Charl' (0.000165)
  18. ' Capital' (0.000159)
  19. ' Madrid' (0.000158)
  20. ' London' (0.000153)

Layer 28 (after transformer block 27) (entropy: 16.465 bits):
   1. ' Berlin' (0.008988)
   2. 'Berlin' (0.002792)
   3. ' BER' (0.000631)
   4. ' berlin' (0.000622)
   5. ' Ber' (0.000585)
   6. ' Бер' (0.000517)
   7. ' Bon' (0.000469)
   8. '柏' (0.000366)
   9. ' Brand' (0.000288)
  10. ' Sans' (0.000254)
  11. 'olis' (0.000205)
  12. ' federally' (0.000197)
  13. ' Charl' (0.000189)
  14. ' Bern' (0.000186)
  15. 'ベル' (0.000185)
  16. ' Capitals' (0.000184)
  17. 'Ber' (0.000181)
  18. ' Tier' (0.000180)
  19. 'Brand' (0.000173)
  20. ' Bras' (0.000172)

Layer 29 (after transformer block 28) (entropy: 16.337 bits):
   1. ' Berlin' (0.010165)
   2. 'Berlin' (0.003065)
   3. ' BER' (0.001374)
   4. ' berlin' (0.000790)
   5. ' Бер' (0.000730)
   6. ' Ber' (0.000699)
   7. '-B' (0.000589)
   8. '柏' (0.000510)
   9. ' Bon' (0.000501)
  10. ' federally' (0.000342)
  11. ' Charl' (0.000341)
  12. 'BER' (0.000339)
  13. ' B' (0.000333)
  14. 'ob' (0.000320)
  15. 'Ber' (0.000301)
  16. 'olis' (0.000296)
  17. ' Tier' (0.000289)
  18. '	B' (0.000276)
  19. ' Brand' (0.000274)
  20. 'ber' (0.000249)

Layer 30 (after transformer block 29) (entropy: 15.993 bits):
   1. ' Berlin' (0.019857)
   2. 'Berlin' (0.004269)
   3. ' BER' (0.002617)
   4. ' Ber' (0.002461)
   5. ' Бер' (0.001907)
   6. ' berlin' (0.001443)
   7. ' Bon' (0.001128)
   8. '-B' (0.001075)
   9. '柏' (0.001060)
  10. 'Ber' (0.000903)
  11. ' ber' (0.000814)
  12. ' B' (0.000659)
  13. '	B' (0.000608)
  14. '_B' (0.000490)
  15. ' Tier' (0.000487)
  16. '_ber' (0.000444)
  17. ' Bern' (0.000437)
  18. ' бер' (0.000414)
  19. ' federally' (0.000384)
  20. ' capital' (0.000370)

Layer 31 (after transformer block 30) (entropy: 14.654 bits):
   1. ' Berlin' (0.077747)
   2. 'Berlin' (0.008449)
   3. ' BER' (0.006117)
   4. ' Ber' (0.005224)
   5. ' berlin' (0.002898)
   6. ' Бер' (0.002616)
   7. ' ber' (0.002390)
   8. ' Bern' (0.002210)
   9. ' Bon' (0.002015)
  10. ' None' (0.001929)
  11. '	B' (0.001347)
  12. ' ' (0.001152)
  13. ' none' (0.001131)
  14. '柏' (0.001088)
  15. ' Germany' (0.001088)
  16. 'Ber' (0.001052)
  17. ' Capital' (0.000959)
  18. 'ber' (0.000914)
  19. ' capital' (0.000872)
  20. ' бер' (0.000794)

Layer 32 (after transformer block 31) (entropy: 1.958 bits):
   1. ' Berlin' (0.836531)
   2. ' Germany' (0.018987)
   3. ' The' (0.015412)
   4. ' ' (0.007149)
   5. ' Ber' (0.004460)
   6. ' This' (0.003630)
   7. ' ' (0.003121)
   8. 'Berlin' (0.003077)
   9. ' (' (0.003074)
  10. ' None' (0.003041)
  11. ' A' (0.002695)
  12. ' Bon' (0.002593)
  13. ' What' (0.002558)
  14. ' It' (0.002223)
  15. ' the' (0.001774)
  16. ' In' (0.001631)
  17. '<|end_of_text|>' (0.001561)
  18. ' There' (0.001404)
  19. ' 
' (0.001374)
  20. ' Frankfurt' (0.001326)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 1.704 bits):
   1. ' Berlin' (0.857033)
   2. ' Germany' (0.017881)
   3. ' The' (0.014447)
   4. ' ' (0.006588)
   5. ' Ber' (0.004067)
   6. ' This' (0.003295)
   7. ' ' (0.002823)
   8. 'Berlin' (0.002783)
   9. ' (' (0.002780)
  10. ' None' (0.002750)
  11. ' A' (0.002430)
  12. ' Bon' (0.002337)
  13. ' What' (0.002304)
  14. ' It' (0.001996)
  15. ' the' (0.001585)
  16. ' In' (0.001454)
  17. '<|end_of_text|>' (0.001390)
  18. ' There' (0.001248)
  19. ' 
' (0.001221)
  20. ' Frankfurt' (0.001177)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 6.022 bits)
   1. ' a' (0.287639)
   2. ' one' (0.058422)
   3. ' the' (0.052595)
   4. ' also' (0.049553)
   5. ' home' (0.032247)
   6. ' known' (0.027375)
   7. ' an' (0.026677)
   8. ' famous' (0.024189)
   9. ' full' (0.021177)
  10. ' located' (0.015286)

Prompt: 'Berlin is the capital of'
  (entropy: 0.928 bits)
   1. ' Germany' (0.895601)
   2. ' the' (0.052477)
   3. ' and' (0.007550)
   4. ' germany' (0.003361)
   5. ' modern' (0.002972)
   6. ' Berlin' (0.002895)
   7. ' united' (0.002628)
   8. ' German' (0.002496)
   9. ' Europe' (0.002210)
  10. ' a' (0.002209)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 7.310 bits)
   1. ' Berlin' (0.149300)
   2. ' The' (0.052456)
   3. ' Which' (0.051124)
   4. ' If' (0.045928)
   5. ' What' (0.045826)
   6. ' (' (0.022199)
   7. ' Now' (0.018858)
   8. ' You' (0.018447)
   9. ' Or' (0.016234)
  10. ' How' (0.015913)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000 bits)
   1. ' Berlin' (1.000000)
   2. ' Germany' (0.000000)
   3. ' The' (0.000000)
   4. ' ' (0.000000)
   5. ' Ber' (0.000000)
   6. ' This' (0.000000)
   7. ' ' (0.000000)
   8. 'Berlin' (0.000000)
   9. ' (' (0.000000)
  10. ' None' (0.000000)
  11. ' A' (0.000000)
  12. ' Bon' (0.000000)
  13. ' What' (0.000000)
  14. ' It' (0.000000)
  15. ' the' (0.000000)

Temperature 2.0:
  (entropy: 14.520 bits)
   1. ' Berlin' (0.034909)
   2. ' Germany' (0.005042)
   3. ' The' (0.004532)
   4. ' ' (0.003061)
   5. ' Ber' (0.002405)
   6. ' This' (0.002164)
   7. ' ' (0.002004)
   8. 'Berlin' (0.001989)
   9. ' (' (0.001988)
  10. ' None' (0.001977)
  11. ' A' (0.001859)
  12. ' Bon' (0.001823)
  13. ' What' (0.001810)
  14. ' It' (0.001685)
  15. ' the' (0.001501)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 32
Model dimension: 4096
Number of heads: 32
Vocab size: 128256
Context length: 8192
=== END OF MODEL STATS ========

