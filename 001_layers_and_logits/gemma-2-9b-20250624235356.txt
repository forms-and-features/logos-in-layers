
============================================================
EVALUATING MODEL: google/gemma-2-9b
============================================================
Loading model: google/gemma-2-9b...
Loaded pretrained model google/gemma-2-9b into HookedTransformer

=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['<bos>', 'Question', ':', ' What', ' is', ' the', ' capital', ' of', ' Germany', '?', ' Answer', ':']

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using NORMALIZED residual stream (LayerNorm applied - more accurate)
------------------------------------------------------------
Layer  0 (entropy: 0.000):
   1. ':' (1.000000)
   2. ',' (0.000000)
   3. ' ' (0.000000)
   4. '.' (0.000000)
   5. '-' (0.000000)
   6. '<pad>' (0.000000)
   7. '<eos>' (0.000000)
   8. '<bos>' (0.000000)
   9. '<unk>' (0.000000)
  10. '<mask>' (0.000000)
  11. '<2mass>' (0.000000)
  12. '[@BOS@]' (0.000000)
  13. '<unused0>' (0.000000)
  14. '<unused1>' (0.000000)
  15. '<unused2>' (0.000000)
  16. '<unused3>' (0.000000)
  17. '<unused4>' (0.000000)
  18. '<unused5>' (0.000000)
  19. '<unused6>' (0.000000)
  20. '<unused7>' (0.000000)

Layer  1 (entropy: 0.000):
   1. ':' (1.000000)
   2. '’:' (0.000000)
   3. ':' (0.000000)
   4. ' :' (0.000000)
   5. ':}' (0.000000)
   6. ':</' (0.000000)
   7. '!:' (0.000000)
   8. '】:' (0.000000)
   9. ':',' (0.000000)
  10. '：' (0.000000)
  11. '»:' (0.000000)
  12. ':");' (0.000000)
  13. '®:' (0.000000)
  14. ':",' (0.000000)
  15. '}}:' (0.000000)
  16. '+:' (0.000000)
  17. ':')' (0.000000)
  18. '”:' (0.000000)
  19. ' _:' (0.000000)
  20. '}:' (0.000000)

Layer  2 (entropy: 0.000):
   1. ':' (1.000000)
   2. '：' (0.000000)
   3. '}:' (0.000000)
   4. ' :' (0.000000)
   5. '✨:' (0.000000)
   6. '’:' (0.000000)
   7. '»:' (0.000000)
   8. ']:' (0.000000)
   9. '%:' (0.000000)
  10. ':}' (0.000000)
  11. '!:' (0.000000)
  12. '】:' (0.000000)
  13. '︰' (0.000000)
  14. '﹕' (0.000000)
  15. ' ：' (0.000000)
  16. '*:' (0.000000)
  17. '**:' (0.000000)
  18. ':',' (0.000000)
  19. ':",' (0.000000)
  20. '':' (0.000000)

Layer  3 (entropy: 0.000):
   1. ':' (1.000000)
   2. ']:' (0.000000)
   3. '’:' (0.000000)
   4. ':}' (0.000000)
   5. '：' (0.000000)
   6. ' :' (0.000000)
   7. '】:' (0.000000)
   8. '»:' (0.000000)
   9. '}:' (0.000000)
  10. '':' (0.000000)
  11. '):' (0.000000)
  12. '":' (0.000000)
  13. '”:' (0.000000)
  14. ':',' (0.000000)
  15. ':",' (0.000000)
  16. ':]' (0.000000)
  17. ' Umum' (0.000000)
  18. '.:' (0.000000)
  19. '$:' (0.000000)
  20. ':\' (0.000000)

Layer  4 (entropy: 0.000):
   1. ':' (1.000000)
   2. ' :' (0.000000)
   3. '：' (0.000000)
   4. '’:' (0.000000)
   5. '):' (0.000000)
   6. '":' (0.000000)
   7. '}:' (0.000000)
   8. '$:' (0.000000)
   9. '*:' (0.000000)
  10. '»:' (0.000000)
  11. '':' (0.000000)
  12. ':}' (0.000000)
  13. ':",' (0.000000)
  14. ':"' (0.000000)
  15. '<td>' (0.000000)
  16. '.:' (0.000000)
  17. ' Geheimnis' (0.000000)
  18. ']:' (0.000000)
  19. '»' (0.000000)
  20. '{' (0.000000)

Layer  5 (entropy: 0.000):
   1. ':' (0.999997)
   2. ' :' (0.000001)
   3. ' menjawab' (0.000001)
   4. ' Batalla' (0.000000)
   5. '：' (0.000000)
   6. ':}' (0.000000)
   7. '

' (0.000000)
   8. '':' (0.000000)
   9. ' Wassers' (0.000000)
  10. ' Grecs' (0.000000)
  11. ''' (0.000000)
  12. '":' (0.000000)
  13. ' Mexique' (0.000000)
  14. ' selaku' (0.000000)
  15. ' Gebir' (0.000000)
  16. ':\' (0.000000)
  17. ' Respuesta' (0.000000)
  18. ':.' (0.000000)
  19. ' Antwort' (0.000000)
  20. ' Münch' (0.000000)

Layer  6 (entropy: 0.000):
   1. ':' (0.999995)
   2. ' :' (0.000002)
   3. ' Grecs' (0.000001)
   4. ' Batalla' (0.000001)
   5. '’' (0.000000)
   6. ':}' (0.000000)
   7. ' Mexique' (0.000000)
   8. '：' (0.000000)
   9. '':' (0.000000)
  10. ' menjawab' (0.000000)
  11. '’:' (0.000000)
  12. ' Jesucristo' (0.000000)
  13. ' explica' (0.000000)
  14. ' =' (0.000000)
  15. '":' (0.000000)
  16. ' hjemmeside' (0.000000)
  17. ':",' (0.000000)
  18. ' resposta' (0.000000)
  19. ' japonais' (0.000000)
  20. ' chêne' (0.000000)

Layer  7 (entropy: 0.002):
   1. ':' (0.999832)
   2. ' answer' (0.000142)
   3. 'answer' (0.000011)
   4. ' Antwort' (0.000004)
   5. ' answers' (0.000003)
   6. 'Answer' (0.000002)
   7. ' resposta' (0.000002)
   8. ' Batalla' (0.000002)
   9. ' :' (0.000001)
  10. ' =' (0.000000)
  11. '：' (0.000000)
  12. ' Answer' (0.000000)
  13. ' menjawab' (0.000000)
  14. '2' (0.000000)
  15. 'resposta' (0.000000)
  16. '

' (0.000000)
  17. '  ' (0.000000)
  18. ' Respuesta' (0.000000)
  19. '":' (0.000000)
  20. ' Münch' (0.000000)

Layer  8 (entropy: 0.055):
   1. ':' (0.992027)
   2. ' answer' (0.005542)
   3. 'Answer' (0.001426)
   4. 'answer' (0.000410)
   5. '  ' (0.000224)
   6. '

' (0.000105)
   7. '1' (0.000080)
   8. '
' (0.000053)
   9. ' answers' (0.000046)
  10. '2' (0.000026)
  11. 's' (0.000016)
  12. ' :' (0.000008)
  13. ' =' (0.000006)
  14. ' Antwort' (0.000005)
  15. ' ' (0.000005)
  16. '<strong>' (0.000004)
  17. 'A' (0.000003)
  18. '' (0.000002)
  19. '3' (0.000002)
  20. '#' (0.000001)

Layer  9 (entropy: 0.511):
   1. ' answer' (0.848505)
   2. 'Answer' (0.130083)
   3. ':' (0.012949)
   4. 'answer' (0.004005)
   5. ' answers' (0.003277)
   6. ' Antwort' (0.000810)
   7. '
' (0.000162)
   8. 'A' (0.000062)
   9. '<strong>' (0.000050)
  10. ' Answer' (0.000029)
  11. '1' (0.000016)
  12. '

' (0.000013)
  13. 's' (0.000009)
  14. '  ' (0.000004)
  15. 'Yes' (0.000004)
  16. '2' (0.000003)
  17. 'a' (0.000002)
  18. '//' (0.000002)
  19. '#' (0.000002)
  20. ' =' (0.000001)

Layer 10 (entropy: 0.226):
   1. ':' (0.952708)
   2. ' answer' (0.039907)
   3. 'Answer' (0.002516)
   4. 'answer' (0.002111)
   5. ' answers' (0.000743)
   6. ' Answer' (0.000488)
   7. '
' (0.000295)
   8. ' =' (0.000232)
   9. 's' (0.000164)
  10. ' Antwort' (0.000118)
  11. 'Solution' (0.000116)
  12. 'a' (0.000103)
  13. 'e' (0.000089)
  14. ' ' (0.000071)
  15. ' :' (0.000067)
  16. 't' (0.000039)
  17. ' Hidup' (0.000026)
  18. 'A' (0.000024)
  19. 'B' (0.000019)
  20. 'i' (0.000015)

Layer 11 (entropy: 0.193):
   1. ':' (0.969282)
   2. 'A' (0.009638)
   3. 'Answer' (0.006272)
   4. 'a' (0.006080)
   5. '
' (0.004791)
   6. '1' (0.001591)
   7. 's' (0.000765)
   8. '2' (0.000420)
   9. 'E' (0.000229)
  10. 'B' (0.000165)
  11. ' ' (0.000116)
  12. 'i' (0.000108)
  13. '

' (0.000103)
  14. 'e' (0.000096)
  15. ' answer' (0.000076)
  16. '3' (0.000040)
  17. ' =' (0.000032)
  18. '  ' (0.000030)
  19. '' (0.000016)
  20. 'I' (0.000015)

Layer 12 (entropy: 0.671):
   1. ':' (0.825909)
   2. '
' (0.115465)
   3. 'a' (0.032027)
   4. 'A' (0.008204)
   5. '

' (0.005706)
   6. 's' (0.004143)
   7. '2' (0.001737)
   8. ' ' (0.001737)
   9. '1' (0.001438)
  10. 'Answer' (0.000628)
  11. 'e' (0.000509)
  12. '3' (0.000438)
  13. ' =' (0.000429)
  14. 'd' (0.000261)
  15. 'i' (0.000203)
  16. '  ' (0.000194)
  17. 'I' (0.000161)
  18. 'B' (0.000145)
  19. '//' (0.000134)
  20. '' (0.000105)

Layer 13 (entropy: 0.749):
   1. 'a' (0.839409)
   2. ':' (0.050127)
   3. 'A' (0.038778)
   4. '
' (0.028047)
   5. '

' (0.009110)
   6. '2' (0.008960)
   7. 's' (0.008433)
   8. '1' (0.007092)
   9. 'i' (0.005606)
  10. ' ' (0.001394)
  11. '3' (0.000512)
  12. 'The' (0.000423)
  13. 'B' (0.000276)
  14. 'I' (0.000241)
  15. 't' (0.000195)
  16. 'd' (0.000143)
  17. 'in' (0.000137)
  18. 'e' (0.000133)
  19. 'C' (0.000115)
  20. ' =' (0.000089)

Layer 14 (entropy: 1.552):
   1. 'a' (0.537812)
   2. 's' (0.188112)
   3. ':' (0.101984)
   4. 'A' (0.070658)
   5. '
' (0.025466)
   6. '

' (0.019010)
   7. '1' (0.012547)
   8. 'i' (0.011972)
   9. '2' (0.008871)
  10. 't' (0.003147)
  11. ' =' (0.002734)
  12. 'The' (0.002517)
  13. ' ' (0.002386)
  14. 'B' (0.001295)
  15. ' a' (0.000948)
  16. 'd' (0.000888)
  17. 'M' (0.000854)
  18. '3' (0.000786)
  19. 'an' (0.000681)
  20. 'x' (0.000662)

Layer 15 (entropy: 1.164):
   1. 'a' (0.671942)
   2. 'A' (0.202512)
   3. ':' (0.031416)
   4. 's' (0.027657)
   5. 'The' (0.017076)
   6. '1' (0.009555)
   7. 'M' (0.005442)
   8. ' ' (0.004964)
   9. ' a' (0.004765)
  10. '
' (0.003689)
  11. '2' (0.003116)
  12. '3' (0.002208)
  13. ' =' (0.001479)
  14. 't' (0.001358)
  15. 'i' (0.001356)
  16. 'B' (0.001258)
  17. 'C' (0.001253)
  18. 'x' (0.001143)
  19. 'L' (0.000883)
  20. 'd' (0.000636)

Layer 16 (entropy: 1.527):
   1. 's' (0.461281)
   2. 'a' (0.227539)
   3. 'A' (0.181394)
   4. ' ' (0.057287)
   5. '1' (0.019623)
   6. ':' (0.017973)
   7. ' a' (0.013297)
   8. ' the' (0.002815)
   9. 'The' (0.002127)
  10. '2' (0.002047)
  11. 'y' (0.001508)
  12. 'i' (0.001334)
  13. 'C' (0.001236)
  14. '3' (0.001201)
  15. 'M' (0.001067)
  16. '
' (0.000999)
  17. 't' (0.000869)
  18. 'B' (0.000676)
  19. ' to' (0.000501)
  20. ''' (0.000470)

Layer 17 (entropy: 1.665):
   1. ' ' (0.323613)
   2. 'a' (0.278349)
   3. 's' (0.248537)
   4. ' a' (0.055579)
   5. ' the' (0.016812)
   6. '1' (0.016808)
   7. ':' (0.016493)
   8. 'The' (0.014841)
   9. 'A' (0.008574)
  10. '’' (0.005969)
  11. ''' (0.005241)
  12. '2' (0.002849)
  13. 'do' (0.001015)
  14. ' to' (0.000842)
  15. '3' (0.000523)
  16. ' =' (0.000477)
  17. 'i' (0.000390)
  18. 'M' (0.000226)
  19. ' The' (0.000225)
  20. 'is' (0.000208)

Layer 18 (entropy: 1.700):
   1. ' ' (0.322579)
   2. ' the' (0.259907)
   3. ' a' (0.185422)
   4. 's' (0.129144)
   5. 'The' (0.052536)
   6. 'a' (0.015399)
   7. ':' (0.014077)
   8. ' to' (0.008828)
   9. ' =' (0.002780)
  10. ''' (0.002204)
  11. '’' (0.001685)
  12. 'A' (0.000915)
  13. ' The' (0.000495)
  14. ' is' (0.000461)
  15. '1' (0.000459)
  16. ' of' (0.000338)
  17. ' answer' (0.000264)
  18. 'i' (0.000256)
  19. '2' (0.000174)
  20. 'do' (0.000158)

Layer 19 (entropy: 0.732):
   1. ' the' (0.784666)
   2. ' a' (0.127859)
   3. ' ' (0.073944)
   4. 'The' (0.003941)
   5. 's' (0.003592)
   6. 'a' (0.002305)
   7. ':' (0.001141)
   8. ' to' (0.000673)
   9. ' is' (0.000310)
  10. '’' (0.000291)
  11. 'i' (0.000230)
  12. ' Gonçalves' (0.000138)
  13. ''' (0.000117)
  14. ' =' (0.000110)
  15. ' of' (0.000101)
  16. ' not' (0.000077)
  17. ' answer' (0.000049)
  18. ' The' (0.000041)
  19. 'A' (0.000037)
  20. 'as' (0.000032)

Layer 20 (entropy: 0.277):
   1. ' the' (0.933830)
   2. ' ' (0.058075)
   3. ' a' (0.005434)
   4. 'The' (0.001934)
   5. 's' (0.000161)
   6. ' The' (0.000112)
   7. ' to' (0.000084)
   8. ':' (0.000067)
   9. ''' (0.000048)
  10. ' is' (0.000039)
  11. 'a' (0.000029)
  12. ' of' (0.000023)
  13. ' It' (0.000023)
  14. ' "' (0.000017)
  15. ' Gonçalves' (0.000014)
  16. '’' (0.000011)
  17. 'i' (0.000009)
  18. '1' (0.000009)
  19. ' “' (0.000009)
  20. ' you' (0.000008)

Layer 21 (entropy: 0.726):
   1. ' the' (0.769608)
   2. ' ' (0.154265)
   3. 'The' (0.064830)
   4. ' a' (0.009818)
   5. ' The' (0.000581)
   6. ':' (0.000190)
   7. ' is' (0.000163)
   8. 's' (0.000066)
   9. 'a' (0.000047)
  10. ' to' (0.000040)
  11. ' Gonçalves' (0.000035)
  12. 'No' (0.000023)
  13. ',' (0.000023)
  14. 'It' (0.000017)
  15. ' you' (0.000015)
  16. '-' (0.000014)
  17. ''' (0.000014)
  18. 'One' (0.000013)
  19. ' of' (0.000012)
  20. 'A' (0.000012)

Layer 22 (entropy: 0.662):
   1. ' the' (0.811818)
   2. ' ' (0.102730)
   3. 'The' (0.065492)
   4. ' a' (0.019715)
   5. ' The' (0.000109)
   6. ':' (0.000044)
   7. ' is' (0.000018)
   8. ',' (0.000010)
   9. '-' (0.000010)
  10. 'a' (0.000009)
  11. ' “' (0.000007)
  12. ' to' (0.000005)
  13. ' in' (0.000005)
  14. 's' (0.000003)
  15. '.' (0.000003)
  16. ' "' (0.000002)
  17. '’' (0.000002)
  18. ' of' (0.000001)
  19. '1' (0.000001)
  20. ' It' (0.000001)

Layer 23 (entropy: 0.703):
   1. ' the' (0.788091)
   2. 'The' (0.126539)
   3. ' ' (0.071903)
   4. ' a' (0.011552)
   5. ' The' (0.001783)
   6. ' is' (0.000032)
   7. ',' (0.000014)
   8. '-' (0.000012)
   9. ' to' (0.000011)
  10. '.' (0.000009)
  11. ' Gonçalves' (0.000007)
  12. ' “' (0.000005)
  13. ' in' (0.000005)
  14. '“' (0.000003)
  15. ' It' (0.000002)
  16. 's' (0.000002)
  17. '1' (0.000002)
  18. 'It' (0.000002)
  19. 'a' (0.000002)
  20. ' ‘' (0.000002)

Layer 24 (entropy: 1.066):
   1. 'The' (0.450361)
   2. ' the' (0.427779)
   3. ' ' (0.087149)
   4. ' a' (0.030291)
   5. ' The' (0.004184)
   6. ',' (0.000090)
   7. ' is' (0.000064)
   8. '.' (0.000019)
   9. 'It' (0.000012)
  10. 'the' (0.000010)
  11. ' in' (0.000007)
  12. ' "' (0.000005)
  13. '-' (0.000004)
  14. 'You' (0.000003)
  15. 's' (0.000003)
  16. ' It' (0.000003)
  17. 'M' (0.000002)
  18. ':' (0.000002)
  19. ' “' (0.000001)
  20. '1' (0.000001)

Layer 25 (entropy: 0.801):
   1. 'The' (0.746053)
   2. ' the' (0.178826)
   3. ' ' (0.033034)
   4. ' The' (0.026608)
   5. ' a' (0.015410)
   6. ',' (0.000018)
   7. 'the' (0.000014)
   8. 'It' (0.000008)
   9. 's' (0.000006)
  10. 'a' (0.000003)
  11. '.' (0.000003)
  12. ' in' (0.000003)
  13. ' it' (0.000003)
  14. ' is' (0.000002)
  15. ' It' (0.000001)
  16. 'What' (0.000001)
  17. 'You' (0.000001)
  18. '-' (0.000001)
  19. 'A' (0.000000)
  20. '1' (0.000000)

Layer 26 (entropy: 0.341):
   1. 'The' (0.922509)
   2. ' the' (0.051448)
   3. ' The' (0.016445)
   4. ' ' (0.009185)
   5. ' a' (0.000354)
   6. 'It' (0.000021)
   7. 'the' (0.000005)
   8. ' It' (0.000004)
   9. '.' (0.000004)
  10. 'What' (0.000003)
  11. ' it' (0.000003)
  12. ',' (0.000002)
  13. 'A' (0.000002)
  14. 'You' (0.000002)
  15. ' is' (0.000001)
  16. '1' (0.000001)
  17. ' "' (0.000001)
  18. 'M' (0.000001)
  19. 's' (0.000001)
  20. '-' (0.000001)

Layer 27 (entropy: 0.718):
   1. 'The' (0.766405)
   2. ' The' (0.172160)
   3. ' the' (0.041291)
   4. ' ' (0.019895)
   5. ' a' (0.000233)
   6. ' it' (0.000003)
   7. 'It' (0.000003)
   8. ' It' (0.000003)
   9. ' is' (0.000001)
  10. 'What' (0.000001)
  11. ',' (0.000001)
  12. 'the' (0.000001)
  13. ' "' (0.000000)
  14. ' in' (0.000000)
  15. 'Great' (0.000000)
  16. 'You' (0.000000)
  17. ' I' (0.000000)
  18. 'A' (0.000000)
  19. 'Don' (0.000000)
  20. 'We' (0.000000)

Layer 28 (entropy: 0.536):
   1. 'The' (0.862892)
   2. ' The' (0.071619)
   3. ' the' (0.044940)
   4. ' ' (0.020471)
   5. ' a' (0.000070)
   6. 'It' (0.000002)
   7. ' "' (0.000001)
   8. 'We' (0.000001)
   9. 'You' (0.000001)
  10. ' it' (0.000001)
  11. 'A' (0.000001)
  12. ' in' (0.000000)
  13. ' It' (0.000000)
  14. ' “' (0.000000)
  15. 's' (0.000000)
  16. 'the' (0.000000)
  17. 'What' (0.000000)
  18. ' is' (0.000000)
  19. '

' (0.000000)
  20. ',' (0.000000)

Layer 29 (entropy: 0.938):
   1. ' The' (0.669702)
   2. 'The' (0.207037)
   3. ' ' (0.068826)
   4. ' the' (0.054377)
   5. ' a' (0.000053)
   6. ' in' (0.000001)
   7. 'We' (0.000001)
   8. 'the' (0.000001)
   9. ' it' (0.000000)
  10. ' It' (0.000000)
  11. 'What' (0.000000)
  12. ' I' (0.000000)
  13. 'It' (0.000000)
  14. ',' (0.000000)
  15. ' (' (0.000000)
  16. ' city' (0.000000)
  17. ' is' (0.000000)
  18. ' "' (0.000000)
  19. ' “' (0.000000)
  20. ' City' (0.000000)

Layer 30 (entropy: 0.885):
   1. ' The' (0.617360)
   2. 'The' (0.311912)
   3. ' the' (0.056526)
   4. ' ' (0.014077)
   5. ' a' (0.000056)
   6. ' Hauptstadt' (0.000026)
   7. ' capital' (0.000012)
   8. ' in' (0.000008)
   9. ' London' (0.000004)
  10. 'We' (0.000003)
  11. ' I' (0.000002)
  12. ' city' (0.000002)
  13. 'It' (0.000001)
  14. ' it' (0.000001)
  15. ' Capital' (0.000001)
  16. ' It' (0.000001)
  17. ' Berlin' (0.000001)
  18. 'the' (0.000001)
  19. ' (' (0.000001)
  20. ' Washington' (0.000001)

Layer 31 (entropy: 0.545):
   1. ' The' (0.860344)
   2. 'The' (0.076933)
   3. ' ' (0.035144)
   4. ' the' (0.027429)
   5. ' a' (0.000063)
   6. ' Berlin' (0.000037)
   7. ' I' (0.000009)
   8. ' It' (0.000008)
   9. ' Germany' (0.000004)
  10. 'We' (0.000004)
  11. ' We' (0.000003)
  12. ' (' (0.000003)
  13. ' in' (0.000003)
  14. ' capital' (0.000003)
  15. ' it' (0.000002)
  16. ',' (0.000001)
  17. ' Washington' (0.000001)
  18. 'Berlin' (0.000001)
  19. 'It' (0.000001)
  20. ' London' (0.000001)

Layer 32 (entropy: 0.788):
   1. ' The' (0.755158)
   2. ' ' (0.169405)
   3. 'The' (0.047609)
   4. ' the' (0.016204)
   5. ' Berlin' (0.006656)
   6. ' capital' (0.004341)
   7. ' Capital' (0.000195)
   8. ' I' (0.000117)
   9. 'Berlin' (0.000072)
  10. ' a' (0.000060)
  11. ' Hauptstadt' (0.000035)
  12. 'Capital' (0.000032)
  13. ' (' (0.000020)
  14. '  ' (0.000010)
  15. ' It' (0.000009)
  16. '1' (0.000008)
  17. ' Washington' (0.000008)
  18. 'We' (0.000007)
  19. ',' (0.000007)
  20. ' in' (0.000006)

Layer 33 (entropy: 0.797):
   1. ' Berlin' (0.687230)
   2. ' The' (0.271638)
   3. ' ' (0.018773)
   4. 'The' (0.012311)
   5. 'Berlin' (0.005730)
   6. ' the' (0.003429)
   7. ' Germany' (0.000328)
   8. ' capital' (0.000322)
   9. ' We' (0.000062)
  10. ' London' (0.000047)
  11. ' It' (0.000023)
  12. ' I' (0.000018)
  13. 'We' (0.000018)
  14. ' Washington' (0.000017)
  15. ' a' (0.000011)
  16. ' Hauptstadt' (0.000008)
  17. ' Capital' (0.000007)
  18. 'It' (0.000005)
  19. ' Paris' (0.000003)
  20. 'Capital' (0.000002)

Layer 34 (entropy: 0.163):
   1. ' Berlin' (0.970543)
   2. 'Berlin' (0.014778)
   3. ' The' (0.011873)
   4. ' ' (0.001549)
   5. 'The' (0.001088)
   6. ' Washington' (0.000054)
   7. ' the' (0.000034)
   8. ' Hauptstadt' (0.000026)
   9. ' Germany' (0.000021)
  10. ' London' (0.000013)
  11. ' capital' (0.000012)
  12. 'Washington' (0.000002)
  13. 'We' (0.000001)
  14. ' I' (0.000001)
  15. 'Capital' (0.000001)
  16. '1' (0.000001)
  17. ' Paris' (0.000001)
  18. ' a' (0.000000)
  19. '  ' (0.000000)
  20. 'This' (0.000000)

Layer 35 (entropy: 0.055):
   1. ' Berlin' (0.991194)
   2. 'Berlin' (0.007465)
   3. ' The' (0.001146)
   4. ' ' (0.000132)
   5. 'The' (0.000057)
   6. ' the' (0.000001)
   7. ' Hauptstadt' (0.000001)
   8. ' London' (0.000001)
   9. ' capital' (0.000001)
  10. ' Germany' (0.000000)
  11. ' Washington' (0.000000)
  12. ' Berlín' (0.000000)
  13. '1' (0.000000)
  14. 'We' (0.000000)
  15. ' I' (0.000000)
  16. 'It' (0.000000)
  17. '

' (0.000000)
  18. 'London' (0.000000)
  19. 'This' (0.000000)
  20. '  ' (0.000000)

Layer 36 (entropy: 0.115):
   1. ' Berlin' (0.979761)
   2. ' The' (0.015261)
   3. 'Berlin' (0.003297)
   4. ' ' (0.000974)
   5. 'The' (0.000675)
   6. ' Hauptstadt' (0.000012)
   7. ' the' (0.000008)
   8. '

' (0.000003)
   9. ' capital' (0.000002)
  10. ' London' (0.000001)
  11. ' Berlín' (0.000001)
  12. ' I' (0.000001)
  13. '1' (0.000000)
  14. ' Washington' (0.000000)
  15. ' (' (0.000000)
  16. 'We' (0.000000)
  17. '  ' (0.000000)
  18. 'Ber' (0.000000)
  19. 'This' (0.000000)
  20. 'New' (0.000000)

Layer 37 (entropy: 0.006):
   1. ' Berlin' (0.999282)
   2. 'Berlin' (0.000718)
   3. ' The' (0.000000)
   4. ' Berlín' (0.000000)
   5. 'Ber' (0.000000)
   6. ' ' (0.000000)
   7. ' Hauptstadt' (0.000000)
   8. 'The' (0.000000)
   9. ' Germany' (0.000000)
  10. ' capital' (0.000000)
  11. ' Ber' (0.000000)
  12. ' BERLIN' (0.000000)
  13. ' Paris' (0.000000)
  14. ' Washington' (0.000000)
  15. ' London' (0.000000)
  16. 'berlin' (0.000000)
  17. ' the' (0.000000)
  18. 'Capital' (0.000000)
  19. ' Capital' (0.000000)
  20. '

' (0.000000)

Layer 38 (entropy: 0.006):
   1. ' Berlin' (0.999318)
   2. 'Berlin' (0.000680)
   3. ' The' (0.000001)
   4. ' Berlín' (0.000000)
   5. 'Ber' (0.000000)
   6. ' Ber' (0.000000)
   7. 'The' (0.000000)
   8. ' ' (0.000000)
   9. ' It' (0.000000)
  10. ' Hauptstadt' (0.000000)
  11. ' This' (0.000000)
  12. ' I' (0.000000)
  13. '

' (0.000000)
  14. ' the' (0.000000)
  15. ' BERLIN' (0.000000)
  16. ' We' (0.000000)
  17. ' capital' (0.000000)
  18. ' There' (0.000000)
  19. 'This' (0.000000)
  20. 'BER' (0.000000)

Layer 39 (entropy: 0.003):
   1. ' Berlin' (0.999737)
   2. ' The' (0.000176)
   3. 'Berlin' (0.000043)
   4. ' ' (0.000026)
   5. '

' (0.000011)
   6. ' It' (0.000003)
   7. ' This' (0.000001)
   8. '  ' (0.000000)
   9. 'The' (0.000000)
  10. ' Ber' (0.000000)
  11. ' “' (0.000000)
  12. ' I' (0.000000)
  13. 'Ber' (0.000000)
  14. ' A' (0.000000)
  15. ' There' (0.000000)
  16. ' What' (0.000000)
  17. ' (' (0.000000)
  18. ' "' (0.000000)
  19. '
' (0.000000)
  20. ' We' (0.000000)

Layer 40 (entropy: 0.000):
   1. ' Berlin' (0.999995)
   2. 'Berlin' (0.000004)
   3. ' The' (0.000001)
   4. ' Germany' (0.000000)
   5. ' It' (0.000000)
   6. ' BERLIN' (0.000000)
   7. ' ' (0.000000)
   8. ' I' (0.000000)
   9. '

' (0.000000)
  10. ' Bonn' (0.000000)
  11. ' This' (0.000000)
  12. ' What' (0.000000)
  13. ' German' (0.000000)
  14. ' We' (0.000000)
  15. '  ' (0.000000)
  16. ' A' (0.000000)
  17. ' “' (0.000000)
  18. ' You' (0.000000)
  19. ' There' (0.000000)
  20. ' In' (0.000000)

Layer 41 (entropy: 0.000):
   1. ' Berlin' (0.999999)
   2. ' The' (0.000000)
   3. ' It' (0.000000)
   4. ' Bonn' (0.000000)
   5. ' Germany' (0.000000)
   6. ' BERLIN' (0.000000)
   7. '

' (0.000000)
   8. ' I' (0.000000)
   9. ' ' (0.000000)
  10. 'Berlin' (0.000000)
  11. ' What' (0.000000)
  12. ' This' (0.000000)
  13. ' “' (0.000000)
  14. ' Hamburg' (0.000000)
  15. ' A' (0.000000)
  16. '  ' (0.000000)
  17. ' Munich' (0.000000)
  18. ' "' (0.000000)
  19. ' There' (0.000000)
  20. ' You' (0.000000)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 1.388):
   1. ' Berlin' (0.790732)
   2. ' The' (0.020494)
   3. ' It' (0.016067)
   4. ' Bonn' (0.013444)
   5. ' Germany' (0.011033)
   6. ' BERLIN' (0.009927)
   7. '

' (0.009078)
   8. ' I' (0.007988)
   9. ' ' (0.007667)
  10. 'Berlin' (0.006253)
  11. ' What' (0.006137)
  12. ' This' (0.004951)
  13. ' “' (0.004634)
  14. ' Hamburg' (0.003492)
  15. ' A' (0.003307)
  16. '  ' (0.003273)
  17. ' Munich' (0.003112)
  18. ' "' (0.002852)
  19. ' There' (0.002710)
  20. ' You' (0.002629)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 3.692)
   1. ' a' (0.212154)
   2. ' one' (0.088893)
   3. ' the' (0.075590)
   4. ' home' (0.058342)
   5. ' known' (0.044896)
   6. ' not' (0.040992)
   7. ' an' (0.031461)
   8. ' full' (0.024788)
   9. ' famous' (0.022445)
  10. ' set' (0.018748)

Prompt: 'Berlin is the capital of'
  (entropy: 0.667)
   1. ' Germany' (0.876573)
   2. ' the' (0.069934)
   3. ' modern' (0.007701)
   4. ' a' (0.005295)
   5. ' ' (0.003393)
   6. ' Europe' (0.003080)
   7. ' German' (0.002839)
   8. ' both' (0.002428)
   9. ' Berlin' (0.001870)
  10. ' germany' (0.001524)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 3.686)
   1. '

' (0.158683)
   2. ' If' (0.111438)
   3. ' Berlin' (0.104638)
   4. ' Of' (0.045569)
   5. ' The' (0.045382)
   6. ' You' (0.033567)
   7. '
' (0.030118)
   8. ' That' (0.026873)
   9. ' Yes' (0.022590)
  10. ' And' (0.020589)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000)
   1. ' Berlin' (1.000000)
   2. ' The' (0.000000)
   3. ' It' (0.000000)
   4. ' Bonn' (0.000000)
   5. ' Germany' (0.000000)
   6. ' BERLIN' (0.000000)
   7. '

' (0.000000)
   8. ' I' (0.000000)
   9. ' ' (0.000000)
  10. 'Berlin' (0.000000)
  11. ' What' (0.000000)
  12. ' This' (0.000000)
  13. ' “' (0.000000)
  14. ' Hamburg' (0.000000)
  15. ' A' (0.000000)

Temperature 2.0:
  (entropy: 7.632)
   1. ' Berlin' (0.085547)
   2. ' The' (0.013772)
   3. ' It' (0.012194)
   4. ' Bonn' (0.011155)
   5. ' Germany' (0.010105)
   6. ' BERLIN' (0.009585)
   7. '

' (0.009166)
   8. ' I' (0.008598)
   9. ' ' (0.008424)
  10. 'Berlin' (0.007607)
  11. ' What' (0.007537)
  12. ' This' (0.006769)
  13. ' “' (0.006549)
  14. ' Hamburg' (0.005685)
  15. ' A' (0.005533)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 42
Model dimension: 3584
Number of heads: 16
Vocab size: 256000
Context length: 8192
=== END OF MODEL STATS ========

