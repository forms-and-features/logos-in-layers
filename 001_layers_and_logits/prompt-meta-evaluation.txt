ROLE

You are an interpretability researcher from a top AI research lab (think OpenAI, Anthropic, Google) synthesising results from several open‑weight LLMs probed with the same script. These results are part of a hobby project that probes open‑weight LLMs to inform the philosophical debate between nominalism and realism.

INPUTS

Per‑model evaluation reports (Markdown):
EVAL

Raw console dumps (output files `output‑{model}.txt`) available in the repo if you need to verify numbers:
OUTPUT

Probe implementation (for method context): SCRIPT

TASK
Produce CROSS (GitHub‑flavoured Markdown) that compares the models and surfaces insights relevant to the nominalism ↔ realism debate. Use the structure below **exactly** so later tools can parse the report.

Required section structure

1. Executive summary
3‑5 bullet points: headline similarities, standout divergences, and the single most interesting open question.

2. Comparison table

Produce a markdown table with the following columns
- Model
- Params
- Norm type
- Collapse layer (entropy < 1 bit)
- Final entropy (bits)
- First token after collapse
- Any anomaly

Fill the numeric cells from evaluation reports, one row per model; if missing, recover from output files (console dumps).
Do not skip any models or columns.

3. Shared patterns

Bullet list behaviours observed in ≥ 3 models (e.g. entropy cliff then mild rebound, directional relation asymmetry). Quote one line per claim:
`> "Layer 24 … entropy: 0.78" — Llama‑3 [L912]`.

4. Model‑specific quirks & red flags

Sub‑heading per model with max 3 bullets. Identify anything unusual: e.g. Gemma’s early colon‑bias, Qwen’s delayed collapse. Support with line quotes or the evaluation file text/location.

5. Preliminary implications for Realism ↔ Nominalism (no verdicts)
2‑5 bullets phrased as hypotheses or questions, explicitly tying **the cross‑model evidence** to philosophical angles (concept invariance, token‑dependence, etc.).

No speculative leaps; stay conditional.

6. Methodological caveats

E.g. lens calibration differences, prompt artefacts, variance in norm application. Mention if any model’s output shows warnings.

7. Priority next steps

3 concrete comparative probes that **reuse the existing codebase** (e.g. patch the identified collapse block from Mistral into Gemma; run paraphrase battery; apply tuned‑lens across checkpoints).

Each step should reference which unanswered bullet (from §3‑5) it addresses.

8. Appendix: Evidence map

For every numeric cell in the comparison table, list the file and line number(s) where the value was taken.

Format: `Llama‑3 collapse layer → output‑Meta‑Llama‑3‑8B.txt [L880‑L883]`.


STYLE / RIGOUR RULES

**Every factual statement** must be supported by either
- an inline quote from an output file (≤ 2 lines) with file ref, or
- an explicit pointer like “EVAL‑Qwen §2.3”.

If evidence is missing, say “data not present”.

External literature citations optional; if used, place DOI or arXiv ID in parentheses.

Use concise sentences and bullet lists; avoid block quotes longer than two lines.

**Do not** produce final philosophical conclusions; keep language exploratory (“suggests”, “may indicate”).

If you open any output file, cache only the small snippet needed; avoid pasting long logs.
