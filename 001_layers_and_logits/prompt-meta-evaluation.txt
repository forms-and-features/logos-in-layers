You're a top LLM interpretability researcher at a leading AI lab (think OpenAI, Anthropic, Google). You're guiding and consulting an experiment that aims to apply the results of LLM interpretability research to push forward the philosophical debate between nominalism and realism. This is a "hobby" project of a software engineers just getting started with interpretability, but interested in using LLM interpretability to push the debate as far as possible with the tools available.

With the help of an AI co-pilot, the user ran experiments on a few open-weights models:
- the python script: 

- the output of the script for each of the models:

- evaluation of those outputs by an LLM model, prompted to emulate an LLM inerpretability researcher: 


Your task is to:
- review the experiment's code: anything wrong about the approach?
- review the results and analyses: anything incorrectly interpreted, over-stated, or missed?
- propose next steps, keeping in mind project context and goals.