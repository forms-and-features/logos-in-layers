
============================================================
EVALUATING MODEL: Qwen/Qwen3-8B
============================================================
Loading model: Qwen/Qwen3-8B...
Loaded pretrained model Qwen/Qwen3-8B into HookedTransformer

=== PROMPT =========================
Question: What is the capital of Germany? Answer:
=== END OF PROMPT =================


=== INSPECTING ====================
Input tokens: ['Question', ':', ' What', ' is', ' the', ' capital', ' of', ' Germany', '?', ' Answer', ':']

Top predictions for next token after 'Question: What is the capital of Germany? Answer:':
Using NORMALIZED residual stream (LayerNorm applied - more accurate)
------------------------------------------------------------
Layer  0 (entropy: 3.931):
   1. 'いらっ' (0.211549)
   2. ' binaries' (0.166734)
   3. 'おすす' (0.092069)
   4. '家喻户' (0.061461)
   5. 'インターネ' (0.057510)
   6. '(DBG' (0.025784)
   7. ' binary' (0.024330)
   8. 'FindObject' (0.017708)
   9. '烟花爆' (0.016815)
  10. 'お互' (0.016379)
  11. 'アメリ' (0.013441)
  12. '特色社会' (0.011393)
  13. 'InputBorder' (0.009543)
  14. 'ご�' (0.008017)
  15. '/import' (0.006691)
  16. '続きを読' (0.005294)
  17. '電話及' (0.005201)
  18. '.CG' (0.004348)
  19. ' DIR' (0.004336)
  20. '(CONT' (0.004285)

Layer  1 (entropy: 6.931):
   1. ' Buccane' (0.085337)
   2. ' Phonetic' (0.027217)
   3. 'öffentlich' (0.026519)
   4. ' coquine' (0.024927)
   5. ' Seeder' (0.018403)
   6. '凝聚力' (0.012637)
   7. ' Cougar' (0.012383)
   8. '三大职业' (0.012149)
   9. ' datings' (0.012139)
  10. '下半场' (0.010405)
  11. '批复' (0.009972)
  12. '直接影响' (0.009294)
  13. ' catcher' (0.008989)
  14. ' Sesso' (0.008230)
  15. 'rijk' (0.008071)
  16. ' ListViewItem' (0.007964)
  17. '嘉年华' (0.007207)
  18. '负面影响' (0.006598)
  19. ' squirt' (0.006041)
  20. '太快' (0.005877)

Layer  2 (entropy: 6.863):
   1. ' Lauderdale' (0.074767)
   2. ' Buccane' (0.044316)
   3. ' Phonetic' (0.027031)
   4. '批复' (0.023238)
   5. ' ValidationResult' (0.021096)
   6. 'nehmer' (0.014314)
   7. 'mittel' (0.012591)
   8. 'arrison' (0.011633)
   9. 'öffentlich' (0.010670)
  10. '三大职业' (0.010575)
  11. 'başı' (0.009609)
  12. 'ismet' (0.009269)
  13. '并不意味' (0.008984)
  14. '直接影响' (0.008946)
  15. '第二批' (0.007730)
  16. ' coquine' (0.007396)
  17. '延误' (0.007086)
  18. 'anzeigen' (0.006894)
  19. ' Seeder' (0.006633)
  20. 'ledon' (0.006322)

Layer  3 (entropy: 7.863):
   1. ' Buccane' (0.019212)
   2. 'caf' (0.016763)
   3. ' Lauderdale' (0.013269)
   4. 'arrison' (0.012478)
   5. '直接影响' (0.012203)
   6. 'führ' (0.011746)
   7. '批复' (0.010498)
   8. '凝聚力' (0.009417)
   9. '第二批' (0.008837)
  10. 'öffentlich' (0.007766)
  11. '昴' (0.006767)
  12. 'nehmer' (0.005918)
  13. '付け' (0.005759)
  14. 'aggregate' (0.005479)
  15. '延误' (0.005316)
  16. 'vlc' (0.005292)
  17. 'mittel' (0.005132)
  18. ' ListViewItem' (0.004916)
  19. 'licken' (0.004812)
  20. 'portion' (0.004761)

Layer  4 (entropy: 8.946):
   1. '直接影响' (0.022559)
   2. 'portion' (0.010447)
   3. 'arrison' (0.009873)
   4. 'razier' (0.006924)
   5. 'führ' (0.006526)
   6. 'ход' (0.006418)
   7. '单职业' (0.005216)
   8. 'caf' (0.005095)
   9. '(typeof' (0.004964)
  10. '在此之前' (0.004918)
  11. '我省' (0.004135)
  12. 'nehmer' (0.004107)
  13. ' тоже' (0.004047)
  14. '并不代表' (0.003858)
  15. 'directories' (0.003538)
  16. '付け' (0.003506)
  17. 'ым' (0.003392)
  18. '.SpringApplication' (0.003332)
  19. 'affe' (0.003115)
  20. ' ebenfalls' (0.002647)

Layer  5 (entropy: 9.805):
   1. '我省' (0.009487)
   2. 'abella' (0.006327)
   3. '(typeof' (0.005238)
   4. '不利于' (0.005027)
   5. 'portion' (0.004055)
   6. ' lâu' (0.003528)
   7. '�인' (0.002696)
   8. 'alter' (0.002546)
   9. 'ständ' (0.002543)
  10. 'führ' (0.002507)
  11. '直接影响' (0.002330)
  12. 'ingle' (0.002212)
  13. 'ivers' (0.002187)
  14. '略' (0.002020)
  15. 'steller' (0.001937)
  16. 'ив' (0.001837)
  17. ' 부분' (0.001784)
  18. ' информации' (0.001728)
  19. 'oster' (0.001708)
  20. 'bbie' (0.001587)

Layer  6 (entropy: 9.309):
   1. 'portion' (0.011783)
   2. 'steller' (0.008585)
   3. '<Entry' (0.006405)
   4. 'führ' (0.006186)
   5. ' Located' (0.006136)
   6. ' lẽ' (0.004740)
   7. '�인' (0.004483)
   8. '样的' (0.003856)
   9. 'alter' (0.003235)
  10. ' Seconds' (0.003150)
  11. 'abella' (0.003131)
  12. 'ив' (0.002863)
  13. ' ArrayAdapter' (0.002760)
  14. ' Rap' (0.002759)
  15. 'ivers' (0.002693)
  16. 'asio' (0.002604)
  17. 'ать' (0.002508)
  18. 'owe' (0.002481)
  19. ' Scientists' (0.002426)
  20. 'Locator' (0.002275)

Layer  7 (entropy: 8.367):
   1. 'steller' (0.050405)
   2. ' عنه' (0.021419)
   3. '付け' (0.019737)
   4. ' accumulator' (0.009679)
   5. '.answer' (0.007187)
   6. ' Wel' (0.007033)
   7. '答' (0.006792)
   8. 'mittel' (0.005833)
   9. ' Ergebn' (0.004963)
  10. 'portion' (0.004100)
  11. 'не' (0.004087)
  12. 'нес' (0.003939)
  13. '在床上' (0.003844)
  14. ' Instead' (0.003412)
  15. 'owe' (0.003396)
  16. ' Locations' (0.003347)
  17. ' San' (0.003072)
  18. '꧁' (0.002993)
  19. 'ivers' (0.002968)
  20. 'abella' (0.002852)

Layer  8 (entropy: 7.832):
   1. ' Mus' (0.060854)
   2. 'ihar' (0.014855)
   3. ' Enumerator' (0.013769)
   4. '答' (0.012626)
   5. ' Eleven' (0.010830)
   6. 'ывать' (0.008757)
   7. ' Iter' (0.008663)
   8. ' Answer' (0.008128)
   9. 'immer' (0.007998)
  10. ' unpopular' (0.006839)
  11. ' Parliamentary' (0.006656)
  12. 'ما' (0.006603)
  13. '付け' (0.006065)
  14. ' nd' (0.005533)
  15. ' Visit' (0.005230)
  16. ' Element' (0.005072)
  17. 'ndern' (0.004642)
  18. ' Sessions' (0.004520)
  19. ' Moral' (0.004293)
  20. ' Tiger' (0.004245)

Layer  9 (entropy: 7.994):
   1. '在游戏中' (0.048284)
   2. 'owe' (0.028272)
   3. '�' (0.025823)
   4. ' Mus' (0.007800)
   5. '的职业' (0.007620)
   6. 'steller' (0.007117)
   7. ' unpopular' (0.006632)
   8. 'abella' (0.006487)
   9. ' Parliamentary' (0.006309)
  10. 'beit' (0.005445)
  11. 'hei' (0.005412)
  12. ' Binary' (0.004612)
  13. '样的' (0.004331)
  14. '瑟' (0.004277)
  15. ' Illegal' (0.003704)
  16. ' *__' (0.003689)
  17. '_PT' (0.003517)
  18. ' Southern' (0.003425)
  19. ' unre' (0.003322)
  20. ' Frames' (0.003244)

Layer 10 (entropy: 8.420):
   1. '在游戏中' (0.030151)
   2. ' Gem' (0.027779)
   3. ' Incorrect' (0.020742)
   4. ' Illegal' (0.015364)
   5. 'áp' (0.010968)
   6. ' Entre' (0.009089)
   7. 'abella' (0.008636)
   8. ' vậy' (0.006909)
   9. ' Boy' (0.006834)
  10. ' Adult' (0.004784)
  11. ' Desk' (0.004666)
  12. ' Participation' (0.004645)
  13. 'owe' (0.004644)
  14. 'gebn' (0.004516)
  15. ' Binary' (0.004267)
  16. ' Mus' (0.004065)
  17. 'anker' (0.004054)
  18. '史上最' (0.003768)
  19. ' Parliamentary' (0.003744)
  20. ' Occupation' (0.003634)

Layer 11 (entropy: 8.426):
   1. ' Answer' (0.030883)
   2. ' Incorrect' (0.028627)
   3. ' Electoral' (0.016887)
   4. ' Desk' (0.011978)
   5. ' Illegal' (0.010644)
   6. ' Boy' (0.009878)
   7. ' Develop' (0.009306)
   8. 'áp' (0.008943)
   9. '不服' (0.008826)
  10. ' Entities' (0.006980)
  11. ' Gem' (0.005611)
  12. ' Cities' (0.005541)
  13. '答え' (0.005354)
  14. ' Prompt' (0.004477)
  15. 'あれ' (0.004120)
  16. ' VE' (0.003856)
  17. '在游戏中' (0.003712)
  18. ' Parliamentary' (0.003708)
  19. ' Begin' (0.003689)
  20. 'abella' (0.003485)

Layer 12 (entropy: 8.099):
   1. ' Answer' (0.069230)
   2. 'abella' (0.023264)
   3. ' Incorrect' (0.017712)
   4. '在游戏中' (0.011104)
   5. '在全球' (0.010015)
   6. ' Binary' (0.008126)
   7. ' VE' (0.007957)
   8. '答え' (0.007139)
   9. ' Frag' (0.006229)
  10. ' thirteen' (0.006035)
  11. ' Prompt' (0.005666)
  12. ' Knowledge' (0.005646)
  13. '_FM' (0.005403)
  14. 'oday' (0.005043)
  15. ' tsl' (0.004540)
  16. ' Intelligent' (0.004096)
  17. ' Boy' (0.004044)
  18. ' Entities' (0.003748)
  19. ' Minecraft' (0.003431)
  20. '_primitive' (0.003431)

Layer 13 (entropy: 7.592):
   1. ' Binary' (0.116481)
   2. ' Answer' (0.040844)
   3. ' Incorrect' (0.012109)
   4. '在游戏中' (0.010906)
   5. ' Prompt' (0.008571)
   6. ' The' (0.008063)
   7. ' Intelligent' (0.007887)
   8. '在全球' (0.007089)
   9. ' Illegal' (0.006808)
  10. ' Boy' (0.006579)
  11. '_FM' (0.006412)
  12. ' Organ' (0.006339)
  13. ' none' (0.006265)
  14. ' Minecraft' (0.006110)
  15. ' Donald' (0.005504)
  16. 'abella' (0.004795)
  17. ' binary' (0.004497)
  18. ' English' (0.003794)
  19. ' Digital' (0.003603)
  20. ' thirteen' (0.003429)

Layer 14 (entropy: 8.043):
   1. ' Answer' (0.034788)
   2. ' Incorrect' (0.023533)
   3. ' Binary' (0.015589)
   4. ' Knowledge' (0.015434)
   5. '在全球' (0.013144)
   6. ' Correct' (0.012295)
   7. '正确的' (0.012202)
   8. '_FM' (0.009734)
   9. ' FM' (0.007223)
  10. ' Statements' (0.007121)
  11. ' Prompt' (0.005497)
  12. ' prefixed' (0.005491)
  13. ' hierarchical' (0.005447)
  14. ' $__' (0.005408)
  15. 'abella' (0.005160)
  16. '不服' (0.004981)
  17. ' The' (0.004848)
  18. ' First' (0.004647)
  19. ' Minecraft' (0.004200)
  20. '太阳城' (0.004149)

Layer 15 (entropy: 8.118):
   1. ' Answer' (0.047318)
   2. ' Classes' (0.032057)
   3. '正确的' (0.018723)
   4. ' Incorrect' (0.012193)
   5. ' Knowledge' (0.010738)
   6. ' Context' (0.009998)
   7. '在全球' (0.007169)
   8. ' política' (0.006334)
   9. ' Earth' (0.005818)
  10. ' The' (0.005611)
  11. ' First' (0.005324)
  12. '在游戏中' (0.005302)
  13. ' ____' (0.005038)
  14. ' Define' (0.004701)
  15. 'ilik' (0.004440)
  16. ' Minecraft' (0.004432)
  17. 'GraphNode' (0.003759)
  18. ' Statements' (0.003751)
  19. ' answer' (0.003732)
  20. ' __' (0.003472)

Layer 16 (entropy: 8.754):
   1. ' Answer' (0.043636)
   2. ' Incorrect' (0.012553)
   3. '编码' (0.008499)
   4. ' __' (0.006194)
   5. ' Classes' (0.005836)
   6. '_encoding' (0.005433)
   7. ' Architect' (0.005328)
   8. '_chunks' (0.004570)
   9. '在全球' (0.003781)
  10. 'ilik' (0.003571)
  11. '_topology' (0.003517)
  12. '_assert' (0.003491)
  13. ' política' (0.003451)
  14. ' answer' (0.003300)
  15. '第一条' (0.003216)
  16. ' First' (0.003183)
  17. 'ichel' (0.003140)
  18. ' trivia' (0.003128)
  19. 'aal' (0.002863)
  20. ' Define' (0.002821)

Layer 17 (entropy: 7.515):
   1. ' Answer' (0.171534)
   2. '回答' (0.026315)
   3. ' None' (0.017732)
   4. '_ANS' (0.014724)
   5. ' Classes' (0.011588)
   6. '在全球' (0.011075)
   7. '.answer' (0.008799)
   8. '编码' (0.006982)
   9. ' answer' (0.004419)
  10. ' 数' (0.004231)
  11. 'GraphNode' (0.003866)
  12. ' Incorrect' (0.003273)
  13. ' none' (0.003193)
  14. '第一条' (0.003026)
  15. ' AssertionError' (0.002839)
  16. ' answered' (0.002730)
  17. '在游戏中' (0.002724)
  18. '_answer' (0.002576)
  19. ' The' (0.002552)
  20. '我知道' (0.002520)

Layer 18 (entropy: 5.930):
   1. ' Answer' (0.324686)
   2. '回答' (0.049062)
   3. '_ANS' (0.014806)
   4. '.answer' (0.012050)
   5. '的回答' (0.009193)
   6. 'คำตอบ' (0.007269)
   7. ' ______' (0.006841)
   8. '我知道' (0.006047)
   9. '_answer' (0.005894)
  10. ' _____' (0.005414)
  11. ' ___' (0.005282)
  12. '嘘' (0.004690)
  13. ' None' (0.004640)
  14. '世界第一' (0.003618)
  15. ' Prompt' (0.003530)
  16. ' Located' (0.003351)
  17. ' ____' (0.003286)
  18. 'ichel' (0.003211)
  19. ' answered' (0.003170)
  20. '的答案' (0.003089)

Layer 19 (entropy: 2.570):
   1. ' Answer' (0.706669)
   2. '_ANS' (0.028930)
   3. '的回答' (0.016019)
   4. '的答案' (0.013249)
   5. ' None' (0.009143)
   6. ' none' (0.008911)
   7. '_answer' (0.005833)
   8. '回答' (0.005237)
   9. ' ______' (0.005000)
  10. ' Yes' (0.004409)
  11. ' ____' (0.004297)
  12. ' answer' (0.004002)
  13. '世界第一' (0.002594)
  14. ' ___' (0.002270)
  15. '.answer' (0.002243)
  16. ' yes' (0.002033)
  17. ' Ans' (0.001939)
  18. 'คำตอบ' (0.001842)
  19. ' unequiv' (0.001819)
  20. ' $__' (0.001605)

Layer 20 (entropy: 1.802):
   1. ' Answer' (0.803659)
   2. '_ANS' (0.025432)
   3. ' ____' (0.007380)
   4. ' none' (0.005602)
   5. '回答' (0.005441)
   6. ' answer' (0.003692)
   7. '我知道' (0.003514)
   8. ' ______' (0.003475)
   9. ' Yes' (0.003414)
  10. ' ___' (0.003129)
  11. '_answer' (0.002948)
  12. ' None' (0.002675)
  13. '的答案' (0.002544)
  14. ' yes' (0.002254)
  15. ' $__' (0.001941)
  16. ' _____' (0.001763)
  17. '这个问题' (0.001590)
  18. 'คำตอบ' (0.001536)
  19. ' __' (0.001478)
  20. '.answer' (0.001431)

Layer 21 (entropy: 2.336):
   1. ' Answer' (0.734852)
   2. ' ____' (0.018264)
   3. '_ANS' (0.017918)
   4. ' ______' (0.009652)
   5. '我知道' (0.007929)
   6. '的答案' (0.006501)
   7. ' Ans' (0.005674)
   8. ' __' (0.005141)
   9. ' ANSW' (0.004648)
  10. '回答' (0.003559)
  11. ' answered' (0.003224)
  12. ' ...

' (0.003175)
  13. ' ___' (0.002930)
  14. '_answer' (0.002627)
  15. ' answer' (0.002588)
  16. '当然是' (0.002587)
  17. 'zell' (0.002521)
  18. '____' (0.002205)
  19. '答' (0.001905)
  20. ' None' (0.001854)

Layer 22 (entropy: 2.460):
   1. ' Answer' (0.555836)
   2. ' ____' (0.115385)
   3. ' ______' (0.053255)
   4. ' The' (0.031127)
   5. ' Ans' (0.026243)
   6. ' __' (0.020287)
   7. '的答案' (0.015138)
   8. ' __________________' (0.012161)
   9. '____' (0.011067)
  10. ' _____' (0.010259)
  11. ' ...
' (0.010185)
  12. ' ...

' (0.005156)
  13. ' answer' (0.004256)
  14. ' answered' (0.004202)
  15. ' ___' (0.003571)
  16. ' ANSW' (0.003540)
  17. ' Please' (0.003474)
  18. '答え' (0.003159)
  19. ' There' (0.002764)
  20. ' Answers' (0.002750)

Layer 23 (entropy: 2.842):
   1. ' ______' (0.347673)
   2. ' ____' (0.154669)
   3. ' The' (0.079706)
   4. ' __' (0.069414)
   5. ' __________________' (0.065208)
   6. ' _____' (0.042478)
   7. '____' (0.041508)
   8. ' ___' (0.019217)
   9. ' Answer' (0.017863)
  10. ' Located' (0.013557)
  11. ' There' (0.011436)
  12. ' ...
' (0.008729)
  13. ' Yes' (0.007642)
  14. ' ...

' (0.005909)
  15. ' None' (0.005316)
  16. '__
' (0.003191)
  17. ' A' (0.002619)
  18. ' Ans' (0.002579)
  19. ' This' (0.002553)
  20. '---------
' (0.002283)

Layer 24 (entropy: 0.978):
   1. ' Germany' (0.797710)
   2. ' ____' (0.055280)
   3. ' ______' (0.034518)
   4. ' __________________' (0.027517)
   5. ' _____' (0.021357)
   6. ' __' (0.020593)
   7. ' ___' (0.010138)
   8. '____' (0.009558)
   9. ' Berlin' (0.006097)
  10. ' German' (0.004132)
  11. ' Deutschland' (0.001130)
  12. ' Europe' (0.000940)
  13. ' The' (0.000852)
  14. ' Munich' (0.000795)
  15. ' Germans' (0.000696)
  16. '__
' (0.000609)
  17. 'Germany' (0.000607)
  18. '____________' (0.000453)
  19. ' _______,' (0.000440)
  20. '_____' (0.000425)

Layer 25 (entropy: 1.980):
   1. ' ____' (0.299554)
   2. ' ______' (0.191108)
   3. '____' (0.145953)
   4. ' __' (0.124050)
   5. ' __________________' (0.092406)
   6. ' _____' (0.049104)
   7. ' Germany' (0.044617)
   8. ' ___' (0.030860)
   9. '________' (0.004059)
  10. '____________' (0.003144)
  11. '__
' (0.002845)
  12. '_____' (0.002468)
  13. '________________' (0.001843)
  14. ' The' (0.001206)
  15. '__[' (0.001163)
  16. ' Berlin' (0.000853)
  17. ' _______,' (0.000730)
  18. '___' (0.000523)
  19. '[__' (0.000522)
  20. '__

' (0.000304)

Layer 26 (entropy: 2.022):
   1. ' Germany' (0.390569)
   2. ' ____' (0.118561)
   3. ' ______' (0.116350)
   4. ' Berlin' (0.107186)
   5. ' __________________' (0.076123)
   6. ' __' (0.069332)
   7. ' ___' (0.034109)
   8. ' _____' (0.031766)
   9. '____' (0.022200)
  10. ' The' (0.009145)
  11. '__
' (0.006246)
  12. ' _______,' (0.002089)
  13. '_____' (0.002051)
  14. ' German' (0.001340)
  15. '__

' (0.001283)
  16. '________' (0.001191)
  17. '____________' (0.001031)
  18. '__[' (0.000757)
  19. '________________' (0.000702)
  20. ' __________________________________' (0.000611)

Layer 27 (entropy: 0.298):
   1. ' Berlin' (0.926707)
   2. ' Germany' (0.067895)
   3. ' Paris' (0.001706)
   4. ' Munich' (0.000447)
   5. ' Frankfurt' (0.000277)
   6. ' Rome' (0.000249)
   7. ' The' (0.000248)
   8. ' __________________' (0.000225)
   9. ' London' (0.000191)
  10. ' Madrid' (0.000171)
  11. ' ______' (0.000162)
  12. 'Berlin' (0.000149)
  13. ' __' (0.000133)
  14. ' cities' (0.000131)
  15. ' ____' (0.000129)
  16. ' _____' (0.000092)
  17. ' Vienna' (0.000090)
  18. ' Москва' (0.000089)
  19. ' Beijing' (0.000079)
  20. ' ___' (0.000062)

Layer 28 (entropy: 0.135):
   1. ' Berlin' (0.971284)
   2. ' Germany' (0.028003)
   3. ' Munich' (0.000328)
   4. ' Frankfurt' (0.000154)
   5. 'Berlin' (0.000130)
   6. 'Germany' (0.000023)
   7. ' Stuttgart' (0.000011)
   8. ' Hamburg' (0.000009)
   9. ' German' (0.000009)
  10. ' Vienna' (0.000008)
  11. ' München' (0.000006)
  12. ' Paris' (0.000005)
  13. '柏林' (0.000005)
  14. ' __________________' (0.000003)
  15. '德国' (0.000002)
  16. ' berlin' (0.000001)
  17. '武汉' (0.000001)
  18. ' Prague' (0.000001)
  19. ' Rome' (0.000001)
  20. ' Germans' (0.000001)

Layer 29 (entropy: 0.604):
   1. ' Berlin' (0.718389)
   2. ' Germany' (0.280483)
   3. ' Frankfurt' (0.000378)
   4. 'Berlin' (0.000280)
   5. 'Germany' (0.000170)
   6. ' Munich' (0.000094)
   7. ' German' (0.000090)
   8. '德国' (0.000029)
   9. ' germany' (0.000022)
  10. '柏林' (0.000016)
  11. ' Stuttgart' (0.000015)
  12. ' Hamburg' (0.000011)
  13. ' Paris' (0.000005)
  14. ' Germans' (0.000004)
  15. 'ドイツ' (0.000004)
  16. ' Deutschland' (0.000003)
  17. ' frankfurt' (0.000001)
  18. ' München' (0.000001)
  19. ' __________________' (0.000001)
  20. ' berlin' (0.000001)

Layer 30 (entropy: 0.007):
   1. ' Berlin' (0.999213)
   2. ' Germany' (0.000408)
   3. 'Berlin' (0.000171)
   4. ' Frankfurt' (0.000153)
   5. ' Munich' (0.000020)
   6. '柏林' (0.000016)
   7. ' Hamburg' (0.000012)
   8. ' Paris' (0.000004)
   9. ' Stuttgart' (0.000001)
  10. 'Germany' (0.000001)
  11. ' berlin' (0.000000)
  12. ' Vienna' (0.000000)
  13. ' München' (0.000000)
  14. ' frankfurt' (0.000000)
  15. ' London' (0.000000)
  16. ' Dresden' (0.000000)
  17. ' Brussels' (0.000000)
  18. '武汉' (0.000000)
  19. ' Cologne' (0.000000)
  20. ' Amsterdam' (0.000000)

Layer 31 (entropy: 0.053):
   1. ' Berlin' (0.991409)
   2. ' Germany' (0.007853)
   3. ' German' (0.000431)
   4. 'Berlin' (0.000142)
   5. '柏林' (0.000100)
   6. '德国' (0.000050)
   7. 'Germany' (0.000005)
   8. ' Germans' (0.000004)
   9. ' Frankfurt' (0.000004)
  10. ' Hamburg' (0.000000)
  11. ' Munich' (0.000000)
  12. ' germany' (0.000000)
  13. 'ドイツ' (0.000000)
  14. 'German' (0.000000)
  15. ' berlin' (0.000000)
  16. ' Stuttgart' (0.000000)
  17. ' Deutschland' (0.000000)
  18. '武汉' (0.000000)
  19. ' Dresden' (0.000000)
  20. ' german' (0.000000)

Layer 32 (entropy: 0.008):
   1. ' Berlin' (0.999185)
   2. ' Germany' (0.000345)
   3. '柏林' (0.000264)
   4. 'Berlin' (0.000174)
   5. ' German' (0.000021)
   6. ' Frankfurt' (0.000007)
   7. ' Munich' (0.000001)
   8. '德国' (0.000000)
   9. 'Germany' (0.000000)
  10. ' berlin' (0.000000)
  11. ' Hamburg' (0.000000)
  12. ' Germans' (0.000000)
  13. ' Stuttgart' (0.000000)
  14. ' germany' (0.000000)
  15. 'ドイツ' (0.000000)
  16. 'German' (0.000000)
  17. ' Dresden' (0.000000)
  18. ' Deutschland' (0.000000)
  19. ' München' (0.000000)
  20. '法兰' (0.000000)

Layer 33 (entropy: 0.008):
   1. ' Berlin' (0.999077)
   2. '柏林' (0.000500)
   3. 'Berlin' (0.000352)
   4. ' Frankfurt' (0.000038)
   5. ' Germany' (0.000012)
   6. ' berlin' (0.000005)
   7. ' Hamburg' (0.000004)
   8. ' Munich' (0.000004)
   9. ' BER' (0.000002)
  10. ' Ber' (0.000001)
  11. ' Bon' (0.000001)
  12. '_ber' (0.000001)
  13. ' Stuttgart' (0.000000)
  14. '法兰' (0.000000)
  15. ' Paris' (0.000000)
  16. ' The' (0.000000)
  17. ' German' (0.000000)
  18. ' Bern' (0.000000)
  19. ' London' (0.000000)
  20. ' Brussels' (0.000000)

Layer 34 (entropy: 0.090):
   1. ' Berlin' (0.989004)
   2. ' Germany' (0.003136)
   3. 'Berlin' (0.001610)
   4. ' The' (0.001327)
   5. ' __' (0.000848)
   6. '柏林' (0.000724)
   7. ' Frankfurt' (0.000679)
   8. ' ______' (0.000558)
   9. ' Hamburg' (0.000232)
  10. ' Bon' (0.000230)
  11. ' Munich' (0.000171)
  12. ' German' (0.000126)
  13. ' __________________' (0.000082)
  14. ' A' (0.000080)
  15. ' Ber' (0.000072)
  16. ' ____' (0.000070)
  17. ' ___' (0.000065)
  18. ' berlin' (0.000055)
  19. ' _____' (0.000055)
  20. ' London' (0.000049)

Layer 35 (entropy: 1.400):
   1. ' Berlin' (0.719232)
   2. ' The' (0.086093)
   3. ' Germany' (0.034847)
   4. ' ' (0.034652)
   5. ' __' (0.034198)
   6. ' 

' (0.010643)
   7. ' ______' (0.009245)
   8. ' A' (0.007960)
   9. ' (' (0.003471)
  10. ' What' (0.002803)
  11. ' Ber' (0.002647)
  12. ' This' (0.002610)
  13. ' Bon' (0.002474)
  14. ' ?

' (0.002392)
  15. ' __________________' (0.002202)
  16. ' [' (0.002137)
  17. ' Frankfurt' (0.002137)
  18. ' ?' (0.001834)
  19. ' I' (0.001672)
  20. ' ___' (0.001398)

============================================================
ACTUAL MODEL PREDICTION (for comparison):
Model's final prediction (entropy: 1.400):
   1. ' Berlin' (0.719238)
   2. ' The' (0.086090)
   3. ' Germany' (0.034847)
   4. ' ' (0.034651)
   5. ' __' (0.034197)
   6. ' 

' (0.010642)
   7. ' ______' (0.009245)
   8. ' A' (0.007960)
   9. ' (' (0.003471)
  10. ' What' (0.002803)
  11. ' Ber' (0.002647)
  12. ' This' (0.002610)
  13. ' Bon' (0.002474)
  14. ' ?

' (0.002392)
  15. ' __________________' (0.002202)
  16. ' Frankfurt' (0.002137)
  17. ' [' (0.002137)
  18. ' ?' (0.001834)
  19. ' I' (0.001672)
  20. ' ___' (0.001398)
============================================================
ADDITIONAL PROBING:

Prompt: 'Germany's capital is'
  (entropy: 2.072)
   1. ' Berlin' (0.616918)
   2. ' not' (0.070298)
   3. ' a' (0.067678)
   4. ' in' (0.041271)
   5. ' located' (0.021940)
   6. ' the' (0.016106)
   7. ' __' (0.015825)
   8. ' called' (0.011860)
   9. ' ______' (0.008225)
  10. ' known' (0.007707)

Prompt: 'Berlin is the capital of'
  (entropy: 0.835)
   1. ' Germany' (0.728571)
   2. ' which' (0.220736)
   3. ' the' (0.023737)
   4. ' what' (0.011355)
   5. ' __' (0.002297)
   6. ' ______' (0.001446)
   7. ' a' (0.001124)
   8. ' German' (0.000802)
   9. ' Berlin' (0.000459)
  10. '
' (0.000439)

Prompt: 'Respond in one word: which city is the capital of Germany?'
  (entropy: 1.132)
   1. ' Berlin' (0.721222)
   2. ' The' (0.174027)
   3. ' Also' (0.015090)
   4. '
' (0.012803)
   5. ' What' (0.009649)
   6. ' 

' (0.008670)
   7. ' ' (0.008417)
   8. ' Ber' (0.007864)
   9. ' Additionally' (0.004229)
  10. ' In' (0.003077)
============================================================
TEMPERATURE EXPLORATION:
(Temperature controls randomness: low=confident, high=creative)

Temperature 0.1:
  (entropy: 0.000)
   1. ' Berlin' (1.000000)
   2. ' The' (0.000000)
   3. ' Germany' (0.000000)
   4. ' ' (0.000000)
   5. ' __' (0.000000)
   6. ' 

' (0.000000)
   7. ' ______' (0.000000)
   8. ' A' (0.000000)
   9. ' (' (0.000000)
  10. ' What' (0.000000)
  11. ' Ber' (0.000000)
  12. ' This' (0.000000)
  13. ' Bon' (0.000000)
  14. ' ?

' (0.000000)
  15. ' __________________' (0.000000)

Temperature 2.0:
  (entropy: 9.109)
   1. ' Berlin' (0.066317)
   2. ' The' (0.022944)
   3. ' Germany' (0.014597)
   4. ' ' (0.014556)
   5. ' __' (0.014460)
   6. ' 

' (0.008067)
   7. ' ______' (0.007518)
   8. ' A' (0.006976)
   9. ' (' (0.004607)
  10. ' What' (0.004140)
  11. ' Ber' (0.004023)
  12. ' This' (0.003995)
  13. ' Bon' (0.003889)
  14. ' ?

' (0.003824)
  15. ' __________________' (0.003669)
=== END OF INSPECTING ==============


=== MODEL STATS ===============
Number of layers: 36
Model dimension: 4096
Number of heads: 32
Vocab size: 151936
Context length: 2048
=== END OF MODEL STATS ========

